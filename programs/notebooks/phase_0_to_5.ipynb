{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase 0: compare old schema vs old data, output the summary in an excel file* to help decision making\n",
    "# phase 1: transform old schema to match (correct version) of old data\n",
    "# phase 2: compare phase 1 schema vs old data, output the summary in an excel file* to help decision making\n",
    "# phase 3: transform old data to match schema from phase 1 (doesn't count dependencies/required keywords)\n",
    "# phase 4: transform schema from phase 1 to mat|ch JSON Schema draft v7 (we will call it 'new schema')\n",
    "# phase 5: transform data from phase 2 to match the new schema (include all dependencies/required keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## common imports ##\n",
    "from os import path, makedirs\n",
    "import glob\n",
    "import json\n",
    "from jsonschema.validators import Draft7Validator\n",
    "from sys import exit\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## common functions ##\n",
    "def fetch_mibig_json_filepaths(dir_path):\n",
    "    \"\"\"fetch mibig json paths from a specific folder\"\"\"\n",
    "    return glob.glob(path.join(dir_path, \"BGC*.json\"))\n",
    "\n",
    "def count_props(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json, construct a list of property paths\n",
    "    along with its presence count in the json object\"\"\"\n",
    "    key_path = cur_path\n",
    "    \n",
    "    if isinstance(input_dict, dict):\n",
    "        for key in input_dict.keys():\n",
    "            result = count_props(input_dict[key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif isinstance(input_dict, list):\n",
    "        key_path = \"{}[]\".format(key_path)\n",
    "        for node in input_dict:\n",
    "            result = count_props(node, \"{}\".format(key_path), result)\n",
    "\n",
    "    if not isinstance(input_dict, dict):\n",
    "        if key_path not in result:\n",
    "            result[key_path] = 0\n",
    "        result[key_path] += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_props_old_schema(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json schema, construct a list of property paths\n",
    "    along with either required == True for each properties\"\"\"\n",
    "    key_path = cur_path\n",
    "    if (\"type\" not in input_dict) or (input_dict[\"type\"] not in [\"object\", \"array\"]):\n",
    "        key_path = \"{}\".format(cur_path) # string / etc.\n",
    "    elif input_dict[\"type\"] == \"object\":\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            result = fetch_props_old_schema(input_dict[\"properties\"][key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif input_dict[\"type\"] == \"array\":\n",
    "        key_path = \"{}[]\".format(cur_path)\n",
    "        result = fetch_props_old_schema(input_dict[\"items\"], \"{}\".format(key_path), result)\n",
    "    \n",
    "    if key_path not in result and \"properties\" not in input_dict:\n",
    "        result[key_path] = \"required\" in input_dict and input_dict[\"required\"] == True\n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_props_new_schema(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json draft7 schema, construct a list of property paths\n",
    "    along with either required == True for each properties\"\"\"\n",
    "    key_path = cur_path\n",
    "    if (\"type\" not in input_dict) or (input_dict[\"type\"] not in [\"object\", \"array\"]):\n",
    "        key_path = \"{}\".format(cur_path) # string / etc.\n",
    "    elif input_dict[\"type\"] == \"object\":\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            result = fetch_props_old_schema(input_dict[\"properties\"][key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif input_dict[\"type\"] == \"array\":\n",
    "        key_path = \"{}[]\".format(cur_path)\n",
    "        result = fetch_props_old_schema(input_dict[\"items\"], \"{}\".format(key_path), result)\n",
    "    \n",
    "    if key_path not in result and \"properties\" not in input_dict:\n",
    "        result[key_path] = False # can't really use this\n",
    "    return result\n",
    "\n",
    "\n",
    "def search_and_delete(key, input_dict):\n",
    "    \"\"\"delete keys from nested dict\"\"\"\n",
    "    if isinstance(input_dict, list):\n",
    "        for i in input_dict:\n",
    "            search_and_delete(key, i)\n",
    "    elif not isinstance(input_dict, dict):\n",
    "        return\n",
    "    to_del = []\n",
    "    for k in input_dict:\n",
    "        if k == key:\n",
    "            to_del.append(k)\n",
    "        elif isinstance(input_dict[k], dict):\n",
    "            search_and_delete(key, input_dict[k])\n",
    "    for k in to_del:\n",
    "        del input_dict[k]\n",
    "\n",
    "        \n",
    "def rename_key(from_key, to_key, parent_dict):\n",
    "    \"\"\"rename key in dict\"\"\"\n",
    "    if from_key in parent_dict:\n",
    "        parent_dict[to_key] = parent_dict[from_key]\n",
    "        del parent_dict[from_key]\n",
    "\n",
    "def del_key(key, parent_dict):\n",
    "    if key in parent_dict:\n",
    "        del parent_dict[key]\n",
    "        \n",
    "import time\n",
    "def date2iso(thedate):\n",
    "    strdate = thedate.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    minute = (time.localtime().tm_gmtoff / 60) % 60\n",
    "    hour = ((time.localtime().tm_gmtoff / 60) - minute) / 60\n",
    "    utcoffset = \"%.2d:%.2d\" %(hour, minute)\n",
    "    if utcoffset[0] != '-':\n",
    "        utcoffset = '+' + utcoffset\n",
    "        return strdate + utcoffset\n",
    "    \n",
    "class ToDelete():\n",
    "    \"\"\"dummy class for lazy deletion of list members\"\"\"\n",
    "    pass\n",
    "\n",
    "def lazily_deletes(input_dict):\n",
    "    \"\"\"traverse and lazily delete list/dict members\"\"\"\n",
    "    if isinstance(input_dict, list):\n",
    "        new_list = []\n",
    "        for i, node in enumerate(input_dict):\n",
    "            if not isinstance(node, ToDelete):\n",
    "                input_dict[i] = lazily_deletes(node)\n",
    "                new_list.append(node)\n",
    "        return new_list\n",
    "    elif isinstance(input_dict, dict):\n",
    "        key_to_dels = []\n",
    "        for key in input_dict:\n",
    "            if not isinstance(input_dict[key], ToDelete):\n",
    "                input_dict[key] = lazily_deletes(input_dict[key])\n",
    "            else:\n",
    "                key_to_dels.append(key)\n",
    "        for key in key_to_dels:\n",
    "            del input_dict[key]\n",
    "    return input_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 0: compare old schema vs old data, output the summary in an excel file* to help decision making ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_props = {}\n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../inputs/json_1.4/\")):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        this_file_props = count_props(json_obj, \"\", {})\n",
    "        for prop in this_file_props:\n",
    "            if prop not in all_props:\n",
    "                all_props[prop] = [path.basename(json_path)]\n",
    "            else:\n",
    "                all_props[prop].append(path.basename(json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"../../preprocessed/reports/\"):\n",
    "    makedirs(\"../../preprocessed/reports/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ../../preprocessed/reports/p0-old_schema_properties.csv\n",
      "File written: ../../preprocessed/reports/p0-old_data_vs_old_schema.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../inputs/mibig_schema.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_old_schema(json_obj, \"\", {})\n",
    "    with open(\"../../preprocessed/reports/p0-old_schema_properties.csv\", \"w\") as o:\n",
    "        for key in sorted(schema_props.keys()):\n",
    "            o.write(\"{},{}\\n\".format(key, schema_props[key]))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    with open(\"../../preprocessed/reports/p0-old_data_vs_old_schema.csv\", \"w\") as o:\n",
    "        not_in_schema = []\n",
    "        for key in sorted(all_props.keys()):\n",
    "            if key not in schema_props.keys():\n",
    "                not_in_schema.append((key, all_props[key]))\n",
    "        for rep in sorted(not_in_schema, key=lambda x: len(x[1]), reverse = True):\n",
    "            o.write(\"{},{}\\n\".format(rep[0], len(rep[1])))\n",
    "        print(\"File written: {}\".format(o.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 1: transform old schema to match (correct version) of old data ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File location: ../../inputs/mibig_schema_phase_1.json\n"
     ]
    }
   ],
   "source": [
    "# (everything is manually done) -- TODO: should write hardcoded scripts to make it reproducible\n",
    "# update all comma-separated based properties into arrays\n",
    "# gene_pubs: integer --> gene_pubs: array\n",
    "print(\"File location: ../../inputs/mibig_schema_phase_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### phase 2: compare phase 1 schema vs old data, output the summary in an excel file* to help decision making ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ../../preprocessed/reports/p2-schema_phase_1_properties.csv\n",
      "File written: ../../preprocessed/reports/p2-old_data_vs_schema_phase_1.csv\n",
      "File written: ../../preprocessed/reports/p2-bgc_to_fix.csv\n"
     ]
    }
   ],
   "source": [
    "# use all_props from phase 0\n",
    "with open(\"../../inputs/mibig_schema_phase_1.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_old_schema(json_obj, \"\", {})\n",
    "    with open(\"../../preprocessed/reports/p2-schema_phase_1_properties.csv\", \"w\") as o:\n",
    "        for key in sorted(schema_props.keys()):\n",
    "            o.write(\"{},{}\\n\".format(key, schema_props[key]))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append((key, all_props[key]))\n",
    "    with open(\"../../preprocessed/reports/p2-old_data_vs_schema_phase_1.csv\", \"w\") as o:\n",
    "        for rep in sorted(not_in_schema, key=lambda x: len(x[1]), reverse = True):\n",
    "            o.write(\"{},{}\\n\".format(rep[0], len(rep[1])))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    with open(\"../../preprocessed/reports/p2-bgc_to_fix.csv\", \"w\") as o:\n",
    "        bgc_to_fix = {}\n",
    "        for rep in not_in_schema:\n",
    "            for bgc in rep[1]:\n",
    "                if bgc not in bgc_to_fix:\n",
    "                    bgc_to_fix[bgc] = []\n",
    "                bgc_to_fix[bgc].append(rep[0])\n",
    "        for bgc in bgc_to_fix:\n",
    "            o.write(\"{},{}\\n\".format(bgc, \";\".join(bgc_to_fix[bgc])))\n",
    "        print(\"File written: {}\".format(o.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### phase 3: transform old data to match schema from phase 1 (doesn't count dependencies/required keywords) ####\n",
    "def match_attributes_to_schema_phase_1(data):\n",
    "    # fix /Comments\n",
    "    rename_key(\"Comments\", \"comments\", data)\n",
    "    # fix /general_params/loci/nucl_acc[]/conn_comp_cluster\n",
    "    for nuac in data[\"general_params\"][\"loci\"][\"nucl_acc\"]:\n",
    "        if \"conn_comp_cluster\" in nuac and isinstance(nuac[\"conn_comp_cluster\"], str):\n",
    "            nuac[\"conn_comp_cluster\"] = [s.strip() for s in nuac[\"conn_comp_cluster\"].split(\",\")]\n",
    "    # fix /general_params/Polyketide/Saccharide\n",
    "    if \"Polyketide\" in data[\"general_params\"] and \"Saccharide\" in data[\"general_params\"][\"Polyketide\"]:\n",
    "        data[\"general_params\"][\"Saccharide\"] = data[\"general_params\"][\"Polyketide\"][\"Saccharide\"]\n",
    "        del data[\"general_params\"][\"Polyketide\"][\"Saccharide\"]\n",
    "    # fix /general_params/Saccharide/Sugar_subclass\n",
    "    if \"Saccharide\" in data[\"general_params\"] and \"Sugar_subclass\" in data[\"general_params\"][\"Saccharide\"]:\n",
    "        rename_key(\"Sugar_subclass\", \"saccharide_subclass\", data[\"general_params\"][\"Saccharide\"])\n",
    "    # fix /general_params/Saccharide/gt_genes[]/sugar_subcluster\n",
    "    if \"Saccharide\" in data[\"general_params\"] and \"gt_genes\" in data[\"general_params\"][\"Saccharide\"]:\n",
    "        for gtg in data[\"general_params\"][\"Saccharide\"][\"gt_genes\"]:\n",
    "            if \"sugar_subcluster\" in gtg and isinstance(gtg[\"sugar_subcluster\"], str):\n",
    "                gtg[\"sugar_subcluster\"] = [s.strip() for s in gtg[\"sugar_subcluster\"].split(\",\")]    \n",
    "    for comp in data[\"general_params\"][\"compounds\"]:\n",
    "        # fix /general_params/compounds[]/chem_target\n",
    "        if \"chem_target\" in comp and isinstance(comp[\"chem_target\"], str):\n",
    "            comp[\"chem_target\"] = [s.strip() for s in comp[\"chem_target\"].split(\",\")]  \n",
    "        # fix /general_params/compounds[]/chem_moieties[]/subcluster\n",
    "        if \"chem_moieties\" in comp:\n",
    "            for moi in comp[\"chem_moieties\"]:\n",
    "                if \"subcluster\" in moi:\n",
    "                    if moi[\"subcluster\"] == \"unknown\":\n",
    "                        del moi[\"subcluster\"]\n",
    "    # fix /general_params/genes/gene[]/evidence_genefunction[][]**\n",
    "    if \"genes\" in data[\"general_params\"]:\n",
    "        if \"gene\" in data[\"general_params\"][\"genes\"]:\n",
    "            for gen in data[\"general_params\"][\"genes\"][\"gene\"]:\n",
    "                if \"evidence_genefunction\" in gen:\n",
    "                    for i, evgen in enumerate(gen[\"evidence_genefunction\"]):\n",
    "                        def getvalevgen(ar):\n",
    "                            if isinstance(ar, list):\n",
    "                                return getvalevgen(ar[0])\n",
    "                            else:\n",
    "                                return ar\n",
    "                        gen[\"evidence_genefunction\"][i] = getvalevgen(evgen)\n",
    "    # fix /general_params/Other/biosyn_class[]\n",
    "    if \"Other\" in data[\"general_params\"] and \"biosyn_class\" in data[\"general_params\"][\"Other\"]:\n",
    "        clas = data[\"general_params\"][\"Other\"][\"biosyn_class\"][0]\n",
    "        del data[\"general_params\"][\"Other\"][\"biosyn_class\"]\n",
    "        data[\"general_params\"][\"Other\"][\"other_subclass\"] = clas\n",
    "    # fix /general_params/publications\n",
    "    if \"publications\" in data[\"general_params\"] and isinstance(data[\"general_params\"][\"publications\"], str):\n",
    "        data[\"general_params\"][\"publications\"] = [s.strip() for s in data[\"general_params\"][\"publications\"].split(\",\")]\n",
    "    # Polyketide\n",
    "    if \"Polyketide\" in data[\"general_params\"]:\n",
    "        pol = data[\"general_params\"][\"Polyketide\"]\n",
    "        # fix /general_params/Polyketide/cyclases\n",
    "        if \"cyclases\" in pol and isinstance(pol[\"cyclases\"], str):\n",
    "            pol[\"cyclases\"] = [s.strip() for s in pol[\"cyclases\"].split(\",\")]\n",
    "        # fix /general_params/Polyketide/pks_genes\n",
    "        if \"pks_genes\" in pol and isinstance(pol[\"pks_genes\"], str):\n",
    "            pol[\"pks_genes\"] = [s.strip() for s in pol[\"pks_genes\"].split(\",\")]\n",
    "        # fix /general_params/Polyketide/pufa_mod_doms\n",
    "        if \"pufa_mod_doms\" in pol and isinstance(pol[\"pufa_mod_doms\"], str):\n",
    "            pol[\"pufa_mod_doms\"] = [s.strip() for s in pol[\"pufa_mod_doms\"].split(\",\")]\n",
    "        # fix /general_params/Polyketide/mod_pks_genes[]/pks_module[]/pks_mod_doms\n",
    "        if \"mod_pks_genes\" in pol:\n",
    "            for modpk in pol[\"mod_pks_genes\"]:\n",
    "                if \"pks_module\" in modpk:\n",
    "                    for pkmod in modpk[\"pks_module\"]:\n",
    "                        if \"pks_mod_doms\" in pkmod and isinstance(pkmod[\"pks_mod_doms\"], str):\n",
    "                            pkmod[\"pks_mod_doms\"] = [s.strip() for s in pkmod[\"pks_mod_doms\"].split(\",\")]\n",
    "    # RiPP\n",
    "    if \"RiPP\" in data[\"general_params\"]:\n",
    "        rip = data[\"general_params\"][\"RiPP\"]\n",
    "        if \"precursor_loci\" in rip:\n",
    "            for ploc in rip[\"precursor_loci\"]:\n",
    "                # fix /general_params/RiPP/precursor_loci[]/cleavage_recogn_site\n",
    "                if \"cleavage_recogn_site\" in ploc and isinstance(ploc[\"cleavage_recogn_site\"], str):\n",
    "                    ploc[\"cleavage_recogn_site\"] = [s.strip() for s in ploc[\"cleavage_recogn_site\"].split(\",\")]\n",
    "                # fix /general_params/RiPP/precursor_loci[]/core_pept_aa\n",
    "                if \"core_pept_aa\" in ploc and isinstance(ploc[\"core_pept_aa\"], str):\n",
    "                    ploc[\"core_pept_aa\"] = [s.strip() for s in ploc[\"core_pept_aa\"].split(\",\")]\n",
    "                            \n",
    "    return\n",
    "\n",
    "if not path.exists(\"../../preprocessed/p3-json/\"):\n",
    "    makedirs(\"../../preprocessed/p3-json/\")\n",
    "    \n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../inputs/json_1.4/\")):\n",
    "    json_obj = None\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        match_attributes_to_schema_phase_1(json_obj)        \n",
    "        with open(path.join(\"../../preprocessed/p3-json/\", path.basename(json_path)), \"w\") as json_file:\n",
    "            json.dump(json_obj, json_file, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that all data matched schema\n",
    "all_props_phase_3 = {}\n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/p3-json/\")):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        this_file_props = count_props(json_obj, \"\", {})\n",
    "        for prop in this_file_props:\n",
    "            if prop not in all_props_phase_3:\n",
    "                all_props_phase_3[prop] = [path.basename(json_path)]\n",
    "            else:\n",
    "                all_props_phase_3[prop].append(path.basename(json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conflicts: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../inputs/mibig_schema_phase_1.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_old_schema(json_obj, \"\", {})\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props_phase_3.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append((key, all_props_phase_3[key]))\n",
    "            print(key)\n",
    "    print(\"Number of conflicts: {}\".format(len(not_in_schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 4: transform schema from phase 1 to match JSON Schema draft v7 (we will call it 'new schema') ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema = None\n",
    "with open(\"../../inputs/mibig_schema_phase_1.json\") as json_file:\n",
    "    new_schema = json.load(json_file) # pre-load with old schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: convert 'required' to Json Schema draft 7 style\n",
    "def fix_required(input_dict):\n",
    "    if \"type\" in input_dict and input_dict[\"type\"] == \"object\":\n",
    "        input_dict[\"required\"] = []\n",
    "        for prop in input_dict[\"properties\"]:\n",
    "            child = input_dict[\"properties\"][prop]\n",
    "            if \"required\" in child and child[\"required\"] == True:\n",
    "                input_dict[\"required\"].append(prop)\n",
    "            fix_required(child)\n",
    "        if len(input_dict[\"required\"]) < 1:\n",
    "            del input_dict[\"required\"]\n",
    "    else:\n",
    "        if \"type\" in input_dict and input_dict[\"type\"] == \"array\":\n",
    "            fix_required(input_dict[\"items\"])\n",
    "        if \"required\" in input_dict:\n",
    "            del input_dict[\"required\"]\n",
    "fix_required(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: convert 'dependencies' to Json Schema draft 7 style\n",
    "def fix_dependencies(input_dict):\n",
    "    if \"type\" in input_dict and input_dict[\"type\"] == \"object\":\n",
    "        input_dict[\"dependencies\"] = {}\n",
    "        for prop in input_dict[\"properties\"]:\n",
    "            child = input_dict[\"properties\"][prop]\n",
    "            if \"dependencies\" in child and isinstance(child[\"dependencies\"], str):\n",
    "                if child[\"dependencies\"] in input_dict[\"properties\"]:\n",
    "                    if child[\"dependencies\"] not in input_dict[\"dependencies\"]:\n",
    "                        input_dict[\"dependencies\"][child[\"dependencies\"]] = []\n",
    "                    input_dict[\"dependencies\"][child[\"dependencies\"]].append(prop)\n",
    "                else:\n",
    "                    print(\"Error: {} not found\".format(child[\"dependencies\"]))\n",
    "            fix_dependencies(child)\n",
    "        if len(input_dict[\"dependencies\"].keys()) < 1:\n",
    "            del input_dict[\"dependencies\"]\n",
    "    else:\n",
    "        if \"type\" in input_dict and input_dict[\"type\"] == \"array\":\n",
    "            fix_dependencies(input_dict[\"items\"])\n",
    "        if \"dependencies\" in input_dict:\n",
    "            del input_dict[\"dependencies\"]\n",
    "fix_dependencies(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: make sure 'enum' contain unique items, and remove all trailing white spaces\n",
    "def fix_enum(input_dict):\n",
    "    if \"type\" in input_dict and input_dict[\"type\"] == \"object\":        \n",
    "        for prop in input_dict[\"properties\"]:\n",
    "            fix_enum(input_dict[\"properties\"][prop])\n",
    "    elif \"type\" in input_dict and input_dict[\"type\"] == \"array\":\n",
    "        fix_enum(input_dict[\"items\"])\n",
    "            \n",
    "    if \"enum\" in input_dict:\n",
    "        for i, item in enumerate(input_dict[\"enum\"]):\n",
    "            input_dict[\"enum\"][i] = item.rstrip().lstrip()\n",
    "        input_dict[\"enum\"] = list(set(input_dict[\"enum\"]))\n",
    "fix_enum(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: manual (but reproducible) curations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "new_schema[\"$schema\"] = \"http://json-schema.org/draft-07/schema#\"\n",
    "new_schema[\"$schema_version\"] = \"2.0\"\n",
    "new_schema[\"$schema_created\"] = date2iso(now)\n",
    "\n",
    "# remove version, replace with changelogs\n",
    "del new_schema[\"properties\"][\"version\"]\n",
    "new_schema[\"properties\"][\"changelogs\"] = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"version\": {\n",
    "                \"type\": \"string\", # or integer?\n",
    "                \"pattern\": \"^\\\\d+(\\\\.\\\\d+)*$\"\n",
    "            },\n",
    "            \"comment\": {\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# remove \"embargo\" (we will track it in the database and not in the json schema)\n",
    "del new_schema[\"properties\"][\"embargo\"]\n",
    "\n",
    "# require \"changelogs\", \"general_params\", \"personal\"\n",
    "new_schema[\"required\"] = [\"changelogs\", \"general_params\", \"personal\"]\n",
    "\n",
    "# require \"mibig_accession\", \"biosyn_class\", \"compounds\", \"publications\"\n",
    "new_schema[\"properties\"][\"general_params\"][\"required\"] = [\"mibig_accession\", \"biosyn_class\", \"compounds\", \"publications\"]\n",
    "\n",
    "# delete properties we don't need anymore (i.e. ones meant for AlpacaJS forms)\n",
    "del new_schema[\"properties\"][\"personal\"][\"properties\"][\"submitter_institution\"][\"format\"]\n",
    "search_and_delete(\"default\", new_schema)\n",
    "search_and_delete(\"description\", new_schema) # we should add the proper descriptions later\n",
    "\n",
    "# rename Polyketide, NRP, RiPP, Terpene, Saccharide, Alkaloid, Other, to lowercases\n",
    "rename_key(\"Polyketide\", \"polyketide\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"NRP\", \"nrp\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"RiPP\", \"ripp\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Terpene\", \"terpene\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Saccharide\", \"saccharide\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Alkaloid\", \"alkaloid\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Other\", \"other\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "\n",
    "# fix publications.pattern (cover pubmed id, (google) patent id, doi, and url)\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"publications\"][\"items\"][\"pattern\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"publications\"][\"items\"][\"oneOf\"] = [\n",
    "    {\"pattern\": \"^pubmed:(\\\\d+)$\"},\n",
    "    {\"pattern\": \"^doi:10\\\\.\\\\d{4,9}/[-\\\\._;()/:a-zA-Z0-9]+$\"},\n",
    "    {\"pattern\": \"^patent:(.+)$\"},\n",
    "    {\"pattern\": \"^url:https?:\\\\/\\\\/(www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{2,256}\\\\.[a-z]{2,6}\\\\b([-a-zA-Z0-9@:%_\\\\+.~#?&//=]*)$\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.compounds ###\n",
    "\n",
    "# remove database_deposited and databases_deposited. we can always infer it from their respective accession ids\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"database_deposited\"]\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"databases_deposited\"]\n",
    "# delete old dependencies format for compounds\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"dependencies\"]\n",
    "# required = [\"compound\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"required\"] = [\"compound\"]\n",
    "# if chem_act == \"other\", require other_chem_act\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"chem_act\": {\"contains\":{\"enum\": [\"Other\"]}}},\n",
    "            \"required\": [\"chem_act\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"other_chem_act\"]\n",
    "          }\n",
    "})\n",
    "# delete old dependencies format for chem_moieties\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"chem_moieties\"][\"items\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"chem_moieties\"][\"items\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"chem_moieties\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"chem_moiety\": {\"enum\": [\"Other\"]}},\n",
    "            \"required\": [\"chem_moiety\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"other_chem_moiety\"]\n",
    "          }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.loci ###\n",
    "\n",
    "# this is a big change: make sure that BGC have ONLY 1 loci (move everything from 'nucl_acc' to the 'loci' level)\n",
    "for nuc_key in new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"][\"items\"][\"properties\"]:\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][nuc_key] = new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"][\"items\"][\"properties\"][nuc_key]\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"]\n",
    "\n",
    "# change 'Accession' to 'accession'\n",
    "rename_key(\"Accession\", \"accession\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"])\n",
    "# accession pattern -- now also accept 'mibig:BGCXXXXXXX': this will cover sequences not in NCBI (we host it ourself)\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"accession\"][\"pattern\"] = \"^([A-Za-z0-9\\\\._]{3,}){1}|(MIBIG:BGC\\\\d{7}){1}$\"\n",
    "\n",
    "# loci.required = [\"complete\", \"accession\", \"conn_comp_cluster\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"required\"] = [\"complete\", \"accession\", \"conn_comp_cluster\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.genes ###\n",
    "\n",
    "# operon.required = [\"operon_genes\", \"evidence_operon\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"operon\"][\"items\"][\"required\"] = [\"operon_genes\", \"evidence_operon\"]\n",
    "# gene.required = [\"gene_function\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"required\"] = [\"gene_function\"]\n",
    "# delete old dependencies format, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"] = []\n",
    "\n",
    "# add gene.strand\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"properties\"][\"strand\"] = {\n",
    "    \"type\": \"integer\",\n",
    "    \"enum\": [ -1, 0, 1 ]\n",
    "}\n",
    "\n",
    "# add gene.aa_seq\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"properties\"][\"aa_seq\"] = {\n",
    "    \"type\": \"string\"\n",
    "}\n",
    "\n",
    "# if gene_id is not supplied, require gene_name\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": {\n",
    "                \"required\": [\"gene_id\"]\n",
    "               }\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"gene_name\"]\n",
    "          },\n",
    "})\n",
    "\n",
    "# if gene_function != Unknown, require evidence_genefunction\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": {\"properties\": {\"gene_function\": {\"enum\": [\"Unknown\"]}}},\n",
    "            \"required\": [\"gene_function\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"evidence_genefunction\"]\n",
    "          }\n",
    "})\n",
    "# if gene_function == Tailoring, require tailoring\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"gene_function\": {\"enum\": [\"Tailoring\"]}},\n",
    "            \"required\": [\"gene_function\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"tailoring\"]\n",
    "          }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.polyketide ###\n",
    "# for now, just remove all dependencies and requirements\n",
    "search_and_delete(\"required\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"])\n",
    "search_and_delete(\"dependencies\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"])\n",
    "\n",
    "# add evidence_at_spec = \"None\"\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"properties\"][\"mod_pks_genes\"][\"items\"][\"properties\"][\"pks_module\"][\"items\"][\"properties\"][\"evidence_at_spec\"][\"enum\"].append(\"None\")\n",
    "\n",
    "# update pks_domains enum\n",
    "# These should be allowed:\n",
    "# \"KS\", “SAT”, \"AT\", \"CAL\", \"DH\", \"KR\", \"ER\", \"T\", \"C\", \"A\", \"E\", “MT”, “TE”, “TD”, \"ST\", \"PT\", \"PPT\"\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"properties\"][\"mod_pks_genes\"][\"items\"][\"properties\"][\"pks_module\"][\"items\"][\"properties\"][\"pks_domains\"][\"items\"][\"enum\"] = [\n",
    "    \"Ketosynthase\", #KS\n",
    "    \"ACP transacylase\", #SAT\n",
    "    \"Acyltransferase\", #AT\n",
    "    \"CoA-ligase\", #CAL\n",
    "    \"Dehydratase\", #DH\n",
    "    \"Ketoreductase\", #KR\n",
    "    \"Enoylreductase\", #ER\n",
    "    \"Thiolation (ACP/PCP)\", #T\n",
    "    \"Condensation\", #C\n",
    "    \"Adenlyation\", #A\n",
    "    \"Epimerization\", #E\n",
    "    \"Methyltransferase\", #MT\n",
    "    \"Thioesterase\", #TE\n",
    "    \"Thioeter reductase\", #TD\n",
    "    \"Sulfotransferase\", #ST\n",
    "    \"Product Template domain\", #PT\n",
    "    \"Phosphopantetheinyl transferase\", #PPT\n",
    "    \"Thiol reductase\" #PPT\n",
    "]\n",
    "\n",
    "if False: #TODO: fix Polyketide schema (requirements, dependencies)\n",
    "    # required = [\"pk_subclass\", \"pks_subclass\", \"lin_cycl_pk\", \"starter_unit\", \"pks_genes\", \"cyclases\"]\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"required\"] = [\"pk_subclass\", \"pks_subclass\", \"pks_te_type\", \"lin_cycl_pk\", \"starter_unit\", \"ketide_length\"]\n",
    "\n",
    "    # delete old dependencies format, replace with allOf\n",
    "    del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"dependencies\"]\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"] = []\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "                \"properties\": {\"pks_subclass\": {\"enum\": [\"Modular type I\"]}},\n",
    "                \"required\": [\"pks_subclass\"]\n",
    "              },\n",
    "              \"then\": {\n",
    "                \"required\": [\"mod_pks_genes\"],\n",
    "                \"properties\": {\"mod_pks_genes\": {\"minItems\": 1}}\n",
    "              }\n",
    "    })\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "                \"properties\": {\"pks_subclass\": {\"enum\": [\"Trans-AT type I\"]}},\n",
    "                \"required\": [\"pks_subclass\"]\n",
    "              },\n",
    "              \"then\": {\n",
    "                \"required\": [\"trans_at\"],\n",
    "                \"properties\": {\"trans_at\": {\"minItems\": 1}}\n",
    "              }\n",
    "    })\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "                \"properties\": {\"pks_subclass\": {\"enum\": [\"Iterative type I\"]}},\n",
    "                \"required\": [\"pks_subclass\"]\n",
    "              },\n",
    "              \"then\": {\n",
    "                \"required\": [\"iterative_subtype\", \"nr_iterations\", \"iter_cycl_type\"]\n",
    "              }\n",
    "    })\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "                \"properties\": {\"pks_subclass\": {\"enum\": [\"PUFA synthase or related\"]}},\n",
    "                \"required\": [\"pks_subclass\"]\n",
    "              },\n",
    "              \"then\": {\n",
    "                \"required\": [\"pufa_mod_doms\"],\n",
    "                \"properties\": {\"pufa_mod_doms\": {\"minItems\": 1}}\n",
    "              }\n",
    "    })\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "                \"not\": { \"properties\": {\"pks_te_type\": {\"enum\": [\"None\"]}} },\n",
    "                \"required\": [\"pks_te_type\"]\n",
    "              },\n",
    "              \"then\": {\n",
    "                \"required\": [\"pks_thioesterase\"],\n",
    "                \"properties\": {\"pks_thioesterase\": {\"minItems\": 1}}\n",
    "              }\n",
    "    })\n",
    "# --- todo: assert dependencies of pks_subtype --> requirements\n",
    "# --- todo: apply dependencies for mod_pks_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.nrp ###\n",
    "# for now, just remove all dependencies and requirements\n",
    "search_and_delete(\"required\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"])\n",
    "search_and_delete(\"dependencies\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"])\n",
    "\n",
    "if False: #TODO: fix NRP schema (requirements, dependencies)\n",
    "    # delete old dependencies format, replace with allOf\n",
    "    del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"dependencies\"]\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"] = []\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "                \"not\": { \"properties\": {\"nrps_te_type\": {\"enum\": [\"None\"]}} },\n",
    "                \"required\": [\"nrps_te_type\"]\n",
    "              },\n",
    "              \"then\": {\n",
    "                \"required\": [\"nrps_thioesterase\"],\n",
    "                \"properties\": {\"nrps_thioesterase\": {\"minItems\": 1}}\n",
    "              }\n",
    "    })\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "                \"properties\": {\"subclass\": {\"enum\": [\"Other lipopeptide\"]}},\n",
    "                \"required\": [\"subclass\"]\n",
    "              },\n",
    "              \"then\": {\n",
    "                \"required\": [\"lipid_moiety\"]\n",
    "              }\n",
    "    })\n",
    "\n",
    "    # nrps_module -- required = [\"module_nr\", \"cdom_subtype\", \"nrps_mod_doms\"]\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"required\"] = [\"module_nr\", \"cdom_subtype\", \"nrps_mod_doms\"]\n",
    "    # nrps_module -- delete old dependencies, replace with allOf\n",
    "    del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"dependencies\"]\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"] = []\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "                \"properties\": {\"nrps_mod_doms\": {\"enum\": [\"Other\"]}},\n",
    "                \"required\": [\"nrps_mod_doms\"]\n",
    "              },\n",
    "              \"then\": {\n",
    "                \"required\": [\"nrps_other_mod_dom\"]\n",
    "              }    \n",
    "    })\n",
    "    new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "                \"not\": {\"properties\": {\"nrps_mod_skip_iter\": {\"enum\": [\"Neither\"]}}},\n",
    "                \"required\": [\"nrps_mod_skip_iter\"]\n",
    "              },\n",
    "              \"then\": {\n",
    "                \"required\": [\"nrps_evidence_skip_iter\"]\n",
    "              }    \n",
    "    })\n",
    "\n",
    "## todo -- assert dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.saccharide ###\n",
    "\n",
    "# delete old dependencies format, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"gt_specificity\": {\"enum\": [\"Other\"]}},\n",
    "            \"required\": [\"gt_specificity\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"other_gt_spec\"]\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": { \"properties\": {\"gt_specificity\": {\"enum\": [\"Unknown\"]}} },\n",
    "            \"required\": [\"gt_specificity\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"evidence_gt_spec\"]\n",
    "          }\n",
    "})\n",
    "\n",
    "# move 'gt_genes.sugar_subcluster' to sugar_subclusters, we can infer the specificities from the list of gene ids\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"sugar_subclusters\"] = {\n",
    "    \"title\": \"Sub-clusters for sugar biosynthesis\",\n",
    "    \"type\": \"array\",\n",
    "    \"items\": new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"properties\"][\"sugar_subcluster\"]\n",
    "}\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"properties\"][\"sugar_subcluster\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for all 'required' fields, if it is an array, specify 'minItems = 1' so that it can't be empty\n",
    "def apply_array_required(input_dict):\n",
    "    if \"properties\" in input_dict:\n",
    "        if \"required\" in input_dict:\n",
    "            for req in input_dict[\"required\"]:\n",
    "                if \"type\" in input_dict[\"properties\"][req] and input_dict[\"properties\"][req][\"type\"] == \"array\":\n",
    "                    if \"minItems\" not in input_dict[\"properties\"][req]:\n",
    "                        input_dict[\"properties\"][req][\"minItems\"] = 1\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            apply_array_required(input_dict[\"properties\"][key])\n",
    "    elif \"items\" in input_dict:\n",
    "        apply_array_required(input_dict[\"items\"])\n",
    "                                \n",
    "apply_array_required(new_schema)\n",
    "\n",
    "### for all 'type' = 'string', if no 'pattern' or 'format' specified, specify 'minLength' = 1\n",
    "def apply_string_minlength(input_dict):\n",
    "    if \"properties\" in input_dict:\n",
    "        if \"type\" in input_dict and input_dict[\"type\"] == \"string\":\n",
    "            if \"pattern\" not in input_dict and \"format\" not in input_dict and \"minLength\" not in input_dict:\n",
    "                input_dict[\"minLength\"] = 1\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            apply_string_minlength(input_dict[\"properties\"][key])\n",
    "    elif \"items\" in input_dict:\n",
    "        apply_string_minlength(input_dict[\"items\"])\n",
    "        \n",
    "apply_string_minlength(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if class = \"Polyketide\", requires \"polyketide\" attribute, etc. except if minimal = true\n",
    "del new_schema[\"properties\"][\"general_params\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"allOf\"] = []\n",
    "prop_attr_pairs = [\n",
    "    (\"NRP\", \"nrp\"),\n",
    "    (\"Polyketide\", \"polyketide\"),\n",
    "    (\"RiPP\", \"ripp\"),\n",
    "    (\"Terpene\", \"terpene\"),\n",
    "    (\"Saccharide\", \"saccharide\"),\n",
    "    (\"Alkaloid\", \"alkaloid\"),\n",
    "    (\"Other\", \"other\")\n",
    "]\n",
    "for prop, attr in prop_attr_pairs:\n",
    "    then = { \"required\": [attr] }\n",
    "    sub_attr = new_schema[\"properties\"][\"general_params\"][\"properties\"][attr]\n",
    "    if \"required\" in sub_attr:\n",
    "        then[\"properties\"] = {}\n",
    "        then[\"properties\"][attr] = {\"required\": sub_attr[\"required\"]}\n",
    "        del sub_attr[\"required\"]\n",
    "    new_schema[\"properties\"][\"general_params\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "            \"not\": {\"properties\": {\"minimal\": {\"const\": True}}, \"required\": [\"minimal\"]},\n",
    "            \"properties\": {\"biosyn_class\": {\"contains\":{\"enum\": [prop]}}}\n",
    "          },\n",
    "          \"then\": then\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: save new schema\n",
    "with open(\"../../preprocessed/p4-mibig_schema_draft7.json\", \"w\") as o:\n",
    "    o.write(json.dumps(new_schema, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### phase 5: transform data from phase 2 to match the new schema (include all dependencies/required keywords) ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: check data vs new schema to get quick overview of changed structures\n",
    "# use all_props from phase 3\n",
    "with open(\"../../preprocessed/p4-mibig_schema_draft7.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_new_schema(json_obj, \"\", {})\n",
    "    with open(\"../../preprocessed/reports/p5-schema_draft7_properties.csv\", \"w\") as o:\n",
    "        for key in sorted(schema_props.keys()):\n",
    "            o.write(\"{}\\n\".format(key, schema_props[key]))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props_phase_3.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append((key, all_props_phase_3[key]))\n",
    "    with open(\"../../preprocessed/reports/p5-data_phase_3_vs_schema_draft7.csv\", \"w\") as o:\n",
    "        for rep in sorted(not_in_schema, key=lambda x: len(x[1]), reverse = True):\n",
    "            o.write(\"{},{}\\n\".format(rep[0], len(rep[1])))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    with open(\"../../preprocessed/reports/p5-bgc_to_fix.csv\", \"w\") as o:\n",
    "        bgc_to_fix = {}\n",
    "        for rep in not_in_schema:\n",
    "            for bgc in rep[1]:\n",
    "                if bgc not in bgc_to_fix:\n",
    "                    bgc_to_fix[bgc] = []\n",
    "                bgc_to_fix[bgc].append(rep[0])\n",
    "        for bgc in bgc_to_fix:\n",
    "            o.write(\"{},{}\\n\".format(bgc, \";\".join(bgc_to_fix[bgc])))\n",
    "        print(\"File written: {}\".format(o.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: fix data and assert that there is no more unrecognized attributes present\n",
    "def match_attributes_to_schema_7(data):\n",
    "    # remove 'embargo'\n",
    "    del_key(\"embargo\", data)\n",
    "    # remove 'version'\n",
    "    del_key(\"version\", data)\n",
    "    con_comp_temp = []\n",
    "    for nuc in data[\"general_params\"][\"loci\"][\"nucl_acc\"]:\n",
    "        # rename Accession to accession\n",
    "        rename_key(\"Accession\", \"accession\", nuc)\n",
    "        if \"conn_comp_cluster\" in nuc:\n",
    "            for con_comp in nuc[\"conn_comp_cluster\"]:\n",
    "                if con_comp not in con_comp_temp:\n",
    "                    con_comp_temp.append(con_comp)\n",
    "            del nuc[\"conn_comp_cluster\"]\n",
    "    # fix /general_params/loci/nucl_acc[]/conn_comp_cluster[]\n",
    "    if len(con_comp_temp) > 0:\n",
    "        data[\"general_params\"][\"loci\"][\"conn_comp_cluster\"] = con_comp_temp\n",
    "    # rename Polyketide, NRP, etc. to its lowercase version\n",
    "    rename_key(\"Polyketide\", \"polyketide\", data[\"general_params\"])\n",
    "    rename_key(\"NRP\", \"nrp\", data[\"general_params\"])\n",
    "    rename_key(\"RiPP\", \"ripp\", data[\"general_params\"])\n",
    "    rename_key(\"Terpene\", \"terpene\", data[\"general_params\"])\n",
    "    rename_key(\"Saccharide\", \"saccharide\", data[\"general_params\"])\n",
    "    rename_key(\"Alkaloid\", \"alkaloid\", data[\"general_params\"])\n",
    "    rename_key(\"Other\", \"other\", data[\"general_params\"])\n",
    "    for comp in data[\"general_params\"][\"compounds\"]:\n",
    "        # fix /general_params/compounds[]/database_deposited\n",
    "        if \"database_deposited\" in comp:\n",
    "            del comp[\"database_deposited\"]\n",
    "        # fix /general_params/compounds[]/databases_deposited[]\n",
    "        if \"databases_deposited\" in comp:\n",
    "            del comp[\"databases_deposited\"]\n",
    "    # fix /general_params/saccharide/gt_genes[]/sugar_subcluster[]\n",
    "    sugsub = []\n",
    "    if \"saccharide\" in data[\"general_params\"]:\n",
    "        if \"gt_genes\" in data[\"general_params\"][\"saccharide\"]:\n",
    "            for gtg in data[\"general_params\"][\"saccharide\"][\"gt_genes\"]:\n",
    "                if \"sugar_subcluster\" in gtg:\n",
    "                    sugsub.append(gtg[\"sugar_subcluster\"])\n",
    "                    del gtg[\"sugar_subcluster\"]\n",
    "        if len(sugsub) > 0:\n",
    "            data[\"general_params\"][\"saccharide\"][\"sugar_subclusters\"] = sugsub\n",
    "    return\n",
    "\n",
    "if not path.exists(\"../../preprocessed/p5-json/\"):\n",
    "    makedirs(\"../../preprocessed/p5-json/\")\n",
    "    \n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/p3-json/\")):\n",
    "    json_obj = None\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        match_attributes_to_schema_7(json_obj)\n",
    "        with open(path.join(\"../../preprocessed/p5-json/\", path.basename(json_path)), \"w\") as json_file:\n",
    "            json.dump(json_obj, json_file, indent=4, separators=(',', ': '))\n",
    "\n",
    "all_props_phase_5 = {}\n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/p5-json/\")):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        this_file_props = count_props(json_obj, \"\", {})\n",
    "        for prop in this_file_props:\n",
    "            if prop not in all_props_phase_5:\n",
    "                all_props_phase_5[prop] = [path.basename(json_path)]\n",
    "            else:\n",
    "                all_props_phase_5[prop].append(path.basename(json_path))\n",
    "\n",
    "with open(\"../../preprocessed/p4-mibig_schema_draft7.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_new_schema(json_obj, \"\", {})\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props_phase_5.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append(key)\n",
    "            print(\"{},{}\".format(key, len(all_props_phase_5[key])))\n",
    "    print(\"Number of conflicts: {}\".format(len(not_in_schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: validate data using JSON Schema V7 validator\n",
    "with open(\"../../preprocessed/p4-mibig_schema_draft7.json\") as json_file:\n",
    "    schema_obj = json.load(json_file)\n",
    "    validator = Draft7Validator(schema_obj)\n",
    "    errors = {}\n",
    "    errors_by_message = {}\n",
    "    for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/p5-json/\")):\n",
    "        bgc_id = path.basename(json_path)\n",
    "        break\n",
    "        print(\"Validating {}...\".format(bgc_id))\n",
    "        with open(json_path, \"r\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "            for error in sorted(validator.iter_errors(data), key=str):\n",
    "                # error by path\n",
    "                path_err = \".\".join([str(i) for i in list(error.schema_path)])\n",
    "                inst_err = \"\"\n",
    "                if not (isinstance(error.instance, dict) or isinstance(error.instance, list)):\n",
    "                    inst_err = str(error.instance)\n",
    "                if path_err not in errors:\n",
    "                    errors[path_err] = {\n",
    "                        \"files\": [],\n",
    "                        \"instances\": []\n",
    "                    }\n",
    "                if bgc_id not in errors[path_err][\"files\"]:\n",
    "                    errors[path_err][\"files\"].append(bgc_id)\n",
    "                if inst_err not in errors[path_err][\"instances\"]:\n",
    "                    errors[path_err][\"instances\"].append(inst_err)\n",
    "                # error by message\n",
    "                if error.message not in errors_by_message:\n",
    "                    errors_by_message[error.message] = []\n",
    "                if bgc_id not in errors_by_message[error.message]:\n",
    "                    errors_by_message[error.message].append(bgc_id)\n",
    "                    \n",
    "    with open(\"../../preprocessed/reports/p5-errors.tsv\", \"w\") as error_list:\n",
    "        for error in sorted(errors.keys(), reverse = True):\n",
    "            error_list.write(\"{}\\t{}\\t{}\\t{}\\n\".format(error, error.split(\".\")[-1], len(errors[error][\"files\"]), \";\".join(errors[error][\"instances\"])))\n",
    "            \n",
    "    with open(\"../../preprocessed/reports/p5-errors_by_message.tsv\", \"w\") as error_list:\n",
    "        for error in sorted(errors_by_message.keys(), reverse = True):\n",
    "            error_list.write(\"{}\\t{}\\n\".format(error, len(errors_by_message[error])))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track validated files so that we don't need to rerun them\n",
    "last_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file to store list of accessions without publications\n",
    "with open(\"../../preprocessed/reports/p5-accession_without_publications.tsv\", \"w\") as acc_list:\n",
    "    acc_list.write(\"\")\n",
    "\n",
    "# track genes not in gbk, but without any accession nor location information\n",
    "with open(\"../../preprocessed/reports/p5-accession_with_lost_genes.tsv\", \"w\") as acc_list:\n",
    "    acc_list.write(\"\")\n",
    "    \n",
    "# track mibig data without accessions\n",
    "with open(\"../../preprocessed/reports/p5-accession_with_lost_nucl_accs.tsv\", \"w\") as acc_list:\n",
    "    acc_list.write(\"\")\n",
    "    \n",
    "import re\n",
    "# open excel list of publications\n",
    "def fetch_publications(xls_file):\n",
    "    import xlrd\n",
    "    accession_and_publications = xlrd.open_workbook(xls_file)\n",
    "    worksheet = accession_and_publications.sheet_by_index(0)\n",
    "    pubs_and_new_accs = {}\n",
    "    i = 1\n",
    "    def value_exist(i, j):\n",
    "        try:\n",
    "            return worksheet.cell(i, j).value != xlrd.empty_cell.value\n",
    "        except:\n",
    "            return False\n",
    "    while value_exist(i, 0):\n",
    "        mibig_acc = worksheet.cell(i, 0).value\n",
    "        pubs = []\n",
    "        if worksheet.cell(i, 2).value != xlrd.empty_cell.value:\n",
    "            for pub in worksheet.cell(i, 2).value.split(\";\"):\n",
    "                pubs.append(pub)\n",
    "        new_accs = []\n",
    "        pubs_and_new_accs[mibig_acc] = (pubs, new_accs)\n",
    "        i += 1\n",
    "    return pubs_and_new_accs\n",
    "pubs_and_new_accs = {}#fetch_publications(\"../../docs/accession_and_publications.xls\")\n",
    "\n",
    "def update_pub_and_accs(data, pubs_and_new_accs):\n",
    "    mibig_acc = data[\"general_params\"][\"mibig_accession\"]\n",
    "    if mibig_acc in pubs_and_new_accs:\n",
    "        pubs, new_accs = pubs_and_new_accs[mibig_acc]\n",
    "        data[\"general_params\"][\"publications\"] = [pub for pub in pubs]\n",
    "    else:\n",
    "        if \"publications\" not in data[\"general_params\"]:\n",
    "            data[\"general_params\"][\"publications\"] = []\n",
    "        else:\n",
    "            pubs = []\n",
    "            for i, pub in enumerate(data[\"general_params\"][\"publications\"]):\n",
    "                for pubbb in pub.split(\";\"):\n",
    "                    for pubb in [s.strip().strip(\"'\").replace(\" \", \"\") for s in pubbb.split(\",\")]:\n",
    "                        if pubb.startswith(\"doi:\"):\n",
    "                            pubb = pubb[4:-1]\n",
    "                        if pubb.startswith(\"PMC\"):\n",
    "                            pubb = pubb[3:-1]\n",
    "                        if pubb.startswith(\"doi.org/\"):\n",
    "                            pubb = pubb[8:-1]\n",
    "                        if pubb == \"-\":\n",
    "                            continue\n",
    "                        if pubb == \"unpublished\":\n",
    "                            continue\n",
    "                        if len(pubb) > 0:\n",
    "                            pubs.append(pubb)\n",
    "            data[\"general_params\"][\"publications\"] = pubs\n",
    "\n",
    "    new_pub_list = []\n",
    "    for i, pub in enumerate(data[\"general_params\"][\"publications\"]):\n",
    "        if re.match(\"^(\\\\d+)$\", pub):\n",
    "            new_pub_list.append(\"pubmed:{}\".format(pub))\n",
    "        elif re.match(\"^10\\\\.\\\\d{4,9}/[-\\\\._;()/:a-zA-Z0-9]+$\", pub):\n",
    "            new_pub_list.append(\"doi:{}\".format(pub))\n",
    "        elif re.match(\"^([A-Z0-9]+)$\", pub):\n",
    "            new_pub_list.append(\"patent:{}\".format(pub))\n",
    "        elif re.match(\"^https?:\\\\/\\\\/(www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{2,256}\\\\.[a-z]{2,6}\\\\b([-a-zA-Z0-9@:%_\\\\+.~#?&//=]*)$\", pub):\n",
    "            new_pub_list.append(\"url:{}\".format(pub))\n",
    "            \n",
    "    data[\"general_params\"][\"publications\"] = new_pub_list\n",
    "\n",
    "    \n",
    "def update_positional_offset(data): # check which offset (zero-based? one-based) this entry used, then adjust to genbank style\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track mibig data without accessions\n",
    "with open(\"../../preprocessed/reports/p5-non_ncbi_accessions.tsv\", \"w\") as acc_list:\n",
    "    acc_list.write(\"\")\n",
    "\n",
    "import urllib.request\n",
    "def update_ncbi_accs(data):\n",
    "    ## mibig old json data is \"a bit\" messed up, e.g. many BGCs semi-predicted by antiSMASH\n",
    "    ## but didn't retained their positional information (look for the ones with startpos == -1\n",
    "    ## and endpos == -1). Moreover, some BGCs didn't have any NCBI accession available.\n",
    "    ## therefore, re-fetch data from the final.gbk instead\n",
    "    ## todo: check for MIxS compliance\n",
    "\n",
    "    new_accessions = []\n",
    "    mibig_id = data[\"general_params\"][\"mibig_accession\"]\n",
    "    whole_gbk = \"\"\n",
    "    try:\n",
    "        if mibig_id == \"BGC0000299\": # Alterochromides, ncbi accession is updated, so fetch directly from mibig instead\n",
    "            raise Exception()\n",
    "        with urllib.request.urlopen(\"https://mibig.secondarymetabolites.org/repository/{}/{}.1.final.gbk\".format(mibig_id, mibig_id)) as response:\n",
    "            whole_gbk = str(response.read()).replace(\" \", \"\").replace(\"\\\\n\", \"\")\n",
    "    except:\n",
    "        whole_gbk = \"VERSION{}.1\".format(mibig_id)\n",
    "        pass\n",
    "    match_gbks = re.findall(\"(theregionbetween(\\d+)-(\\d+)ntfrom){0,1}GenBankID([A-Z0-9_\\.]+)\\.\", whole_gbk)\n",
    "    if len(match_gbks) > 0:\n",
    "        for match_gbk in match_gbks:\n",
    "            loci = { \"accession\": match_gbk[-1] }\n",
    "            if len(match_gbk[0]) > 0:\n",
    "                loci[\"start_coord\"] = int(match_gbk[1])\n",
    "                loci[\"end_coord\"] = int(match_gbk[2])\n",
    "            new_accessions.append(loci)\n",
    "    else: # use mibig gbk\n",
    "        match_mibig_bgcs = re.findall(\"VERSION{}\\.(\\d)+\".format(mibig_id), whole_gbk)\n",
    "        for match_mibig_bgc in match_mibig_bgcs:\n",
    "            loci = { \"accession\": \"MIBIG:{}.{}\".format(mibig_id, match_mibig_bgc[-1]) }\n",
    "            with open(\"../../preprocessed/reports/p5-non_ncbi_accessions.tsv\", \"a\") as acc_list:\n",
    "                acc_list.write(\"{}.{}\\n\".format(mibig_id, match_mibig_bgc[-1]))\n",
    "            new_accessions.append(loci)\n",
    "    data[\"general_params\"][\"loci\"][\"nucl_acc\"] = new_accessions\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../preprocessed/reports/p5-multi_loci_bgcs.tsv\", \"w\") as of:\n",
    "    of.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: fix conflicts, then save to final output folder\n",
    "def fix_data_new_schema(data, error):\n",
    "    if len(error.path) < 1:\n",
    "        # problem is in root, need a separate approach\n",
    "        if error.validator == \"required\":\n",
    "            missing_keys = set(error.validator_value) - set(data.keys())\n",
    "            if \"personal\" in missing_keys:\n",
    "                data[\"personal\"] = {\n",
    "                    \"submitter_name\": \"mibig.secondarymetabolites.org\",\n",
    "                    \"submitter_institution\": \"MIBiG\",\n",
    "                    \"submitter_email\": \"info@mibig.secondarymetabolites.org\"\n",
    "                }\n",
    "            if \"changelogs\" in missing_keys:\n",
    "                data[\"changelogs\"] = [{\n",
    "                    \"version\": \"2.0\",\n",
    "                    \"comment\": \"Submitted\"\n",
    "                }]\n",
    "    else:        \n",
    "        # get problematic parent instance from data (so that we can fix it)\n",
    "        error_container = data\n",
    "        error_container_parent = None # for catching grandparent\n",
    "        error_container_attribute = None\n",
    "        while len(error.path) > 1:\n",
    "            if len(error.path) == 2:\n",
    "                error_container_parent = error_container\n",
    "            error_container_attribute = error.path.popleft()\n",
    "            error_container = error_container[error_container_attribute] # parent node containing the error instance\n",
    "        error_attribute = error.path.popleft() # attribute from parent node containing the error instance\n",
    "\n",
    "        if isinstance(error_container, ToDelete):\n",
    "            return\n",
    "        elif isinstance(error_container[error_attribute], ToDelete):\n",
    "            return\n",
    "        \n",
    "        # function to replace attribute values\n",
    "        def replace_attr(attr_pairs):\n",
    "            for attr_from, attr_to in attr_pairs:\n",
    "                if error.instance == attr_from:\n",
    "                    error_container[error_attribute] = attr_to\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        # fix type errors (should be generalizable)\n",
    "        if error.validator == \"type\":\n",
    "            if error.validator_value == \"integer\":\n",
    "                try:\n",
    "                    error_container[error_attribute] = int(error.instance)\n",
    "                    return\n",
    "                except:\n",
    "                    # may need tailored fix\n",
    "                    pass\n",
    "            elif error.validator_value == \"number\":\n",
    "                try:\n",
    "                    error_container[error_attribute] = float(error.instance)\n",
    "                    return\n",
    "                except:\n",
    "                    # may need tailored fix\n",
    "                    pass\n",
    "            elif error.validator_value == \"string\":\n",
    "                # fix mut_pheno, error list shown that these are None/null values, delete the attribute instead\n",
    "                if error_attribute == \"mut_pheno\":\n",
    "                    del error_container[error_attribute]\n",
    "                # fix gene_annotation, name, id\n",
    "                elif error_attribute == \"gene_id\":\n",
    "                    if len(set([\"gene_name\"]).intersection(error_container.keys())) == 1:\n",
    "                        # it is a custom annotation\n",
    "                        del error_container[error_attribute]\n",
    "                    else:\n",
    "                        mibig_acc = data[\"general_params\"][\"mibig_accession\"]\n",
    "                        with open(\"../../preprocessed/reports/p5-accession_with_lost_genes.tsv\", \"a\") as acc_list:\n",
    "                            acc_list.write(\"1\\t{}\\t{}\\n\".format(mibig_acc, \", \".join([\"{}:{}\".format(k, v) for k, v in error_container_parent[error_container_attribute].items() if isinstance(v, str)])))\n",
    "                        # for now, delete the gene (?)\n",
    "                        error_container_parent[error_container_attribute] = ToDelete()\n",
    "                elif error_attribute == \"gene_annotation\":\n",
    "                    del error_container[error_attribute]                    \n",
    "                elif error_attribute == \"gene_name\":\n",
    "                    if \"gene_id\" in error_container:\n",
    "                        del error_container[error_attribute]\n",
    "                    else:\n",
    "                        mibig_acc = data[\"general_params\"][\"mibig_accession\"]\n",
    "                        with open(\"../../preprocessed/reports/p5-accession_with_lost_genes.tsv\", \"a\") as acc_list:\n",
    "                            acc_list.write(\"3\\t{}\\t{}\\n\".format(mibig_acc, \", \".join([\"{}:{}\".format(k, v) for k, v in error_container_parent[error_container_attribute].items() if isinstance(v, str)])))\n",
    "                        # for now, delete the gene (?)\n",
    "                        error_container_parent[error_container_attribute] = ToDelete()\n",
    "                elif error_attribute == \"gene_function\":\n",
    "                    error_container[error_attribute] = \"Unknown\"\n",
    "                    if \"evidence_genefunction\" in error_container:\n",
    "                        error_container[\"evidence_genefunction\"] = ToDelete()\n",
    "                # fix comments, gene_comments\n",
    "                elif error_attribute in [\"comments\", \"gene_comments\"]:\n",
    "                    error_container[error_attribute] = \"\"\n",
    "\n",
    "        # fix minimum errors (generalizable, but needs to be careful)\n",
    "        elif error.validator == \"minimum\":\n",
    "            # fix gene.startpos\n",
    "            if error_attribute == \"gene_startpos\":\n",
    "                error_container[error_attribute] = ToDelete()\n",
    "                if \"gene_endpos\" in error_container:\n",
    "                    error_container[\"gene_endpos\"] = ToDelete()\n",
    "            # fix gene.endpos\n",
    "            if error_attribute == \"gene_endpos\":\n",
    "                error_container[error_attribute] = ToDelete()\n",
    "                if \"gene_startpos\" in error_container:\n",
    "                    error_container[\"gene_startpos\"] = ToDelete()\n",
    "\n",
    "        # fix minItems errors (generalizable but limited e.g. need to consider 'required')\n",
    "        elif error.validator == \"minItems\":\n",
    "            # fix operon_genes = [], then delete operon\n",
    "            if error_attribute == \"operon_genes\":\n",
    "                del data[\"general_params\"][\"genes\"][\"operon\"]\n",
    "            # fix ripp..gene_id = []\n",
    "            elif error_attribute == \"gene_id\":\n",
    "                del data[\"general_params\"][\"ripp\"][\"precursor_loci\"]\n",
    "                data[\"general_params\"][\"minimal\"] = True\n",
    "            # fix publications\n",
    "            elif error_attribute == \"publications\":\n",
    "                mibig_acc = data[\"general_params\"][\"mibig_accession\"]\n",
    "                print(\"Missing publication: {}\".format(mibig_acc))\n",
    "                with open(\"../../preprocessed/reports/p5-accession_without_publications.tsv\", \"a\") as acc_list:\n",
    "                    acc_list.write(\"{}\\t{}\\n\".format(mibig_acc, data[\"general_params\"][\"compounds\"][0][\"compound\"]))\n",
    "                # for now, fill with empty string (let's tackle other issues first)\n",
    "                error_container[error_attribute] = [\"pubmed:0\"]\n",
    "\n",
    "                \n",
    "        # fix enum errors (semi-generalizable, needs to know what to put in place of the wrong value)\n",
    "        elif error.validator == \"enum\":\n",
    "            # fix biosyn_class\n",
    "            if error_container_attribute == \"biosyn_class\":\n",
    "                error_container[error_attribute] = \"Other\"\n",
    "                if \"other\" not in data[\"general_params\"]:\n",
    "                    data[\"general_params\"][\"other\"] = {}\n",
    "                data[\"general_params\"][\"other\"][\"other_subclass\"] = error.instance\n",
    "            # fix genes..gene_function\n",
    "            if error_attribute == \"gene_function\":\n",
    "                attr_pairs = [\n",
    "                    (\"\", \"Unknown\"),\n",
    "                    (\"None\", \"Unknown\"),\n",
    "                    (\"Scaffold Biosynthesis\", \"Scaffold biosynthesis\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Unknown\"\n",
    "                    pass\n",
    "                if error_container[error_attribute] == \"Unknown\":       \n",
    "                    if \"evidence_genefunction\" in error_container:\n",
    "                        error_container[\"evidence_genefunction\"] = ToDelete()\n",
    "            # fix genes..tailoring\n",
    "            if error_attribute == \"tailoring\":\n",
    "                if error_container.get(\"gene_function\") == \"Tailoring\":\n",
    "                    attr_pairs = [\n",
    "                        (\"None\", \"Unknown\"),\n",
    "                    ]\n",
    "                    if not replace_attr(attr_pairs):\n",
    "                        # let it be, we will update the schema instead\n",
    "                        pass\n",
    "                else: # we don't need \"tailoring\"\n",
    "                    error_container[error_attribute] = ToDelete()\n",
    "            # fix genes..evidence_genefunction\n",
    "            if error_container_attribute == \"evidence_genefunction\":\n",
    "                evidences = []\n",
    "                errorinstance = error.instance.replace(\" \", \"\").replace(\"-\", \"\").lower()\n",
    "                if \"activityassay\" in errorinstance:\n",
    "                    evidences.append(\"Activity assay\")\n",
    "                if \"knockout\" in errorinstance:\n",
    "                    evidences.append(\"Knock-out\")\n",
    "                if \"invivo\" in errorinstance:\n",
    "                    evidences.append(\"Other in vivo study\")\n",
    "                if \"sequence\" in errorinstance:\n",
    "                    evidences.append(\"Sequence-based prediction\")\n",
    "                if \"expression\" in errorinstance:\n",
    "                    evidences.append(\"Heterologous expression\")\n",
    "                if len(evidences) < 1:\n",
    "                    evidences.append(error.instance) # let it be, we will update the schema instead\n",
    "                error_container[error_attribute] = evidences[0]\n",
    "                if len(evidences) > 1:\n",
    "                    error_container.extend(evidences)\n",
    "            # fix loci..complete\n",
    "            elif error_attribute == \"complete\":\n",
    "                if error.instance == \"partial\":\n",
    "                    error_container[error_attribute] = \"incomplete\"\n",
    "            # fix loci..conn_comp_cluster\n",
    "            elif error_container_attribute == \"conn_comp_cluster\":\n",
    "                attr_pairs = [\n",
    "                    (\"Proven expression in natural host\", \"Gene expression correlated with compound production\"),\n",
    "                    (\"Knock-outstudies\", \"Knock-out studies\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix compounds..chem_act\n",
    "            elif error_container_attribute == \"chem_act\":\n",
    "                attr_pairs = [\n",
    "                    (\"\", \"Unknown\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    error_container_parent[\"other_chem_act\"] = error.instance # later on, we will update this\n",
    "                    pass\n",
    "            # fix compounds..chem_moiety\n",
    "            elif error_attribute == \"chem_moiety\":\n",
    "                attr_pairs = []\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    error_container[\"other_chem_moiety\"] = error.instance # later on, we will update this\n",
    "                    pass                \n",
    "            # fix compounds.mass_ion_type\n",
    "            elif error_attribute == \"mass_ion_type\":\n",
    "                # let it be, we will update the schema instead\n",
    "                pass\n",
    "            # fix polyketide.pks_subclass\n",
    "            elif error_container_attribute == \"pks_subclass\":\n",
    "                attr_pairs = [\n",
    "                    (\"Type I\", \"Modular type I\"), # (aculeximycin)\n",
    "                    (\"Iterative typeI\", \"Iterative type I\"),\n",
    "                    (\"Modular Type I\", \"Modular type I\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    # should be empty\n",
    "                    pass\n",
    "            # fix polyketide.starter_unit\n",
    "            elif error_attribute == \"starter_unit\":\n",
    "                if error.instance == \"None\":\n",
    "                    error_container[error_attribute] = ToDelete()\n",
    "                # else, we will keep for schema update\n",
    "            # fix polyketide.pks_te_type\n",
    "            elif error_attribute == \"pks_te_type\":\n",
    "                attr_pairs = [\n",
    "                    (\"other\", \"Other\"),\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    # should be empty\n",
    "                    pass\n",
    "            # fix polyketide..pks_domains\n",
    "            elif error_container_attribute == \"pks_domains\":\n",
    "                attr_pairs = [\n",
    "                    (\"AT\", \"Acyltransferase\"),\n",
    "                    (\"DH\", \"Dehydratase\"),\n",
    "                    (\"KR\", \"Ketoreductase\"),\n",
    "                    (\"ACP\", \"Thiolation (ACP/PCP)\"),\n",
    "                    (\"PCP\", \"Thiolation (ACP/PCP)\"),\n",
    "                    (\"T\", \"Thiolation (ACP/PCP)\"),\n",
    "                    (\"CAL\", \"CoA-ligase\"),\n",
    "                    (\"ER\", \"Enoylreductase\"),\n",
    "                    (\"KS\", \"Ketosynthase\"),\n",
    "                    (\"FAAL\", \"CoA-ligase\"),\n",
    "                    (\"CMET\", \"Methyltransferase\"),\n",
    "                    (\"TE/CLC\", \"Thioesterase\"),\n",
    "                    (\"TE\", \"Thioesterase\"),\n",
    "                    (\"SulphurT\", \"Sulfotransferase\"),\n",
    "                    (\"ST\", \"Sulfotransferase\"),\n",
    "                    (\"PT\", \"Product Template domain\"),\n",
    "                    (\"C\", \"Condensation\"),\n",
    "                    (\"A\", \"Adenlyation\"),\n",
    "                    (\"E\", \"Epimerization\"),\n",
    "                    (\"PPTASE\", \"Phosphopantetheinyl transferase\"),\n",
    "                    (\"SAT\", \"ACP transacylase\"),\n",
    "                    (\"TR\", \"Thiol reductase\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    #should be empty\n",
    "                    pass\n",
    "            # fix mod_pks_genes..kr_stereochem, A->S->D-OH, B->R->L-OH\n",
    "            elif error_attribute == \"kr_stereochem\":\n",
    "                attr_pairs = [\n",
    "                    (\"A-group\", \"D-OH\"),\n",
    "                    (\"B-group\", \"L-OH\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    #should be empty\n",
    "                    pass\n",
    "            # fix mod_pks_genes..at_substr_spec\n",
    "            elif error_attribute == \"at_substr_spec\":\n",
    "                attr_pairs = [\n",
    "                    (\"malonyl-CoA\", \"Malonyl-CoA\"),\n",
    "                    (\"methylmalonyl-CoA\", \"Methylmalonyl-CoA\"),\n",
    "                    (\"Malonyl-CoA/Malonyl-CoA/Malonyl-CoA\", \"Malonyl-CoA\"),\n",
    "                    (\"Methylmalonyl-CoA/Methylmalonyl-CoA\", \"Methylmalonyl-CoA\"),\n",
    "                    (\"N/A\", \"None\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    if error.instance == \"Acetyl-CoA/Methylmalonyl-CoA\":\n",
    "                        error_container[error_attribute] = \"Multiple (promiscuous)\"\n",
    "                        error_container[\"at_multiple_spec\"] = [\"Acetyl-CoA\", \"Methylmalonyl-CoA\"]\n",
    "                    elif error.instance == \"Methylmalonyl-CoA/Malonyl-CoA\":\n",
    "                        error_container[error_attribute] = \"Multiple (promiscuous)\"\n",
    "                        error_container[\"at_multiple_spec\"] = [\"Malonyl-CoA\", \"Methylmalonyl-CoA\"]\n",
    "                    elif error.instance == \"Acetyl-CoA + Malonyl CoA\":\n",
    "                        error_container[error_attribute] = \"Multiple (promiscuous)\"\n",
    "                        error_container[\"at_multiple_spec\"] = [\"Malonyl-CoA\", \"Acetyl-CoA\"]\n",
    "                    else:\n",
    "                        # what to do? 4-hydroxyphenylpyruvate;Various atypical acyl-CoAs;phenylacetate-like;Decanoyl-CoA\n",
    "                        # let it be, we'll update the schema instead\n",
    "                        pass\n",
    "            # fix mod_pks_genes..evidence_at_spec\n",
    "            elif error_attribute == \"evidence_at_spec\":\n",
    "                attr_pairs = [\n",
    "                    #(\"Feeding study\", \"Other\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    # let it be, we'll update the schema instead\n",
    "                    pass\n",
    "            # fix mod_pks_genes..pks_mod_doms\n",
    "            elif error_container_attribute == \"pks_mod_doms\":\n",
    "                attr_pairs = []\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    error_container_parent[\"pks_other_mod_dom\"] = error.instance # we'll update the schema later\n",
    "                    pass\n",
    "            # fix nrps_modules..nrps_mod_doms\n",
    "            elif error_attribute == \"nrps_mod_doms\":\n",
    "                attr_pairs = []\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    error_container[\"nrps_other_mod_dom\"] = error.instance # we'll update the schema later\n",
    "                    pass\n",
    "            # fix nrps_modules..prot_adom_spec\n",
    "            elif error_attribute == \"prot_adom_spec\":\n",
    "                if error.instance == \"Asparigine\":\n",
    "                    error_container[error_attribute] = \"Asparagine\"\n",
    "            # fix nrp..cdom_subtype\n",
    "            elif error_attribute == \"cdom_subtype\":\n",
    "                if error.instance in [\"N/A\", \"None\"]:\n",
    "                    error_container[error_attribute] = ToDelete()\n",
    "            # fix nrp..nonprot_adom_spec\n",
    "            elif error_attribute == \"nonprot_adom_spec\":\n",
    "                error_container[error_attribute] = \"Other\"\n",
    "                error_container[\"other_spec\"] = error.instance # we'll fix the schema later on\n",
    "            # fix ripp..ripp_subclass\n",
    "            elif error_attribute == \"ripp_subclass\":\n",
    "                attr_pairs = [\n",
    "                    (\"Lantipeptide\", \"Lanthipeptide\"),\n",
    "                    (\"Head-To-Tail Cyclized Peptide\", \"Head-to-tailcyclized peptide\"),\n",
    "                    (\"Lap\", \"LAP\"),\n",
    "                    (\"Lap / Microcin\", \"LAP\"),\n",
    "                    (\"Lasso Peptide\", \"Lassopeptide\"),\n",
    "                    (\"None\", \"Unknown\") ## this is not available atm, but we'll fix that later\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    # should be empty\n",
    "                    pass\n",
    "            # fix ripp..lin_cycl_ripp\n",
    "            elif error_attribute == \"lin_cycl_ripp\":\n",
    "                attr_pairs = [\n",
    "                    (\"linear\", \"Linear\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    # should be empty\n",
    "                    pass\n",
    "            # fix saccharide..saccharide_subclass\n",
    "            elif error_attribute == \"saccharide_subclass\":\n",
    "                if error.instance == \"hyrbid/tailoring\":\n",
    "                    error_container[error_attribute] = \"hybrid/tailoring\"\n",
    "            # fix saccharide..gt_specificity\n",
    "            elif error_attribute == \"gt_specificity\":\n",
    "                if error.instance == \"None\":\n",
    "                    error_container[error_attribute] = \"Unknown\"\n",
    "                else:\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    error_container[\"other_gt_spec\"] = error.instance # we'll fix this later\n",
    "            # fix saccharide..evidence_gt_spec\n",
    "            elif error_attribute == \"evidence_gt_spec\":\n",
    "                if error.instance == \"structure-based inference\":\n",
    "                    error_container[error_attribute] = \"Structure-based inference\"                \n",
    "            # fix other.other_subclass\n",
    "            elif error_attribute == \"other_subclass\":\n",
    "                error_container[error_attribute] = \"Unknown\" # it's either other/none, basically they have no idea what the class is\n",
    "            ## try to fix \"None\"/\"N/A\"/\"\" --> \"Unknown\"\n",
    "            else:\n",
    "                use_others_instead = [\n",
    "                    \"crosslink_type\",\n",
    "                    \"evidence_a_spec\",\n",
    "                    \"nrps_evidence_skip_iter\",\n",
    "                    \"terpene_subclass\",\n",
    "                    \"terpene_c_len\",\n",
    "                    \"terpene_precursor\",\n",
    "                    \"pk_subclass\",\n",
    "                    \"subclass\",\n",
    "                    \"pks_evidence_skip_iter\"\n",
    "                ]\n",
    "                if error_attribute in use_others_instead:\n",
    "                    # keep them, we'll fix the schema instead\n",
    "                    pass\n",
    "                else:\n",
    "                    attr_to_use = \"Unknown\"\n",
    "                    attr_pairs = [\n",
    "                        (\"N/A\", attr_to_use),\n",
    "                        (\"None\", attr_to_use),\n",
    "                        (\"\", attr_to_use)\n",
    "                    ]\n",
    "                    if not replace_attr(attr_pairs):\n",
    "                        # keep them, we'll fix the schema instead\n",
    "                        pass\n",
    "                \n",
    "        # fix requirement errors (needs hand-on approach)        \n",
    "        elif error.validator == \"required\":\n",
    "            missing_keys = set(error.validator_value) - set(error_container[error_attribute].keys())\n",
    "            # fix loci (only fix BGCs with a single loci. for multi-loci, put them into the \"retired\" list)\n",
    "            if \"accession\" in missing_keys:\n",
    "                if len(data[\"general_params\"][\"loci\"][\"nucl_acc\"]) > 1:\n",
    "                    mibig_acc = data[\"general_params\"][\"mibig_accession\"]\n",
    "                    all_locis = []\n",
    "                    for loci in data[\"general_params\"][\"loci\"][\"nucl_acc\"]:\n",
    "                        acc_and_loc = loci[\"accession\"]\n",
    "                        if \"start_coord\" in loci and \"end_coord\" in loci:\n",
    "                            acc_and_loc = \"{}({}-{})\".format(acc_and_loc, loci[\"start_coord\"], loci[\"end_coord\"])\n",
    "                        all_locis.append(acc_and_loc)\n",
    "                    with open(\"../../preprocessed/reports/p5-multi_loci_bgcs.tsv\", \"a\") as of:\n",
    "                        of.write(\"{}\\t{}\\n\".format(bgc_id, \"; \".join(all_locis)))\n",
    "                for nkey in data[\"general_params\"][\"loci\"][\"nucl_acc\"][0]:\n",
    "                    data[\"general_params\"][\"loci\"][nkey] = data[\"general_params\"][\"loci\"][\"nucl_acc\"][0][nkey]\n",
    "                del data[\"general_params\"][\"loci\"][\"nucl_acc\"]\n",
    "                    \n",
    "                    \n",
    "            # fix saccharide subclasses\n",
    "            if \"saccharide\" in missing_keys:\n",
    "                # if PKS+Saccharide, assume it is a tailoring GT\n",
    "                if \"Polyketide\" in data[\"general_params\"][\"biosyn_class\"]:\n",
    "                    data[\"general_params\"][\"saccharide\"] = { \"saccharide_subclass\": \"hybrid/tailoring\" }\n",
    "                # if NR+Saccharide, assume it is a tailoring GT\n",
    "                elif \"NRP\" in data[\"general_params\"][\"biosyn_class\"]:\n",
    "                    data[\"general_params\"][\"saccharide\"] = { \"saccharide_subclass\": \"hybrid/tailoring\" }\n",
    "            # fix other subclasses\n",
    "            if \"other\" in missing_keys:\n",
    "                data[\"general_params\"][\"other\"] = {\"other_subclass\": \"Unknown\"} # we'll fix that later\n",
    "            # fix nrp subclasses\n",
    "            if \"nrp\" in missing_keys:\n",
    "                # set minimal = true\n",
    "                data[\"general_params\"][\"minimal\"] = True\n",
    "            # fix polyketide subclasses\n",
    "            if \"polyketide\" in missing_keys:\n",
    "                # set minimal = true\n",
    "                data[\"general_params\"][\"minimal\"] = True\n",
    "            # fix publication\n",
    "            if \"publications\" in missing_keys:\n",
    "                if \"accession\" in data[\"general_params\"][\"loci\"]: # otherwise, don't bother\n",
    "                    ncbi_acc = data[\"general_params\"][\"loci\"][\"accession\"]\n",
    "                    def fix_publications(acc_pub_pairs):\n",
    "                        for ncbi_accs, pubs in acc_pub_pairs:\n",
    "                            if ncbi_acc in ncbi_accs:\n",
    "                                error_container[error_attribute][\"publications\"] = pubs\n",
    "                                return True\n",
    "                        return False\n",
    "                    acc_pub_pairs = []\n",
    "                    if not fix_publications(acc_pub_pairs):\n",
    "                        mibig_acc = data[\"general_params\"][\"mibig_accession\"]\n",
    "                        print(\"Missing publication: {}\".format(mibig_acc))\n",
    "                        with open(\"../../preprocessed/reports/p5-accession_without_publications.tsv\", \"a\") as acc_list:\n",
    "                            acc_list.write(\"{}\\t{}\\n\".format(mibig_acc, data[\"general_params\"][\"compounds\"][0][\"compound\"]))\n",
    "                        # for now, fill with empty publication\n",
    "                        error_container[error_attribute][\"publications\"] = [\"pubmed:0\"]\n",
    "                    \n",
    "            # fix compounds.other_chem_act, remove the entry of \"Other\" from \"chem_act\"\n",
    "            if \"other_chem_act\" in missing_keys:\n",
    "                if \"chem_act\" in error_container[error_attribute]:\n",
    "                    for i, act in enumerate(error_container[error_attribute][\"chem_act\"]):\n",
    "                        if act == \"Other\":\n",
    "                            error_container[error_attribute][\"chem_act\"][i] = ToDelete()\n",
    "                error_container[error_attribute][\"other_chem_act\"] = ToDelete()\n",
    "            # fix compounds..other_chem_moiety\n",
    "            if \"other_chem_moiety\" in missing_keys:\n",
    "                error_container[error_attribute] = ToDelete()\n",
    "            # fix loci.evidence_struct, set to 'Other'\n",
    "            if \"conn_comp_cluster\" in missing_keys:\n",
    "                error_container[error_attribute][\"conn_comp_cluster\"] = [\"Unknown\"] # we'll fix this later\n",
    "            # fix loci.complete, set to 'unknown'\n",
    "            if \"complete\" in missing_keys:\n",
    "                error_container[error_attribute][\"complete\"] = \"unknown\"\n",
    "            # fix genes..tailoring\n",
    "            if \"tailoring\" in missing_keys:\n",
    "                error_container[error_attribute][\"tailoring\"] = \"Unknown\"\n",
    "            # fix genes..gene_function\n",
    "            if \"gene_function\" in missing_keys:\n",
    "                error_container[error_attribute][\"gene_function\"] = \"Unknown\"\n",
    "                if \"evidence_genefunction\" in error_container[error_attribute]:\n",
    "                    error_container[error_attribute][\"evidence_genefunction\"] = ToDelete()\n",
    "            # fix genes..evidence_genefunction\n",
    "            if \"evidence_genefunction\" in missing_keys:\n",
    "                if \"mut_pheno\" in error_container[error_attribute]:\n",
    "                    error_container[error_attribute][\"evidence_genefunction\"] = [\"Knock-out\"]\n",
    "                else:\n",
    "                    error_container[error_attribute][\"evidence_genefunction\"] = [\"Unknown\"]\n",
    "            # fix genes..evidence_operon\n",
    "            if \"evidence_operon\" in missing_keys:\n",
    "                error_container[error_attribute][\"evidence_operon\"] = \"Unknown\" # should be fixed later\n",
    "            # fix saccharide..evidence_gt_spec\n",
    "            if \"evidence_gt_spec\" in missing_keys:\n",
    "                error_container[error_attribute][\"evidence_gt_spec\"] = \"Unknown\" # should be fixed later\n",
    "            # fix saccharide..gt_gene\n",
    "            if \"gt_gene\" in missing_keys:\n",
    "                error_container_parent[error_container_attribute] = ToDelete()\n",
    "            # fix ripp..precursor_loci by setting minimal=True (no good solution otherwise)\n",
    "            if \"precursor_loci\" in missing_keys:\n",
    "                data[\"general_params\"][\"minimal\"] = True\n",
    "                \n",
    "        # fix pattern errors (needs hand-on approach)        \n",
    "        elif error.validator == \"pattern\":\n",
    "            # fix pks and nrp module_nr\n",
    "            if error_attribute == \"module_nr\":\n",
    "                # X/x, let's delete it for now (?)\n",
    "                error_container[error_attribute] = ToDelete()\n",
    "            elif error_attribute == \"accession\": # ncbi accession\n",
    "                accession = error_container[error_attribute]\n",
    "                attr_pairs = {\n",
    "                    \"    KF899892\": \"KF899892\",\n",
    "                    \"KE145356.1 \": \"KE145356.1\",\n",
    "                    \"MQUP01000022 \": \"MQUP01000022\",\n",
    "                    \"MG837518.1 \": \"MG837518.1\",\n",
    "                    \"MG837519.1 \": \"MG837519.1\",\n",
    "                    \"MG837520.1 \": \"MG837520.1\",\n",
    "                    \"MG837521.1 \": \"MG837521.1\",\n",
    "                    \"MG837522.1 \": \"MG837522.1\",\n",
    "                    \"MG837524.1 \": \"MG837524.1\",\n",
    "                    \"NZ_PKFQ01000001 \": \"NZ_PKFQ01000001\",\n",
    "                    \"MG266907 \": \"MG266907\",\n",
    "                    \"MG459168 \": \"MG459168\"\n",
    "                }\n",
    "                if accession in attr_pairs:\n",
    "                    error_container[error_attribute] = attr_pairs[accession]\n",
    "                else:\n",
    "                    # mark as missing, fill placeholder string in place\n",
    "                    with open(\"../../preprocessed/reports/p5-accession_with_lost_nucl_accs.tsv\", \"a\") as acc_list:\n",
    "                        acc_list.write(\"{}\\t{}\\n\".format(data[\"general_params\"][\"mibig_accession\"], accession))\n",
    "                    error_container[error_attribute][\"accession\"] = \"MIBIG:{}\".format(data[\"general_params\"][\"mibig_accession\"])\n",
    "                    pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_enum(input_dict, cur_path, result):\n",
    "    key_path = cur_path\n",
    "    if (\"type\" not in input_dict) or (input_dict[\"type\"] not in [\"object\", \"array\"]):\n",
    "        key_path = \"{}\".format(cur_path) # string / etc.\n",
    "    elif input_dict[\"type\"] == \"object\":\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            result = fetch_enum(input_dict[\"properties\"][key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif input_dict[\"type\"] == \"array\":\n",
    "        key_path = \"{}[]\".format(cur_path)\n",
    "        result = fetch_enum(input_dict[\"items\"], \"{}\".format(key_path), result)\n",
    "    \n",
    "    if key_path not in result and \"enum\" in input_dict:\n",
    "        result[key_path] = {}\n",
    "        for enum_val in input_dict[\"enum\"]:\n",
    "            result[key_path][enum_val] = []\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../preprocessed/reports/p5-unknown_enum_options.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tpath\\tadded_enum\\tdecision\\n\")\n",
    "    \n",
    "def update_schema_enum(bgc_id, schema_obj, error):\n",
    "    if error.validator == \"enum\":\n",
    "        enum_container = schema_obj\n",
    "        schema_path = error.schema_path\n",
    "        schema_path_string = \".\".join(schema_path)\n",
    "        while len(schema_path) > 0:\n",
    "            enum_container = enum_container[schema_path.popleft()]\n",
    "        # update the schema enum\n",
    "        if error.instance not in enum_container:\n",
    "            val = error.instance\n",
    "            decision = \"\"\n",
    "            if error.instance in [\"None\", \"Unknown\"]:\n",
    "                decision = \"approve\"\n",
    "            else:\n",
    "                decision = \"retire\"\n",
    "                \n",
    "            with open(\"../../preprocessed/reports/p5-unknown_enum_options.tsv\", \"a\") as o:\n",
    "                o.write(\"{}\\t{}\\t{}\\t{}\\n\".format(bgc_id, schema_path_string, val, decision))\n",
    "                \n",
    "            if decision == \"approve\":\n",
    "                enum_container.append(val)\n",
    "                return True\n",
    "            elif decision == \"retire\":                \n",
    "                return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_contain_others = []\n",
    "with open(\"../../preprocessed/reports/p5-enum_contain_others.tsv\", \"w\") as o:\n",
    "    o.write(\"\")\n",
    "def check_and_remove_other(keys, input_dict, attribute, parent, grandparent):\n",
    "    if isinstance(input_dict, list):\n",
    "        for i, node in enumerate(input_dict):\n",
    "            check_and_remove_other(keys, node, i, input_dict, parent)\n",
    "    elif isinstance(input_dict, dict):\n",
    "        for k in input_dict:\n",
    "            check_and_remove_other(\"{}/{}\".format(keys, k), input_dict[k], k, input_dict, parent)\n",
    "        for toDel in [\"nrps_other_mod_dom\", \"other_chem_act\", \"other_chem_moiety\", \"other_gt_spec\", \"pks_other_mod_dom\", \"other_spec\"]:\n",
    "            input_dict.pop(toDel, None)\n",
    "    else:\n",
    "        val = str(input_dict)\n",
    "        if val.lower().rstrip().lstrip() in [\"other\", \"others\"]:\n",
    "            if keys not in data_contain_others:\n",
    "                data_contain_others.append(keys)\n",
    "            if keys == \"/general_params/nrp/nrps_genes/nrps_module/nrps_mod_doms\":\n",
    "                parent[attribute] = parent.get(\"nrps_other_mod_dom\", \"Unknown\")\n",
    "            elif keys == \"/general_params/compounds/chem_act\":\n",
    "                parent[attribute] = grandparent.get(\"other_chem_act\", \"Unknown\")\n",
    "            elif keys == \"/general_params/compounds/chem_moieties/chem_moiety\":\n",
    "                parent[attribute] = parent.get(\"other_chem_moiety\", \"Unknown\")\n",
    "            elif keys == \"/general_params/saccharide/gt_genes/gt_specificity\":\n",
    "                parent[attribute] = parent.get(\"other_gt_spec\", \"Unknown\")\n",
    "            elif keys == \"/general_params/polyketide/mod_pks_genes/pks_module/pks_mod_doms\":\n",
    "                parent[attribute] = grandparent.get(\"pks_other_mod_dom\", \"Unknown\")\n",
    "            elif keys == \"/general_params/nrp/nrps_genes/nrps_module/a_substr_spec/nonprot_adom_spec\":\n",
    "                parent[attribute] = parent.get(\"other_spec\", \"Unknown\")\n",
    "            elif keys == \"/general_params/polyketide/pks_subclass\":\n",
    "                pass\n",
    "            elif keys == \"/general_params/polyketide/pk_subclass\":\n",
    "                pass\n",
    "            elif keys == \"/general_params/biosyn_class\":\n",
    "                pass\n",
    "            else:\n",
    "                parent[attribute] = \"Unknown\"\n",
    "        elif val.lower().rstrip().lstrip() == \"n/a\":\n",
    "            parent[attribute] = \"Unknown\"\n",
    "        elif val.lower().rstrip().lstrip() == \"none\":\n",
    "            parent[attribute] = \"None\"\n",
    "        elif val.lower().rstrip().lstrip() == \"unknown\":\n",
    "            parent[attribute] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_other_from_schema(schema):\n",
    "    if \"other_spec\" not in schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"properties\"][\"a_substr_spec\"][\"properties\"]:\n",
    "        return # is done\n",
    "    # [\"nrps_other_mod_dom\", \"other_chem_act\", \"other_chem_moiety\", \"other_gt_spec\", \"pks_other_mod_dom\", \"other_spec\"]\n",
    "    del schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"properties\"][\"nrps_other_mod_dom\"]\n",
    "    del schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"other_chem_act\"]\n",
    "    del schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"allOf\"]\n",
    "    del schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"chem_moieties\"][\"items\"][\"properties\"][\"other_chem_moiety\"]\n",
    "    del schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"chem_moieties\"][\"items\"][\"allOf\"]\n",
    "    del schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"properties\"][\"other_gt_spec\"]\n",
    "    del schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"allOf\"][0]\n",
    "    del schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"properties\"][\"mod_pks_genes\"][\"items\"][\"properties\"][\"pks_module\"][\"items\"][\"properties\"][\"pks_other_mod_dom\"]\n",
    "    del schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"properties\"][\"a_substr_spec\"][\"properties\"][\"other_spec\"]\n",
    "    def remove_other_from_enum(schema):\n",
    "        if isinstance(schema, dict):\n",
    "            for key in schema:\n",
    "                if key == \"allOf\":\n",
    "                    continue\n",
    "                elif key in [\"pks_subclass\", \"pk_subclass\", \"biosyn_class\"]:\n",
    "                    continue\n",
    "                elif key == \"enum\":\n",
    "                    if \"Other\" in schema[key]:\n",
    "                        new_enums = []\n",
    "                        for enum in schema[key]:\n",
    "                            if enum != \"Other\":\n",
    "                                new_enums.append(enum)\n",
    "                        schema[key] == new_enums\n",
    "                        pass\n",
    "                    elif \"unknown\" in schema[key]:\n",
    "                        new_enums = []\n",
    "                        for enum in schema[key]:\n",
    "                            if enum != \"Other\":\n",
    "                                new_enums.append(enum)\n",
    "                            else:\n",
    "                                new_enums.append(\"Unknown\")\n",
    "                        schema[key] == new_enums\n",
    "                        pass\n",
    "                else:\n",
    "                    remove_other_from_enum(schema[key])\n",
    "        else:\n",
    "            return\n",
    "    remove_other_from_enum(schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not path.exists(\"../../preprocessed/p5-json/\"):\n",
    "    makedirs(\"../../preprocessed/p5-json/\")\n",
    "    \n",
    "with open(\"../../preprocessed/p4-mibig_schema_draft7.json\") as json_file:\n",
    "    schema_obj = json.load(json_file)\n",
    "    validator = Draft7Validator(schema_obj)\n",
    "    errors = {}\n",
    "    for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/p5-json/\")):\n",
    "        bgc_id = path.basename(json_path)\n",
    "        id_int = int(bgc_id[3:-5])\n",
    "        if (id_int < last_error) and True:\n",
    "            continue\n",
    "        with open(json_path, \"r\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "            error_counts_before = 0\n",
    "            error_counts_after = 0\n",
    "            update_pub_and_accs(data, pubs_and_new_accs)\n",
    "            update_ncbi_accs(data)\n",
    "            for error in sorted(validator.iter_errors(data), key=str):\n",
    "                fix_data_new_schema(data, error)\n",
    "                error_counts_before += 1\n",
    "            lazily_deletes(data)\n",
    "            error_counts = 0\n",
    "            check_and_remove_other(\"\", data, None, None, None)\n",
    "            remove_other_from_schema(schema_obj)\n",
    "            validator = Draft7Validator(schema_obj)\n",
    "            for error in sorted(validator.iter_errors(data), key=str):\n",
    "                # fix remaining enum errors by updating the schema\n",
    "                update_schema_enum(bgc_id, schema_obj, error)\n",
    "            validator = Draft7Validator(schema_obj)\n",
    "            for error in sorted(validator.iter_errors(data), key=str):\n",
    "                if error.validator != \"enum\":\n",
    "                    error_counts_after += 1\n",
    "            print(\"Validated and fixed {}... Before {} error(s), After: {} error(s)\".format(bgc_id, error_counts_before, error_counts_after))\n",
    "            with open(path.join(\"../../preprocessed/p5-json/\", bgc_id), \"w\") as jo:\n",
    "                json.dump(data, jo, indent=4, separators=(',', ': '))\n",
    "            if error_counts_after > 0:\n",
    "                last_error = id_int\n",
    "                exit(1)\n",
    "    \n",
    "    with open(\"../../preprocessed/p5-mibig_schema_draft7.json\", \"w\") as o:\n",
    "        o.write(json.dumps(schema_obj, indent=4, separators=(',', ': ')))\n",
    "        \n",
    "    print(\"All data validated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../preprocessed/reports/p5-enum_contain_others.tsv\", \"w\") as o:\n",
    "    for path in data_contain_others:\n",
    "        o.write(\"{}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all retired BGCs into one tsv\n",
    "retired_bgcs = {}\n",
    "with open(\"../../preprocessed/reports/p5-accession_without_publications.tsv\", \"r\") as o:\n",
    "    for line in o:\n",
    "        bgc_id = line.split(\"\\t\")[0]\n",
    "        if bgc_id not in retired_bgcs:\n",
    "            retired_bgcs[bgc_id] = set()\n",
    "        retired_bgcs[bgc_id].add(\"no_publication\")\n",
    "with open(\"../../preprocessed/reports/p5-multi_loci_bgcs.tsv\", \"r\") as o:\n",
    "    for line in o:\n",
    "        bgc_id = line.split(\"\\t\")[0].split(\".\")[0]\n",
    "        if bgc_id not in retired_bgcs:\n",
    "            retired_bgcs[bgc_id] = set()\n",
    "        retired_bgcs[bgc_id].add(\"multi_loci\")\n",
    "with open(\"../../preprocessed/reports/p5-unknown_enum_options.tsv\", \"r\") as o:\n",
    "    for line in o:\n",
    "        bgc_id, path, added_enum, decision = line.rstrip().split(\"\\t\")\n",
    "        if decision == \"retire\":\n",
    "            bgc_id = bgc_id.split(\".\")[0]\n",
    "            if bgc_id not in retired_bgcs:\n",
    "                retired_bgcs[bgc_id] = set()\n",
    "            retired_bgcs[bgc_id].add(\"enum\")\n",
    "\n",
    "with open(\"../../preprocessed/reports/p5-retired_list.tsv\", \"w\") as o:\n",
    "    for bgc_id in retired_bgcs:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \";\".join(retired_bgcs[bgc_id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
