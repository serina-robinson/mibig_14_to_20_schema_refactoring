{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 7: re-check gene annotations, comparing it to the GBK, filter out ones without any new information added ######\n",
    "#### it also populate organism and taxon properties from mibig finalgbk ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "import glob\n",
    "import json\n",
    "from jsonschema import validate, Draft7Validator\n",
    "from tempfile import TemporaryDirectory\n",
    "from Bio import SeqIO, Entrez\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "from Bio.Alphabet import generic_protein\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import certifi\n",
    "http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n",
    "from jsonschema.validators import Draft7Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gbk(nucl_acc, email, clean_cache = False):\n",
    "    cache_folder = \"../../preprocessed/cache/cached_gbks/\"\n",
    "    cache_path = path.join(cache_folder, \"{}.gbk\".format(nucl_acc.replace(\":\", \".\")))\n",
    "\n",
    "    if not path.exists(cache_folder):\n",
    "        makedirs(cache_folder)\n",
    "    \n",
    "    if clean_cache or not path.exists(cache_path) or path.getsize(cache_path) < 100:\n",
    "        if re.match(\"^MIBIG:BGC\\d{7}\\.\\d+$\", nucl_acc):\n",
    "            bgc_id = nucl_acc.split(\":\")[1].split(\".\")[0]\n",
    "            cl_id = nucl_acc.split(\":\")[1].split(\".\")[1]\n",
    "            mibig_url = \"https://mibig.secondarymetabolites.org/repository/{}/{}.1.cluster{:03d}.gbk\".format(bgc_id, bgc_id, int(cl_id))\n",
    "            resp = http.request('GET', mibig_url)\n",
    "            with open(cache_path, \"w\") as gbk_file:\n",
    "                gbk_file.write(resp.data.decode('utf-8', 'ignore'))\n",
    "        else: # (re)download from ncbi\n",
    "            Entrez.email = email\n",
    "            handle = Entrez.efetch(db=\"nucleotide\", id=nucl_acc, rettype=\"gbwithparts\", retmode=\"text\")\n",
    "            if not path.exists(cache_folder):\n",
    "                makedirs(cache_folder)\n",
    "            with open(cache_path, \"w\") as gbk_file:\n",
    "                gbk_file.write(handle.read())\n",
    "            \n",
    "    return open(cache_path, \"r\")\n",
    "\n",
    "# from antismash\n",
    "def get_aa_translation(seq_record, feature):\n",
    "    \"\"\"Obtain content for translation qualifier for specific CDS feature in sequence record\"\"\"\n",
    "    extracted = feature.extract(seq_record.seq).ungap('-')\n",
    "\n",
    "    # ensure the extracted section is a multiple of three by trimming any excess\n",
    "    if len(extracted) % 3 != 0:\n",
    "        extracted = extracted[:-(len(extracted) % 3)]\n",
    "\n",
    "    fasta_seq = extracted.translate(to_stop=True)\n",
    "    if len(fasta_seq) == 0:\n",
    "        print(\"Retranslating {} with stop codons\".format(feature.id))\n",
    "        fasta_seq = extracted.translate()\n",
    "\n",
    "    # replace ambiguous aminos with an explicit unknown\n",
    "    string_version = str(fasta_seq)\n",
    "    for bad in \"*BJOUZ\":\n",
    "        string_version = string_version.replace(bad, \"X\")\n",
    "\n",
    "    # and remove any gaps\n",
    "    string_version = string_version.replace(\"-\", \"\")\n",
    "    fasta_seq = Seq(string_version, generic_protein)\n",
    "\n",
    "    return fasta_seq\n",
    "\n",
    "def get_aa_sequence(feature, to_stop=False):\n",
    "    \"\"\"Extract sequence from specific CDS feature in sequence record\"\"\"\n",
    "    fasta_seq = feature.qualifiers['translation'][0]\n",
    "    if \"*\" in fasta_seq:\n",
    "        if to_stop:\n",
    "            fasta_seq = fasta_seq.split('*')[0]\n",
    "        else:\n",
    "            fasta_seq = fasta_seq.replace(\"*\",\"X\")\n",
    "    if \"-\" in fasta_seq:\n",
    "        fasta_seq = fasta_seq.replace(\"-\",\"\")\n",
    "    return fasta_seq\n",
    "\n",
    "\n",
    "def count_props(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json, construct a list of property paths\n",
    "    along with its presence count in the json object\"\"\"\n",
    "    key_path = cur_path\n",
    "    \n",
    "    if isinstance(input_dict, dict):\n",
    "        for key in input_dict.keys():\n",
    "            result = count_props(input_dict[key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif isinstance(input_dict, list):\n",
    "        key_path = \"{}[]\".format(key_path)\n",
    "        for node in input_dict:\n",
    "            result = count_props(node, \"{}\".format(key_path), result)\n",
    "\n",
    "    if not isinstance(input_dict, dict):\n",
    "        if key_path not in result:\n",
    "            result[key_path] = 0\n",
    "        result[key_path] += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_props_new_schema(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json draft7 schema, construct a list of property paths\n",
    "    along with either required == True for each properties\"\"\"\n",
    "    key_path = cur_path\n",
    "    if (\"type\" not in input_dict) or (input_dict[\"type\"] not in [\"object\", \"array\"]):\n",
    "        key_path = \"{}\".format(cur_path) # string / etc.\n",
    "    elif input_dict[\"type\"] == \"object\":\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            result = fetch_props_new_schema(input_dict[\"properties\"][key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif input_dict[\"type\"] == \"array\":\n",
    "        key_path = \"{}[]\".format(cur_path)\n",
    "        result = fetch_props_new_schema(input_dict[\"items\"], \"{}\".format(key_path), result)\n",
    "    \n",
    "    if key_path not in result and \"properties\" not in input_dict:\n",
    "        result[key_path] = False # can't really use this\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_mibig_final_gbk(bgc_id, clean_cache = False):\n",
    "    cache_folder = \"../../preprocessed/cache/cached_mibig_finalgbks/\"\n",
    "    cache_path = path.join(cache_folder, \"{}.1.final.gbk\".format(bgc_id))\n",
    "    \n",
    "    if clean_cache or not path.exists(cache_path) or path.getsize(cache_path) < 100:\n",
    "        mibig_url = \"https://mibig.secondarymetabolites.org/repository/{}/{}.1.final.gbk\".format(bgc_id, bgc_id)\n",
    "        resp = http.request('GET', mibig_url)\n",
    "        with open(cache_path, \"w\") as cf:\n",
    "            cf.write(resp.data.decode('utf-8', 'ignore'))\n",
    "        \n",
    "    return open(cache_path, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_extra_genes = {}\n",
    "removed_annotations = {}\n",
    "removed_operons = {}\n",
    "added_mibig_genes = {}\n",
    "no_gbk = {}\n",
    "\n",
    "def check_gene_annotations(data):\n",
    "    loci = data[\"cluster\"][\"loci\"]\n",
    "    annots = data[\"cluster\"].get(\"genes\", {})\n",
    "    gbk_acc = loci[\"accession\"].upper()\n",
    "    bgc_id = data[\"cluster\"][\"mibig_accession\"]\n",
    "    email = \"mibig@secondarymetabolites.org\"\n",
    "    gbk_record = None\n",
    "    if True:\n",
    "        num_try = 1\n",
    "        clean_cache = False\n",
    "        while num_try < 6:\n",
    "            try:\n",
    "                with fetch_gbk(gbk_acc, email, clean_cache) as gbk_handle:\n",
    "                    seq_record = SeqIO.read(gbk_handle, \"genbank\") # the gbk should contains only 1 file\n",
    "                    if len(seq_record.seq) < 1:\n",
    "                        raise Exception(\"Empty sequence record {}\".format(gbk_acc))\n",
    "                    gbk_record = seq_record\n",
    "                    break\n",
    "            except:\n",
    "                print(\"Error...\")\n",
    "                clean_cache = True\n",
    "            num_try += 1\n",
    "            time.sleep(5)\n",
    "        if not isinstance(gbk_record, SeqRecord): # failed to download NCBI data\n",
    "            print(\"{} Failed to download: {}\".format(data[\"cluster\"][\"mibig_accession\"], gbk_acc))\n",
    "            no_gbk[bgc_id] = gbk_acc\n",
    "            return {}\n",
    "\n",
    "    # fetch all CDS inside the cluster\n",
    "    cluster_cds = []\n",
    "    cluster_cds_ids = set([\"\", \"No protein ID\"]) # quick lookup for existing IDs\n",
    "    for feature in seq_record.features:\n",
    "        if feature.type == \"CDS\":\n",
    "            if (\"start_coord\" not in loci) or (feature.location.start >= loci[\"start_coord\"]-1 and feature.location.end <= loci[\"end_coord\"]):\n",
    "                cluster_cds.append(feature)\n",
    "                if \"gene\" in feature.qualifiers:\n",
    "                    cluster_cds_ids.add(feature.qualifiers[\"gene\"][0])\n",
    "                if \"protein_id\" in feature.qualifiers:\n",
    "                    cluster_cds_ids.add(feature.qualifiers[\"protein_id\"][0])\n",
    "                if \"locus_tag\" in feature.qualifiers:\n",
    "                    cluster_cds_ids.add(feature.qualifiers[\"locus_tag\"][0])\n",
    "                \n",
    "    # fetch all CDS in MIBiG finalgbk\n",
    "    mibig_extra_genes = []\n",
    "    with fetch_mibig_final_gbk(bgc_id) as mibig_gbk_handle:\n",
    "        mibig_seq_records = SeqIO.parse(mibig_gbk_handle, \"genbank\")\n",
    "        for mibig_seq_record in mibig_seq_records:\n",
    "            # add organism information\n",
    "            loci[\"organism\"] = mibig_seq_record.annotations[\"organism\"]\n",
    "            loci[\"taxonomy\"] = mibig_seq_record.annotations[\"taxonomy\"]\n",
    "            # look for gene information\n",
    "            match = re.search(\"(between(?P<start>\\d+)-(?P<end>\\d+)ntfrom){0,1}GenBankID(?P<acc>[A-Z0-9_\\.]+)\\.\", mibig_seq_record.annotations['comment'].replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "            if match:\n",
    "                gbk_acc_mibig = match.group(\"acc\").upper()\n",
    "                if gbk_acc_mibig == gbk_acc:\n",
    "                    offset = 0\n",
    "                    if match.group(\"start\"):\n",
    "                        offset = int(match.group(\"start\")) - 1\n",
    "                    for feature in mibig_seq_record.features:\n",
    "                        if feature.type == \"CDS\":\n",
    "                            ids = []\n",
    "                            if \"gene\" in feature.qualifiers:\n",
    "                                if feature.qualifiers[\"gene\"][0] in cluster_cds_ids:\n",
    "                                    continue\n",
    "                                ids.append(feature.qualifiers[\"gene\"][0])\n",
    "                            if \"protein_id\" in feature.qualifiers:\n",
    "                                if feature.qualifiers[\"protein_id\"][0] in cluster_cds_ids:\n",
    "                                    continue\n",
    "                                ids.append(feature.qualifiers[\"protein_id\"][0])\n",
    "                            if \"locus_tag\" in feature.qualifiers:\n",
    "                                if feature.qualifiers[\"locus_tag\"][0] in cluster_cds_ids:\n",
    "                                    continue\n",
    "                                ids.append(feature.qualifiers[\"locus_tag\"][0])\n",
    "                            cluster_cds_ids.update(set(ids))\n",
    "                            mibig_extra_genes.append(feature._shift(offset)) \n",
    "                            if bgc_id not in added_mibig_genes:\n",
    "                                added_mibig_genes[bgc_id] = []\n",
    "                            added_mibig_genes[bgc_id].append(\"/\".join(ids))\n",
    "                            \n",
    "    # add mibig extra genes\n",
    "    for feature in mibig_extra_genes:\n",
    "        if \"extra_genes\" not in annots:\n",
    "            annots[\"extra_genes\"] = []\n",
    "        if \"annotations\" not in annots:\n",
    "            annots[\"annotations\"] = []\n",
    "        gene_id = feature.qualifiers.get(\"locus_tag\", feature.qualifiers.get(\"protein_id\", feature.qualifiers.get(\"gene\", [None])))[0]\n",
    "        if gene_id != None:\n",
    "            extra_gene = {\n",
    "                \"id\": gene_id,\n",
    "                \"location\": {\n",
    "                    \"exons\": [{\"start\": location.start + 1, \"end\": location.end} for location in feature.location.parts],\n",
    "                    \"strand\": feature.location.strand\n",
    "                }\n",
    "            }\n",
    "            if \"translation\" in feature.qualifiers:\n",
    "                extra_gene[\"translation\"] = feature.qualifiers[\"translation\"][0]\n",
    "            annots[\"extra_genes\"].append(extra_gene)\n",
    "            annot = {\n",
    "                \"id\": gene_id\n",
    "            }\n",
    "            if \"product\" in feature.qualifiers:\n",
    "                annot[\"product\"] = feature.qualifiers[\"product\"][0]\n",
    "            if \"note\" in feature.qualifiers:\n",
    "                annot[\"comments\"] = feature.qualifiers[\"note\"][0]\n",
    "            if len(annot.keys()) > 1:\n",
    "                annots[\"annotations\"].append(annot)\n",
    "                \n",
    "    # check extra genes annotation\n",
    "    extra_genes = []\n",
    "    for i, extra_gene in enumerate(annots.get(\"extra_genes\", [])):\n",
    "        gene_id = extra_gene[\"id\"]\n",
    "        if gene_id not in cluster_cds_ids or gene_id in \"/\".join(added_mibig_genes.get(bgc_id, \"\")).split(\"/\"):\n",
    "            extra_genes.append(extra_gene)\n",
    "            cluster_cds_ids.add(gene_id)\n",
    "        else:\n",
    "            if bgc_id not in removed_extra_genes:\n",
    "                removed_extra_genes[bgc_id] = []\n",
    "            removed_extra_genes[bgc_id].append(str(i))\n",
    "    if len(extra_genes) > 0:\n",
    "        annots[\"extra_genes\"] = extra_genes\n",
    "    else:\n",
    "        annots.pop(\"extra_genes\", None)\n",
    "            \n",
    "    # check annotation, remove if no added info\n",
    "    gene_annots = []\n",
    "    for i, annot in enumerate(annots.get(\"annotations\", [])):\n",
    "        gene_id = annot.get(\"id\", annot.get(\"name\", None))\n",
    "        approve = False\n",
    "        if gene_id in cluster_cds_ids:\n",
    "            if len(annot.get(\"name\", \"\")) < 1:\n",
    "                annot.pop(\"name\", None)\n",
    "            if len(annot.get(\"product\", \"\")) < 1:\n",
    "                annot.pop(\"product\", None)\n",
    "            if len(annot.get(\"mut_pheno\", \"\")) < 1:\n",
    "                annot.pop(\"mut_pheno\", None)\n",
    "            if len(annot.get(\"comments\", \"\")) < 1:\n",
    "                annot.pop(\"comments\", None)\n",
    "            g_functions = []\n",
    "            for g_function in annot.get(\"functions\", []):\n",
    "                if len(g_function.get(\"category\", \"\")) < 1:\n",
    "                    continue\n",
    "                elif len(g_function.get(\"evidence\", [])) < 1:\n",
    "                    continue\n",
    "                elif len(set(g_function[\"evidence\"]) - set([\"Sequence-based prediction\", \"Other in vivo study\", \"Heterologous expression\", \"Knock-out\", \"Activity assay\"])) > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    g_functions.append(g_function)\n",
    "            if len(g_functions) > 0:\n",
    "                annot[\"functions\"] = g_functions\n",
    "            else:\n",
    "                annot.pop(\"functions\", None)\n",
    "            if len(annot.get(\"tailoring\", [])) < 1:\n",
    "                annot.pop(\"tailoring\", None)\n",
    "            if len(annot.get(\"publications\", [])) < 1:\n",
    "                annot.pop(\"publications\", None)\n",
    "            if len(annot.keys()) > 1: # 1 is the id\n",
    "                approve = True\n",
    "        if approve:\n",
    "            gene_annots.append(annot)\n",
    "        else:\n",
    "            if bgc_id not in removed_annotations:\n",
    "                removed_annotations[bgc_id] = []\n",
    "            removed_annotations[bgc_id].append(str(i))\n",
    "    if len(gene_annots) > 0:\n",
    "        annots[\"annotations\"] = gene_annots\n",
    "    else:\n",
    "        annots.pop(\"annotations\", None)\n",
    "                \n",
    "    # check operons, remove if no added info\n",
    "    operons = []\n",
    "    for i, operon in enumerate(annots.get(\"operons\", [])):\n",
    "        o_genes = set(operon.get(\"genes\", []))\n",
    "        o_evidence = set(operon.get(\"evidence\", [])).intersection(set([\"Sequence-based prediction\", \"RACE\", \"ChIPseq\", \"RNAseq\"]))\n",
    "        if len(o_genes) > 0 and len(o_genes - cluster_cds_ids) < 1:\n",
    "            if len(o_evidence) > 0:\n",
    "                operons.append({\n",
    "                    \"genes\": list(o_genes),\n",
    "                    \"evidence\": list(o_evidence)\n",
    "                })\n",
    "                continue\n",
    "        if bgc_id not in removed_operons:\n",
    "            removed_operons[bgc_id] = []\n",
    "        removed_operons[bgc_id].append(str(i))\n",
    "        \n",
    "    if len(operons) > 0:\n",
    "        annots[\"operons\"] = operons\n",
    "    else:\n",
    "        annots.pop(\"operons\", None)\n",
    "        \n",
    "    if len(annots.keys()) > 0:\n",
    "        data[\"cluster\"][\"genes\"] = annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data, validator, schema_props):\n",
    "    for error in sorted(validator.iter_errors(data), key=str):\n",
    "        if error.path[-2] == \"evidence\" or error.path[-1] == \"evidence\":\n",
    "            if error.path[-3] == \"loci\" or error.path[-2] == \"loci\":\n",
    "                continue\n",
    "        elif error.path[-1] == \"nr_iterations\":\n",
    "            continue\n",
    "        elif error.path[-1] == \"module_number\":\n",
    "            continue\n",
    "        elif error.path[-2] == \"proteinogenic\":\n",
    "            continue\n",
    "        elif error.path[-1] in [\"leader_sequence\", \"follower_sequence\"]:\n",
    "            continue\n",
    "        elif error.path[-1] == \"organism\":\n",
    "            if data[\"cluster\"][\"mibig_accession\"] in no_gbk:\n",
    "                continue\n",
    "        print(error.message)\n",
    "        #sys.exit(0)\n",
    "    this_file_props = count_props(data, \"\", {})\n",
    "    for prop in this_file_props:\n",
    "        if prop not in schema_props.keys():\n",
    "            print(prop)\n",
    "            sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../inputs/mibig_schema_phase_6.json\") as schema_6:\n",
    "    schema_obj = json.load(schema_6)\n",
    "    schema_obj[\"properties\"][\"cluster\"][\"properties\"][\"loci\"][\"properties\"][\"organism\"] = {\n",
    "        \"title\": \"Organism's name\",\n",
    "        \"type\": \"string\"\n",
    "    }\n",
    "    schema_obj[\"properties\"][\"cluster\"][\"properties\"][\"loci\"][\"properties\"][\"taxonomy\"] = {\n",
    "        \"title\": \"Organism's Taxonomy\",\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    }\n",
    "    schema_obj[\"properties\"][\"cluster\"][\"properties\"][\"loci\"][\"required\"].append(\"organism\")\n",
    "    with open(\"../../preprocessed/p7-mibig_schema_draft7.json\", \"w\") as schema_7:\n",
    "        json.dump(schema_obj, schema_7, indent=4, separators=(',', ': '), sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../../preprocessed/p6-json/\"\n",
    "output_folder = \"../../preprocessed/p7-json/\"\n",
    "\n",
    "if not path.exists(output_folder):\n",
    "    makedirs(output_folder)\n",
    "\n",
    "validator = None\n",
    "schema_props = {}\n",
    "with open(\"../../preprocessed/p7-mibig_schema_draft7.json\") as json_file:\n",
    "    schema_obj = json.load(json_file)\n",
    "    validator = Draft7Validator(schema_obj)\n",
    "    schema_props = fetch_props_new_schema(schema_obj, \"\", {})\n",
    "\n",
    "    \n",
    "for json_path in sorted(glob.glob(path.join(input_path, \"BGC*.json\"))):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        bgc_id = path.basename(json_path).split(\".\")[0]\n",
    "        data = json.load(json_file)\n",
    "        print(\"Scanning {}\".format(bgc_id))\n",
    "        check_gene_annotations(data)\n",
    "        validate_data(data, validator, schema_props)\n",
    "        with open(path.join(output_folder, \"{}.json\".format(bgc_id)), \"w\") as o:\n",
    "            o.write(json.dumps(data, indent=4, separators=(',', ': '), sort_keys=True))\n",
    "            \n",
    "print(\"All data fetched!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retired_bgcs = {}\n",
    "\n",
    "with open(\"../../preprocessed/reports/p7-removed_extra_genes.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tgene_ids\\n\")\n",
    "    for bgc_id in removed_extra_genes:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \",\".join(removed_extra_genes[bgc_id])))\n",
    "        if bgc_id not in retired_bgcs:\n",
    "            retired_bgcs[bgc_id] = set()\n",
    "        retired_bgcs[bgc_id].add(\"extra_genes_removed\")\n",
    "    \n",
    "with open(\"../../preprocessed/reports/p7-removed_annotations.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tgene_ids\\n\")\n",
    "    for bgc_id in removed_annotations:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \",\".join(removed_annotations[bgc_id])))\n",
    "        if bgc_id not in retired_bgcs:\n",
    "            retired_bgcs[bgc_id] = set()\n",
    "        retired_bgcs[bgc_id].add(\"annotations_removed\")\n",
    "    \n",
    "with open(\"../../preprocessed/reports/p7-removed_operons.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tindexes\\n\")\n",
    "    for bgc_id in removed_operons:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \",\".join(removed_operons[bgc_id])))\n",
    "        if bgc_id not in retired_bgcs:\n",
    "            retired_bgcs[bgc_id] = set()\n",
    "        retired_bgcs[bgc_id].add(\"operons_removed\")\n",
    "\n",
    "with open(\"../../preprocessed/reports/p7-no_gbk.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\taccession\\n\")\n",
    "    for bgc_id in no_gbk:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, no_gbk[bgc_id]))\n",
    "        if bgc_id not in retired_bgcs:\n",
    "            retired_bgcs[bgc_id] = set()\n",
    "        retired_bgcs[bgc_id].add(\"no_gbk\")\n",
    "        \n",
    "with open(\"../../preprocessed/reports/p7-retired_list.tsv\", \"w\") as o:\n",
    "    for bgc_id in retired_bgcs:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \";\".join(retired_bgcs[bgc_id])))\n",
    "    \n",
    "with open(\"../../preprocessed/reports/p7-added_mibig_genes.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tgenes\\n\")\n",
    "    for bgc_id in added_mibig_genes:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \",\".join(added_mibig_genes[bgc_id])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
