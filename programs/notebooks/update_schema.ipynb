{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase 0: compare old schema vs old data, output the summary in an excel file* to help decision making\n",
    "# phase 1: transform old schema to match (correct version) of old data\n",
    "# phase 2: compare phase 1 schema vs old data, output the summary in an excel file* to help decision making\n",
    "# phase 3: transform old data to match schema from phase 1 (doesn't count dependencies/required keywords)\n",
    "# phase 4: transform schema from phase 1 to match JSON Schema draft v7 (we will call it 'new schema')\n",
    "# phase 5: transform data from phase 2 to match the new schema (include all dependencies/required keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## common imports ##\n",
    "from os import path, makedirs\n",
    "import glob\n",
    "import json\n",
    "from jsonschema import validate, Draft7Validator\n",
    "from sys import exit\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## common functions ##\n",
    "def fetch_mibig_json_filepaths(dir_path):\n",
    "    \"\"\"fetch mibig json paths from a specific folder\"\"\"\n",
    "    return glob.glob(path.join(dir_path, \"BGC*.json\"))\n",
    "\n",
    "def count_props(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json, construct a list of property paths\n",
    "    along with its presence count in the json object\"\"\"\n",
    "    key_path = cur_path\n",
    "    \n",
    "    if isinstance(input_dict, dict):\n",
    "        for key in input_dict.keys():\n",
    "            result = count_props(input_dict[key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif isinstance(input_dict, list):\n",
    "        key_path = \"{}[]\".format(key_path)\n",
    "        for node in input_dict:\n",
    "            result = count_props(node, \"{}\".format(key_path), result)\n",
    "\n",
    "    if not isinstance(input_dict, dict):\n",
    "        if key_path not in result:\n",
    "            result[key_path] = 0\n",
    "        result[key_path] += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_props_old_schema(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json schema, construct a list of property paths\n",
    "    along with either required == True for each properties\"\"\"\n",
    "    key_path = cur_path\n",
    "    if (\"type\" not in input_dict) or (input_dict[\"type\"] not in [\"object\", \"array\"]):\n",
    "        key_path = \"{}\".format(cur_path) # string / etc.\n",
    "    elif input_dict[\"type\"] == \"object\":\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            result = fetch_props_old_schema(input_dict[\"properties\"][key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif input_dict[\"type\"] == \"array\":\n",
    "        key_path = \"{}[]\".format(cur_path)\n",
    "        result = fetch_props_old_schema(input_dict[\"items\"], \"{}\".format(key_path), result)\n",
    "    \n",
    "    if key_path not in result and \"properties\" not in input_dict:\n",
    "        result[key_path] = \"required\" in input_dict and input_dict[\"required\"] == True\n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_props_new_schema(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json draft7 schema, construct a list of property paths\n",
    "    along with either required == True for each properties\"\"\"\n",
    "    key_path = cur_path\n",
    "    if (\"type\" not in input_dict) or (input_dict[\"type\"] not in [\"object\", \"array\"]):\n",
    "        key_path = \"{}\".format(cur_path) # string / etc.\n",
    "    elif input_dict[\"type\"] == \"object\":\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            result = fetch_props_old_schema(input_dict[\"properties\"][key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif input_dict[\"type\"] == \"array\":\n",
    "        key_path = \"{}[]\".format(cur_path)\n",
    "        result = fetch_props_old_schema(input_dict[\"items\"], \"{}\".format(key_path), result)\n",
    "    \n",
    "    if key_path not in result and \"properties\" not in input_dict:\n",
    "        result[key_path] = False # can't really use this\n",
    "    return result\n",
    "\n",
    "\n",
    "def search_and_delete(key, input_dict):\n",
    "    \"\"\"delete keys from nested dict\"\"\"\n",
    "    if isinstance(input_dict, list):\n",
    "        for i in input_dict:\n",
    "            search_and_delete(key, i)\n",
    "    elif not isinstance(input_dict, dict):\n",
    "        return\n",
    "    to_del = []\n",
    "    for k in input_dict:\n",
    "        if k == key:\n",
    "            to_del.append(k)\n",
    "        elif isinstance(input_dict[k], dict):\n",
    "            search_and_delete(key, input_dict[k])\n",
    "    for k in to_del:\n",
    "        del input_dict[k]\n",
    "\n",
    "        \n",
    "def rename_key(from_key, to_key, parent_dict):\n",
    "    \"\"\"rename key in dict\"\"\"\n",
    "    if from_key in parent_dict:\n",
    "        parent_dict[to_key] = parent_dict[from_key]\n",
    "        del parent_dict[from_key]\n",
    "\n",
    "def del_key(key, parent_dict):\n",
    "    if key in parent_dict:\n",
    "        del parent_dict[key]\n",
    "        \n",
    "import time\n",
    "def date2iso(thedate):\n",
    "    strdate = thedate.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    minute = (time.localtime().tm_gmtoff / 60) % 60\n",
    "    hour = ((time.localtime().tm_gmtoff / 60) - minute) / 60\n",
    "    utcoffset = \"%.2d:%.2d\" %(hour, minute)\n",
    "    if utcoffset[0] != '-':\n",
    "        utcoffset = '+' + utcoffset\n",
    "        return strdate + utcoffset\n",
    "    \n",
    "class ToDelete():\n",
    "    \"\"\"dummy class for lazy deletion of list members\"\"\"\n",
    "    pass\n",
    "\n",
    "def lazily_deletes(input_dict):\n",
    "    \"\"\"traverse and lazily delete list/dict members\"\"\"\n",
    "    if isinstance(input_dict, list):\n",
    "        new_list = []\n",
    "        for i, node in enumerate(input_dict):\n",
    "            if not isinstance(node, ToDelete):\n",
    "                input_dict[i] = lazily_deletes(node)\n",
    "                new_list.append(node)\n",
    "        return new_list\n",
    "    elif isinstance(input_dict, dict):\n",
    "        key_to_dels = []\n",
    "        for key in input_dict:\n",
    "            if not isinstance(input_dict[key], ToDelete):\n",
    "                input_dict[key] = lazily_deletes(input_dict[key])\n",
    "            else:\n",
    "                key_to_dels.append(key)\n",
    "        for key in key_to_dels:\n",
    "            del input_dict[key]\n",
    "    return input_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 0: compare old schema vs old data, output the summary in an excel file* to help decision making ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_props = {}\n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../inputs/json_1.4/\")):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        this_file_props = count_props(json_obj, \"\", {})\n",
    "        for prop in this_file_props:\n",
    "            if prop not in all_props:\n",
    "                all_props[prop] = [path.basename(json_path)]\n",
    "            else:\n",
    "                all_props[prop].append(path.basename(json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ../../preprocessed/old_schema_properties.csv\n",
      "File written: ../../preprocessed/old_data_vs_old_schema.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../inputs/mibig_schema.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_old_schema(json_obj, \"\", {})\n",
    "    with open(\"../../preprocessed/old_schema_properties.csv\", \"w\") as o:\n",
    "        for key in sorted(schema_props.keys()):\n",
    "            o.write(\"{},{}\\n\".format(key, schema_props[key]))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    with open(\"../../preprocessed/old_data_vs_old_schema.csv\", \"w\") as o:\n",
    "        not_in_schema = []\n",
    "        for key in sorted(all_props.keys()):\n",
    "            if key not in schema_props.keys():\n",
    "                not_in_schema.append((key, all_props[key]))\n",
    "        for rep in sorted(not_in_schema, key=lambda x: len(x[1]), reverse = True):\n",
    "            o.write(\"{},{}\\n\".format(rep[0], len(rep[1])))\n",
    "        print(\"File written: {}\".format(o.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 1: transform old schema to match (correct version) of old data ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ../../preprocessed/mibig_schema_phase_1.json\n"
     ]
    }
   ],
   "source": [
    "# (everything is manually done) -- TODO: should write hardcoded scripts to make it reproducible\n",
    "# update all comma-separated based properties into arrays\n",
    "# gene_pubs: integer --> gene_pubs: array\n",
    "print(\"File written: ../../preprocessed/mibig_schema_phase_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### phase 2: compare phase 1 schema vs old data, output the summary in an excel file* to help decision making ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ../../preprocessed/schema_phase_1_properties.csv\n",
      "File written: ../../preprocessed/old_data_vs_schema_phase_1.csv\n",
      "File written: ../../preprocessed/bgc_to_fix_phase_2.csv\n"
     ]
    }
   ],
   "source": [
    "# use all_props from phase 0\n",
    "with open(\"../../preprocessed/mibig_schema_phase_1.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_old_schema(json_obj, \"\", {})\n",
    "    with open(\"../../preprocessed/schema_phase_1_properties.csv\", \"w\") as o:\n",
    "        for key in sorted(schema_props.keys()):\n",
    "            o.write(\"{},{}\\n\".format(key, schema_props[key]))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append((key, all_props[key]))\n",
    "    with open(\"../../preprocessed/old_data_vs_schema_phase_1.csv\", \"w\") as o:\n",
    "        for rep in sorted(not_in_schema, key=lambda x: len(x[1]), reverse = True):\n",
    "            o.write(\"{},{}\\n\".format(rep[0], len(rep[1])))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    with open(\"../../preprocessed/bgc_to_fix_phase_2.csv\", \"w\") as o:\n",
    "        bgc_to_fix = {}\n",
    "        for rep in not_in_schema:\n",
    "            for bgc in rep[1]:\n",
    "                if bgc not in bgc_to_fix:\n",
    "                    bgc_to_fix[bgc] = []\n",
    "                bgc_to_fix[bgc].append(rep[0])\n",
    "        for bgc in bgc_to_fix:\n",
    "            o.write(\"{},{}\\n\".format(bgc, \";\".join(bgc_to_fix[bgc])))\n",
    "        print(\"File written: {}\".format(o.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### phase 3: transform old data to match schema from phase 1 (doesn't count dependencies/required keywords) ####\n",
    "def match_attributes_to_schema_phase_1(data):\n",
    "    # fix /Comments\n",
    "    rename_key(\"Comments\", \"comments\", data)\n",
    "    # fix /general_params/loci/nucl_acc[]/conn_comp_cluster\n",
    "    for nuac in data[\"general_params\"][\"loci\"][\"nucl_acc\"]:\n",
    "        if \"conn_comp_cluster\" in nuac and isinstance(nuac[\"conn_comp_cluster\"], str):\n",
    "            nuac[\"conn_comp_cluster\"] = nuac[\"conn_comp_cluster\"].replace(\" \", \"\").split(\",\")\n",
    "    # fix /general_params/Polyketide/Saccharide\n",
    "    if \"Polyketide\" in data[\"general_params\"] and \"Saccharide\" in data[\"general_params\"][\"Polyketide\"]:\n",
    "        data[\"general_params\"][\"Saccharide\"] = data[\"general_params\"][\"Polyketide\"][\"Saccharide\"]\n",
    "        del data[\"general_params\"][\"Polyketide\"][\"Saccharide\"]\n",
    "    # fix /general_params/Saccharide/Sugar_subclass\n",
    "    if \"Saccharide\" in data[\"general_params\"] and \"Sugar_subclass\" in data[\"general_params\"][\"Saccharide\"]:\n",
    "        rename_key(\"Sugar_subclass\", \"saccharide_subclass\", data[\"general_params\"][\"Saccharide\"])\n",
    "    # fix /general_params/Saccharide/gt_genes[]/sugar_subcluster\n",
    "    if \"Saccharide\" in data[\"general_params\"] and \"gt_genes\" in data[\"general_params\"][\"Saccharide\"]:\n",
    "        for gtg in data[\"general_params\"][\"Saccharide\"][\"gt_genes\"]:\n",
    "            if \"sugar_subcluster\" in gtg and isinstance(gtg[\"sugar_subcluster\"], str):\n",
    "                gtg[\"sugar_subcluster\"] = gtg[\"sugar_subcluster\"].replace(\" \", \"\").split(\",\")    \n",
    "    for comp in data[\"general_params\"][\"compounds\"]:\n",
    "        # fix /general_params/compounds[]/chem_target\n",
    "        if \"chem_target\" in comp and isinstance(comp[\"chem_target\"], str):\n",
    "            comp[\"chem_target\"] = comp[\"chem_target\"].replace(\" \", \"\").split(\",\")  \n",
    "        # fix /general_params/compounds[]/chem_moieties[]/subcluster\n",
    "        if \"chem_moieties\" in comp:\n",
    "            for moi in comp[\"chem_moieties\"]:\n",
    "                if \"subcluster\" in moi:\n",
    "                    if moi[\"subcluster\"] == \"unknown\":\n",
    "                        del moi[\"subcluster\"]\n",
    "    # fix /general_params/genes/gene[]/evidence_genefunction[][]**\n",
    "    if \"genes\" in data[\"general_params\"]:\n",
    "        if \"gene\" in data[\"general_params\"][\"genes\"]:\n",
    "            for gen in data[\"general_params\"][\"genes\"][\"gene\"]:\n",
    "                if \"evidence_genefunction\" in gen:\n",
    "                    for i, evgen in enumerate(gen[\"evidence_genefunction\"]):\n",
    "                        def getvalevgen(ar):\n",
    "                            if isinstance(ar, list):\n",
    "                                return getvalevgen(ar[0])\n",
    "                            else:\n",
    "                                return ar\n",
    "                        gen[\"evidence_genefunction\"][i] = getvalevgen(evgen)\n",
    "    # fix /general_params/Other/biosyn_class[]\n",
    "    if \"Other\" in data[\"general_params\"] and \"biosyn_class\" in data[\"general_params\"][\"Other\"]:\n",
    "        clas = data[\"general_params\"][\"Other\"][\"biosyn_class\"][0]\n",
    "        del data[\"general_params\"][\"Other\"][\"biosyn_class\"]\n",
    "        data[\"general_params\"][\"Other\"][\"other_subclass\"] = clas\n",
    "    # fix /general_params/publications\n",
    "    if \"publications\" in data[\"general_params\"] and isinstance(data[\"general_params\"][\"publications\"], str):\n",
    "        data[\"general_params\"][\"publications\"] = data[\"general_params\"][\"publications\"].replace(\" \", \"\").split(\",\")\n",
    "    # Polyketide\n",
    "    if \"Polyketide\" in data[\"general_params\"]:\n",
    "        pol = data[\"general_params\"][\"Polyketide\"]\n",
    "        # fix /general_params/Polyketide/cyclases\n",
    "        if \"cyclases\" in pol and isinstance(pol[\"cyclases\"], str):\n",
    "            pol[\"cyclases\"] = pol[\"cyclases\"].replace(\" \", \"\").split(\",\")\n",
    "        # fix /general_params/Polyketide/pks_genes\n",
    "        if \"pks_genes\" in pol and isinstance(pol[\"pks_genes\"], str):\n",
    "            pol[\"pks_genes\"] = pol[\"pks_genes\"].replace(\" \", \"\").split(\",\")\n",
    "        # fix /general_params/Polyketide/pufa_mod_doms\n",
    "        if \"pufa_mod_doms\" in pol and isinstance(pol[\"pufa_mod_doms\"], str):\n",
    "            pol[\"pufa_mod_doms\"] = pol[\"pufa_mod_doms\"].replace(\" \", \"\").split(\",\")\n",
    "        # fix /general_params/Polyketide/mod_pks_genes[]/pks_module[]/pks_mod_doms\n",
    "        if \"mod_pks_genes\" in pol:\n",
    "            for modpk in pol[\"mod_pks_genes\"]:\n",
    "                if \"pks_module\" in modpk:\n",
    "                    for pkmod in modpk[\"pks_module\"]:\n",
    "                        if \"pks_mod_doms\" in pkmod and isinstance(pkmod[\"pks_mod_doms\"], str):\n",
    "                            pkmod[\"pks_mod_doms\"] = pkmod[\"pks_mod_doms\"].replace(\" \", \"\").split(\",\")\n",
    "    # RiPP\n",
    "    if \"RiPP\" in data[\"general_params\"]:\n",
    "        rip = data[\"general_params\"][\"RiPP\"]\n",
    "        if \"precursor_loci\" in rip:\n",
    "            for ploc in rip[\"precursor_loci\"]:\n",
    "                # fix /general_params/RiPP/precursor_loci[]/cleavage_recogn_site\n",
    "                if \"cleavage_recogn_site\" in ploc and isinstance(ploc[\"cleavage_recogn_site\"], str):\n",
    "                    ploc[\"cleavage_recogn_site\"] = ploc[\"cleavage_recogn_site\"].replace(\" \", \"\").split(\",\")\n",
    "                # fix /general_params/RiPP/precursor_loci[]/core_pept_aa\n",
    "                if \"core_pept_aa\" in ploc and isinstance(ploc[\"core_pept_aa\"], str):\n",
    "                    ploc[\"core_pept_aa\"] = ploc[\"core_pept_aa\"].replace(\" \", \"\").split(\",\")\n",
    "                            \n",
    "    return\n",
    "\n",
    "if not path.exists(\"../../preprocessed/json_1.4_phase_3/\"):\n",
    "    makedirs(\"../../preprocessed/json_1.4_phase_3/\")\n",
    "    \n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../inputs/json_1.4/\")):\n",
    "    json_obj = None\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        match_attributes_to_schema_phase_1(json_obj)        \n",
    "        with open(path.join(\"../../preprocessed/json_1.4_phase_3/\", path.basename(json_path)), \"w\") as json_file:\n",
    "            json.dump(json_obj, json_file, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that all data matched schema\n",
    "all_props_phase_3 = {}\n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/json_1.4_phase_3/\")):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        this_file_props = count_props(json_obj, \"\", {})\n",
    "        for prop in this_file_props:\n",
    "            if prop not in all_props_phase_3:\n",
    "                all_props_phase_3[prop] = [path.basename(json_path)]\n",
    "            else:\n",
    "                all_props_phase_3[prop].append(path.basename(json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conflicts: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../preprocessed/mibig_schema_phase_1.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_old_schema(json_obj, \"\", {})\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props_phase_3.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append((key, all_props_phase_3[key]))\n",
    "            print(key)\n",
    "    print(\"Number of conflicts: {}\".format(len(not_in_schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 4: transform schema from phase 1 to match JSON Schema draft v7 (we will call it 'new schema') ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema = None\n",
    "with open(\"../../preprocessed/mibig_schema_phase_1.json\") as json_file:\n",
    "    new_schema = json.load(json_file) # pre-load with old schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: convert 'required' to Json Schema draft 7 style\n",
    "def fix_required(input_dict):\n",
    "    if \"type\" in input_dict and input_dict[\"type\"] == \"object\":\n",
    "        input_dict[\"required\"] = []\n",
    "        for prop in input_dict[\"properties\"]:\n",
    "            child = input_dict[\"properties\"][prop]\n",
    "            if \"required\" in child and child[\"required\"] == True:\n",
    "                input_dict[\"required\"].append(prop)\n",
    "            fix_required(child)\n",
    "        if len(input_dict[\"required\"]) < 1:\n",
    "            del input_dict[\"required\"]\n",
    "    else:\n",
    "        if \"type\" in input_dict and input_dict[\"type\"] == \"array\":\n",
    "            fix_required(input_dict[\"items\"])\n",
    "        if \"required\" in input_dict:\n",
    "            del input_dict[\"required\"]\n",
    "fix_required(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: convert 'dependencies' to Json Schema draft 7 style\n",
    "def fix_dependencies(input_dict):\n",
    "    if \"type\" in input_dict and input_dict[\"type\"] == \"object\":\n",
    "        input_dict[\"dependencies\"] = {}\n",
    "        for prop in input_dict[\"properties\"]:\n",
    "            child = input_dict[\"properties\"][prop]\n",
    "            if \"dependencies\" in child and isinstance(child[\"dependencies\"], str):\n",
    "                if child[\"dependencies\"] in input_dict[\"properties\"]:\n",
    "                    if child[\"dependencies\"] not in input_dict[\"dependencies\"]:\n",
    "                        input_dict[\"dependencies\"][child[\"dependencies\"]] = []\n",
    "                    input_dict[\"dependencies\"][child[\"dependencies\"]].append(prop)\n",
    "                else:\n",
    "                    print(\"Error: {} not found\".format(child[\"dependencies\"]))\n",
    "            fix_dependencies(child)\n",
    "        if len(input_dict[\"dependencies\"].keys()) < 1:\n",
    "            del input_dict[\"dependencies\"]\n",
    "    else:\n",
    "        if \"type\" in input_dict and input_dict[\"type\"] == \"array\":\n",
    "            fix_dependencies(input_dict[\"items\"])\n",
    "        if \"dependencies\" in input_dict:\n",
    "            del input_dict[\"dependencies\"]\n",
    "fix_dependencies(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: make sure 'enum' contain unique items, and remove all trailing white spaces\n",
    "def fix_enum(input_dict):\n",
    "    if \"type\" in input_dict and input_dict[\"type\"] == \"object\":        \n",
    "        for prop in input_dict[\"properties\"]:\n",
    "            fix_enum(input_dict[\"properties\"][prop])\n",
    "    elif \"type\" in input_dict and input_dict[\"type\"] == \"array\":\n",
    "        fix_enum(input_dict[\"items\"])\n",
    "            \n",
    "    if \"enum\" in input_dict:\n",
    "        for i, item in enumerate(input_dict[\"enum\"]):\n",
    "            input_dict[\"enum\"][i] = item.rstrip().lstrip()\n",
    "        input_dict[\"enum\"] = list(set(input_dict[\"enum\"]))\n",
    "fix_enum(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: manual (but reproducible) curations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "new_schema[\"$schema\"] = \"http://json-schema.org/draft-07/schema#\"\n",
    "new_schema[\"$schema_version\"] = \"2.0\"\n",
    "new_schema[\"$schema_created\"] = date2iso(now)\n",
    "\n",
    "# remove version, replace with created and modified (datetime)\n",
    "del new_schema[\"properties\"][\"version\"]\n",
    "new_schema[\"properties\"][\"created\"] = { \"type\": \"string\", \"format\": \"date-time\" }\n",
    "new_schema[\"properties\"][\"modified\"] = { \"type\": \"string\", \"format\": \"date-time\" }\n",
    "\n",
    "# require \"created\", \"modified\", \"general_params\", \"personal\"\n",
    "new_schema[\"required\"] = [\"created\", \"modified\", \"general_params\", \"personal\"]\n",
    "\n",
    "# require \"mibig_accession\", \"biosyn_class\", \"compounds\", \"publications\"\n",
    "new_schema[\"properties\"][\"general_params\"][\"required\"] = [\"mibig_accession\", \"biosyn_class\", \"compounds\", \"publications\"]\n",
    "\n",
    "# delete properties we don't need anymore (i.e. ones meant for AlpacaJS forms)\n",
    "del new_schema[\"properties\"][\"personal\"][\"properties\"][\"submitter_institution\"][\"format\"]\n",
    "search_and_delete(\"default\", new_schema)\n",
    "\n",
    "# rename Polyketide, NRP, RiPP, Terpene, Saccharide, Alkaloid, Other, to lowercases\n",
    "rename_key(\"Polyketide\", \"polyketide\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"NRP\", \"nrp\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"RiPP\", \"ripp\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Terpene\", \"terpene\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Saccharide\", \"saccharide\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Alkaloid\", \"alkaloid\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Other\", \"other\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "\n",
    "# fix publications.pattern (it doesn't cover doi entries)\n",
    "# for now, just delete the pattern matching constraint\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"publications\"][\"items\"][\"pattern\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.compounds ###\n",
    "\n",
    "# remove database_deposited and databases_deposited. we can always infer it from their respective accession ids\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"database_deposited\"]\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"databases_deposited\"]\n",
    "# delete old dependencies format for compounds\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"dependencies\"]\n",
    "# required = [\"compound\", \"evidence_struct\", \"chem_act\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"required\"] = [\"compound\", \"evidence_struct\", \"chem_act\"]\n",
    "# if chem_act == \"other\", require other_chem_act\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"chem_act\": {\"contains\":{\"enum\": [\"Other\"]}}},\n",
    "            \"required\": [\"chem_act\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"other_chem_act\"]\n",
    "          }\n",
    "})\n",
    "# delete old dependencies format for chem_moieties\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"chem_moieties\"][\"items\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"chem_moieties\"][\"items\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"chem_moieties\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"chem_moiety\": {\"enum\": [\"Other\"]}},\n",
    "            \"required\": [\"chem_moiety\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"other_chem_moiety\"]\n",
    "          }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.loci ###\n",
    "\n",
    "# move conn_comp_cluster from nucl_acc to loci (a cluster should only have one conn_comp_cluster)\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"conn_comp_cluster\"] = new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"][\"items\"][\"properties\"][\"conn_comp_cluster\"]\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"][\"items\"][\"properties\"][\"conn_comp_cluster\"]\n",
    "# loci.required = [\"complete\", \"nucl_acc\", \"conn_comp_cluster\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"required\"] = [\"complete\", \"nucl_acc\", \"conn_comp_cluster\"]\n",
    "# change 'Accession' to 'accession'\n",
    "rename_key(\"Accession\", \"accession\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"][\"items\"][\"properties\"])\n",
    "# nucl_acc.required = [\"accession\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"][\"items\"][\"required\"] = [\"accession\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.genes ###\n",
    "\n",
    "# operon.required = [\"operon_genes\", \"evidence_operon\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"operon\"][\"items\"][\"required\"] = [\"operon_genes\", \"evidence_operon\"]\n",
    "# gene.required = [\"gene_function\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"required\"] = [\"gene_function\"]\n",
    "# add nucl_acc, to specify which of the supplied accessions does this gene belongs to\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"properties\"][\"nucl_acc\"] = { \"type\": \"string\" }\n",
    "# delete gene.not_in_gbk, we can infer it from gene_id\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"properties\"][\"not_in_gbk\"]\n",
    "# delete old dependencies format, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"] = []\n",
    "\"\"\" -- disable this? then gene can be exist without positional information\n",
    "# if gene_id is not supplied, require gene_startpos,gene_endpos,gene_name,accession\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": {\"required\": [\"gene_id\"]}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"gene_startpos\", \"gene_endpos\", \"gene_name\", \"nucl_acc\"]\n",
    "          },\n",
    "})\n",
    "\"\"\"\n",
    "# if gene_function != Unknown, require evidence_genefunction\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": {\"properties\": {\"gene_function\": {\"enum\": [\"Unknown\"]}}},\n",
    "            \"required\": [\"gene_function\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"evidence_genefunction\"]\n",
    "          }\n",
    "})\n",
    "# if gene_function == Tailoring, require tailoring\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"gene_function\": {\"enum\": [\"Tailoring\"]}},\n",
    "            \"required\": [\"gene_function\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"tailoring\"]\n",
    "          }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO: fix Polyketide schema (requirements, dependencies)\\n# required = [\"pk_subclass\", \"pks_subclass\", \"lin_cycl_pk\", \"starter_unit\", \"pks_genes\", \"cyclases\"]\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"required\"] = [\"pk_subclass\", \"pks_subclass\", \"pks_te_type\", \"lin_cycl_pk\", \"starter_unit\", \"ketide_length\"]\\n\\n# delete old dependencies format, replace with allOf\\ndel new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"dependencies\"]\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"] = []\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\\n    \"if\": {\\n            \"properties\": {\"pks_subclass\": {\"enum\": [\"Modular type I\"]}},\\n            \"required\": [\"pks_subclass\"]\\n          },\\n          \"then\": {\\n            \"required\": [\"mod_pks_genes\"],\\n            \"properties\": {\"mod_pks_genes\": {\"minItems\": 1}}\\n          }\\n})\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\\n    \"if\": {\\n            \"properties\": {\"pks_subclass\": {\"enum\": [\"Trans-AT type I\"]}},\\n            \"required\": [\"pks_subclass\"]\\n          },\\n          \"then\": {\\n            \"required\": [\"trans_at\"],\\n            \"properties\": {\"trans_at\": {\"minItems\": 1}}\\n          }\\n})\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\\n    \"if\": {\\n            \"properties\": {\"pks_subclass\": {\"enum\": [\"Iterative type I\"]}},\\n            \"required\": [\"pks_subclass\"]\\n          },\\n          \"then\": {\\n            \"required\": [\"iterative_subtype\", \"nr_iterations\", \"iter_cycl_type\"]\\n          }\\n})\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\\n    \"if\": {\\n            \"properties\": {\"pks_subclass\": {\"enum\": [\"PUFA synthase or related\"]}},\\n            \"required\": [\"pks_subclass\"]\\n          },\\n          \"then\": {\\n            \"required\": [\"pufa_mod_doms\"],\\n            \"properties\": {\"pufa_mod_doms\": {\"minItems\": 1}}\\n          }\\n})\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\\n    \"if\": {\\n            \"not\": { \"properties\": {\"pks_te_type\": {\"enum\": [\"None\"]}} },\\n            \"required\": [\"pks_te_type\"]\\n          },\\n          \"then\": {\\n            \"required\": [\"pks_thioesterase\"],\\n            \"properties\": {\"pks_thioesterase\": {\"minItems\": 1}}\\n          }\\n})\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### general_params.polyketide ###\n",
    "# for now, just remove all dependencies and requirements\n",
    "search_and_delete(\"required\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"])\n",
    "search_and_delete(\"dependencies\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"])\n",
    "\n",
    "# add evidence_at_spec = \"None\"\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"properties\"][\"mod_pks_genes\"][\"items\"][\"properties\"][\"pks_module\"][\"items\"][\"properties\"][\"evidence_at_spec\"][\"enum\"].append(\"None\")\n",
    "\"\"\" TODO: fix Polyketide schema (requirements, dependencies)\n",
    "# required = [\"pk_subclass\", \"pks_subclass\", \"lin_cycl_pk\", \"starter_unit\", \"pks_genes\", \"cyclases\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"required\"] = [\"pk_subclass\", \"pks_subclass\", \"pks_te_type\", \"lin_cycl_pk\", \"starter_unit\", \"ketide_length\"]\n",
    "\n",
    "# delete old dependencies format, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"pks_subclass\": {\"enum\": [\"Modular type I\"]}},\n",
    "            \"required\": [\"pks_subclass\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"mod_pks_genes\"],\n",
    "            \"properties\": {\"mod_pks_genes\": {\"minItems\": 1}}\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"pks_subclass\": {\"enum\": [\"Trans-AT type I\"]}},\n",
    "            \"required\": [\"pks_subclass\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"trans_at\"],\n",
    "            \"properties\": {\"trans_at\": {\"minItems\": 1}}\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"pks_subclass\": {\"enum\": [\"Iterative type I\"]}},\n",
    "            \"required\": [\"pks_subclass\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"iterative_subtype\", \"nr_iterations\", \"iter_cycl_type\"]\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"pks_subclass\": {\"enum\": [\"PUFA synthase or related\"]}},\n",
    "            \"required\": [\"pks_subclass\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"pufa_mod_doms\"],\n",
    "            \"properties\": {\"pufa_mod_doms\": {\"minItems\": 1}}\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": { \"properties\": {\"pks_te_type\": {\"enum\": [\"None\"]}} },\n",
    "            \"required\": [\"pks_te_type\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"pks_thioesterase\"],\n",
    "            \"properties\": {\"pks_thioesterase\": {\"minItems\": 1}}\n",
    "          }\n",
    "})\n",
    "\"\"\"\n",
    "# --- todo: assert dependencies of pks_subtype --> requirements\n",
    "# --- todo: apply dependencies for mod_pks_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO: fix NRP schema (requirements, dependencies)\\n# delete old dependencies format, replace with allOf\\ndel new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"dependencies\"]\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"] = []\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"].append({\\n    \"if\": {\\n            \"not\": { \"properties\": {\"nrps_te_type\": {\"enum\": [\"None\"]}} },\\n            \"required\": [\"nrps_te_type\"]\\n          },\\n          \"then\": {\\n            \"required\": [\"nrps_thioesterase\"],\\n            \"properties\": {\"nrps_thioesterase\": {\"minItems\": 1}}\\n          }\\n})\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"].append({\\n    \"if\": {\\n            \"properties\": {\"subclass\": {\"enum\": [\"Other lipopeptide\"]}},\\n            \"required\": [\"subclass\"]\\n          },\\n          \"then\": {\\n            \"required\": [\"lipid_moiety\"]\\n          }\\n})\\n\\n# nrps_module -- required = [\"module_nr\", \"cdom_subtype\", \"nrps_mod_doms\"]\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"required\"] = [\"module_nr\", \"cdom_subtype\", \"nrps_mod_doms\"]\\n# nrps_module -- delete old dependencies, replace with allOf\\ndel new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"dependencies\"]\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"] = []\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"].append({\\n    \"if\": {\\n            \"properties\": {\"nrps_mod_doms\": {\"enum\": [\"Other\"]}},\\n            \"required\": [\"nrps_mod_doms\"]\\n          },\\n          \"then\": {\\n            \"required\": [\"nrps_other_mod_dom\"]\\n          }    \\n})\\nnew_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"].append({\\n    \"if\": {\\n            \"not\": {\"properties\": {\"nrps_mod_skip_iter\": {\"enum\": [\"Neither\"]}}},\\n            \"required\": [\"nrps_mod_skip_iter\"]\\n          },\\n          \"then\": {\\n            \"required\": [\"nrps_evidence_skip_iter\"]\\n          }    \\n})\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### general_params.nrp ###\n",
    "# for now, just remove all dependencies and requirements\n",
    "search_and_delete(\"required\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"])\n",
    "search_and_delete(\"dependencies\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"])\n",
    "\"\"\" TODO: fix NRP schema (requirements, dependencies)\n",
    "# delete old dependencies format, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": { \"properties\": {\"nrps_te_type\": {\"enum\": [\"None\"]}} },\n",
    "            \"required\": [\"nrps_te_type\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"nrps_thioesterase\"],\n",
    "            \"properties\": {\"nrps_thioesterase\": {\"minItems\": 1}}\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"subclass\": {\"enum\": [\"Other lipopeptide\"]}},\n",
    "            \"required\": [\"subclass\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"lipid_moiety\"]\n",
    "          }\n",
    "})\n",
    "\n",
    "# nrps_module -- required = [\"module_nr\", \"cdom_subtype\", \"nrps_mod_doms\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"required\"] = [\"module_nr\", \"cdom_subtype\", \"nrps_mod_doms\"]\n",
    "# nrps_module -- delete old dependencies, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"nrps_mod_doms\": {\"enum\": [\"Other\"]}},\n",
    "            \"required\": [\"nrps_mod_doms\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"nrps_other_mod_dom\"]\n",
    "          }    \n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": {\"properties\": {\"nrps_mod_skip_iter\": {\"enum\": [\"Neither\"]}}},\n",
    "            \"required\": [\"nrps_mod_skip_iter\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"nrps_evidence_skip_iter\"]\n",
    "          }    \n",
    "})\n",
    "\"\"\"\n",
    "## todo -- assert dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.saccharide ###\n",
    "\n",
    "# delete old dependencies format, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"gt_specificity\": {\"enum\": [\"Other\"]}},\n",
    "            \"required\": [\"gt_specificity\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"other_gt_spec\"]\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": { \"properties\": {\"gt_specificity\": {\"enum\": [\"Unknown\"]}} },\n",
    "            \"required\": [\"gt_specificity\"]\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"evidence_gt_spec\"]\n",
    "          }\n",
    "})\n",
    "\n",
    "# move 'gt_genes.sugar_subcluster' to sugar_subclusters, we can infer the specificities from the list of gene ids\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"sugar_subclusters\"] = {\n",
    "    \"title\": \"Sub-clusters for sugar biosynthesis\",\n",
    "    \"type\": \"array\",\n",
    "    \"items\": new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"properties\"][\"sugar_subcluster\"]\n",
    "}\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"properties\"][\"sugar_subcluster\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for all 'required' fields, if it is an array, specify 'minItems = 1' so that it can't be empty\n",
    "def apply_array_required(input_dict):\n",
    "    if \"properties\" in input_dict:\n",
    "        if \"required\" in input_dict:\n",
    "            for req in input_dict[\"required\"]:\n",
    "                if \"type\" in input_dict[\"properties\"][req] and input_dict[\"properties\"][req][\"type\"] == \"array\":\n",
    "                    if \"minItems\" not in input_dict[\"properties\"][req]:\n",
    "                        input_dict[\"properties\"][req][\"minItems\"] = 1\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            apply_array_required(input_dict[\"properties\"][key])\n",
    "    elif \"items\" in input_dict:\n",
    "        apply_array_required(input_dict[\"items\"])\n",
    "                                \n",
    "apply_array_required(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if class = \"Polyketide\", requires \"polyketide\" attribute, etc. except if minimal = true\n",
    "del new_schema[\"properties\"][\"general_params\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"allOf\"] = []\n",
    "prop_attr_pairs = [\n",
    "    (\"NRP\", \"nrp\"),\n",
    "    (\"Polyketide\", \"polyketide\"),\n",
    "    (\"RiPP\", \"ripp\"),\n",
    "    (\"Terpene\", \"terpene\"),\n",
    "    (\"Saccharide\", \"saccharide\"),\n",
    "    (\"Alkaloid\", \"alkaloid\"),\n",
    "    (\"Other\", \"other\")\n",
    "]\n",
    "for prop, attr in prop_attr_pairs:\n",
    "    then = { \"required\": [attr] }\n",
    "    sub_attr = new_schema[\"properties\"][\"general_params\"][\"properties\"][attr]\n",
    "    if \"required\" in sub_attr:\n",
    "        then[\"properties\"] = {}\n",
    "        then[\"properties\"][attr] = {\"required\": sub_attr[\"required\"]}\n",
    "        del sub_attr[\"required\"]\n",
    "    new_schema[\"properties\"][\"general_params\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "            \"not\": {\"properties\": {\"minimal\": {\"const\": True}}, \"required\": [\"minimal\"]},\n",
    "            \"properties\": {\"biosyn_class\": {\"contains\":{\"enum\": [prop]}}}\n",
    "          },\n",
    "          \"then\": then\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: save new schema\n",
    "with open(\"../../outputs/mibig_schema_draft7.json\", \"w\") as o:\n",
    "    o.write(json.dumps(new_schema, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### phase 5: transform data from phase 2 to match the new schema (include all dependencies/required keywords) ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ../../preprocessed/schema_draft7_properties.csv\n",
      "File written: ../../preprocessed/data_phase_3_vs_schema_draft7.csv\n",
      "File written: ../../preprocessed/bgc_to_fix_phase_5.csv\n"
     ]
    }
   ],
   "source": [
    "# 1: check data vs new schema to get quick overview of changed structures\n",
    "# use all_props from phase 3\n",
    "with open(\"../../outputs/mibig_schema_draft7.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_new_schema(json_obj, \"\", {})\n",
    "    with open(\"../../preprocessed/schema_draft7_properties.csv\", \"w\") as o:\n",
    "        for key in sorted(schema_props.keys()):\n",
    "            o.write(\"{}\\n\".format(key, schema_props[key]))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props_phase_3.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append((key, all_props_phase_3[key]))\n",
    "    with open(\"../../preprocessed/data_phase_3_vs_schema_draft7.csv\", \"w\") as o:\n",
    "        for rep in sorted(not_in_schema, key=lambda x: len(x[1]), reverse = True):\n",
    "            o.write(\"{},{}\\n\".format(rep[0], len(rep[1])))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    with open(\"../../preprocessed/bgc_to_fix_phase_5.csv\", \"w\") as o:\n",
    "        bgc_to_fix = {}\n",
    "        for rep in not_in_schema:\n",
    "            for bgc in rep[1]:\n",
    "                if bgc not in bgc_to_fix:\n",
    "                    bgc_to_fix[bgc] = []\n",
    "                bgc_to_fix[bgc].append(rep[0])\n",
    "        for bgc in bgc_to_fix:\n",
    "            o.write(\"{},{}\\n\".format(bgc, \";\".join(bgc_to_fix[bgc])))\n",
    "        print(\"File written: {}\".format(o.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conflicts: 0\n"
     ]
    }
   ],
   "source": [
    "# 2: fix data and assert that there is no more unrecognized attributes present\n",
    "def match_attributes_to_schema_7(data):\n",
    "    # remove 'version'\n",
    "    del_key(\"version\", data)\n",
    "    con_comp_temp = []\n",
    "    for nuc in data[\"general_params\"][\"loci\"][\"nucl_acc\"]:\n",
    "        # rename Accession to accession\n",
    "        rename_key(\"Accession\", \"accession\", nuc)\n",
    "        if \"conn_comp_cluster\" in nuc:\n",
    "            for con_comp in nuc[\"conn_comp_cluster\"]:\n",
    "                if con_comp not in con_comp_temp:\n",
    "                    con_comp_temp.append(con_comp)\n",
    "            del nuc[\"conn_comp_cluster\"]\n",
    "    # fix /general_params/loci/nucl_acc[]/conn_comp_cluster[]\n",
    "    if len(con_comp_temp) > 0:\n",
    "        data[\"general_params\"][\"loci\"][\"conn_comp_cluster\"] = con_comp_temp\n",
    "    # rename Polyketide, NRP, etc. to its lowercase version\n",
    "    rename_key(\"Polyketide\", \"polyketide\", data[\"general_params\"])\n",
    "    rename_key(\"NRP\", \"nrp\", data[\"general_params\"])\n",
    "    rename_key(\"RiPP\", \"ripp\", data[\"general_params\"])\n",
    "    rename_key(\"Terpene\", \"terpene\", data[\"general_params\"])\n",
    "    rename_key(\"Saccharide\", \"saccharide\", data[\"general_params\"])\n",
    "    rename_key(\"Alkaloid\", \"alkaloid\", data[\"general_params\"])\n",
    "    rename_key(\"Other\", \"other\", data[\"general_params\"])\n",
    "    for comp in data[\"general_params\"][\"compounds\"]:\n",
    "        # fix /general_params/compounds[]/database_deposited\n",
    "        if \"database_deposited\" in comp:\n",
    "            del comp[\"database_deposited\"]\n",
    "        # fix /general_params/compounds[]/databases_deposited[]\n",
    "        if \"databases_deposited\" in comp:\n",
    "            del comp[\"databases_deposited\"]\n",
    "    # fix /general_params/genes/gene[]/not_in_gbk\n",
    "    if \"genes\" in data[\"general_params\"]:\n",
    "        if \"gene\" in data[\"general_params\"][\"genes\"]:\n",
    "            for gen in data[\"general_params\"][\"genes\"][\"gene\"]:\n",
    "                del_key(\"not_in_gbk\", gen)\n",
    "    # fix /general_params/saccharide/gt_genes[]/sugar_subcluster[]\n",
    "    sugsub = []\n",
    "    if \"saccharide\" in data[\"general_params\"]:\n",
    "        if \"gt_genes\" in data[\"general_params\"][\"saccharide\"]:\n",
    "            for gtg in data[\"general_params\"][\"saccharide\"][\"gt_genes\"]:\n",
    "                if \"sugar_subcluster\" in gtg:\n",
    "                    sugsub.append(gtg[\"sugar_subcluster\"])\n",
    "                    del gtg[\"sugar_subcluster\"]\n",
    "        if len(sugsub) > 0:\n",
    "            data[\"general_params\"][\"saccharide\"][\"sugar_subclusters\"] = sugsub\n",
    "    return\n",
    "\n",
    "if not path.exists(\"../../preprocessed/json_1.4_phase_5/\"):\n",
    "    makedirs(\"../../preprocessed/json_1.4_phase_5/\")\n",
    "    \n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/json_1.4_phase_3/\")):\n",
    "    json_obj = None\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        match_attributes_to_schema_7(json_obj)\n",
    "        with open(path.join(\"../../preprocessed/json_1.4_phase_5/\", path.basename(json_path)), \"w\") as json_file:\n",
    "            json.dump(json_obj, json_file, indent=4, separators=(',', ': '))\n",
    "\n",
    "all_props_phase_5 = {}\n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/json_1.4_phase_5/\")):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        this_file_props = count_props(json_obj, \"\", {})\n",
    "        for prop in this_file_props:\n",
    "            if prop not in all_props_phase_5:\n",
    "                all_props_phase_5[prop] = [path.basename(json_path)]\n",
    "            else:\n",
    "                all_props_phase_5[prop].append(path.basename(json_path))\n",
    "\n",
    "with open(\"../../outputs/mibig_schema_draft7.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_new_schema(json_obj, \"\", {})\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props_phase_5.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append(key)\n",
    "            print(\"{},{}\".format(key, len(all_props_phase_5[key])))\n",
    "    print(\"Number of conflicts: {}\".format(len(not_in_schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: validate data using JSON Schema V7 validator\n",
    "with open(\"../../outputs/mibig_schema_draft7.json\") as json_file:\n",
    "    schema_obj = json.load(json_file)\n",
    "    validator = Draft7Validator(schema_obj)\n",
    "    errors = {}\n",
    "    errors_by_message = {}\n",
    "    for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/json_1.4_phase_5/\")):\n",
    "        break\n",
    "        bgc_id = path.basename(json_path)\n",
    "        print(\"Validating {}...\".format(bgc_id))\n",
    "        with open(json_path, \"r\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "            for error in sorted(validator.iter_errors(data), key=str):\n",
    "                # error by path\n",
    "                path_err = \".\".join([str(i) for i in list(error.schema_path)])\n",
    "                inst_err = \"\"\n",
    "                if not (isinstance(error.instance, dict) or isinstance(error.instance, list)):\n",
    "                    inst_err = str(error.instance)\n",
    "                if path_err not in errors:\n",
    "                    errors[path_err] = {\n",
    "                        \"files\": [],\n",
    "                        \"instances\": []\n",
    "                    }\n",
    "                if bgc_id not in errors[path_err][\"files\"]:\n",
    "                    errors[path_err][\"files\"].append(bgc_id)\n",
    "                if inst_err not in errors[path_err][\"instances\"]:\n",
    "                    errors[path_err][\"instances\"].append(inst_err)\n",
    "                # error by message\n",
    "                if error.message not in errors_by_message:\n",
    "                    errors_by_message[error.message] = []\n",
    "                if bgc_id not in errors_by_message[error.message]:\n",
    "                    errors_by_message[error.message].append(bgc_id)\n",
    "                    \n",
    "    #with open(\"../../preprocessed/phase_5_errors.tsv\", \"w\") as error_list:\n",
    "    #    for error in sorted(errors.keys(), reverse = True):\n",
    "    #        error_list.write(\"{}\\t{}\\t{}\\t{}\\n\".format(error, error.split(\".\")[-1], len(errors[error][\"files\"]), \";\".join(errors[error][\"instances\"])))\n",
    "            \n",
    "    #with open(\"../../preprocessed/phase_5_errors_by_message.tsv\", \"w\") as error_list:\n",
    "    #    for error in sorted(errors_by_message.keys(), reverse = True):\n",
    "    #        error_list.write(\"{}\\t{}\\n\".format(error, len(errors_by_message[error])))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated and fixed BGC0001445.json... Before 27 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001446.json... Before 11 error(s), After: 0 error(s)\n",
      "Missing publication: AHBX01000216\n",
      "Validated and fixed BGC0001447.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001448.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001449.json... Before 11 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001450.json... Before 7 error(s), After: 0 error(s)\n",
      "Missing publication: NZ_LGTG01000643\n",
      "Validated and fixed BGC0001451.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001452.json... Before 5 error(s), After: 0 error(s)\n",
      "Missing publication: LT989883\n",
      "Validated and fixed BGC0001453.json... Before 8 error(s), After: 0 error(s)\n",
      "Missing publication: LT989884\n",
      "Validated and fixed BGC0001454.json... Before 8 error(s), After: 0 error(s)\n",
      "Missing publication: LT989885\n",
      "Validated and fixed BGC0001455.json... Before 9 error(s), After: 0 error(s)\n",
      "Missing publication: LT989886\n",
      "Validated and fixed BGC0001456.json... Before 8 error(s), After: 0 error(s)\n",
      "Missing publication: LT990689\n",
      "Validated and fixed BGC0001457.json... Before 8 error(s), After: 0 error(s)\n",
      "Missing publication: NC_009380\n",
      "Validated and fixed BGC0001458.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001459.json... Before 4 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001460.json... Before 4 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001461.json... Before 4 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001462.json... Before 4 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001463.json... Before 3 error(s), After: 0 error(s)\n",
      "Missing publication: KR011923\n",
      "Validated and fixed BGC0001464.json... Before 8 error(s), After: 0 error(s)\n",
      "Missing publication: NC_015276\n",
      "Validated and fixed BGC0001465.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001466.json... Before 2 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001467.json... Before 11 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001468.json... Before 2 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001469.json... Before 18 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001470.json... Before 72 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001471.json... Before 2 error(s), After: 0 error(s)\n",
      "Missing publication: KC894738\n",
      "Validated and fixed BGC0001472.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001473.json... Before 2 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001474.json... Before 3 error(s), After: 0 error(s)\n",
      "Missing publication: NT_165929\n",
      "Validated and fixed BGC0001475.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001476.json... Before 2 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001477.json... Before 91 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001478.json... Before 3 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001479.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001480.json... Before 36 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001481.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001483.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001484.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001485.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001486.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001487.json... Before 7 error(s), After: 0 error(s)\n",
      "Missing publication: CM001025\n",
      "Validated and fixed BGC0001488.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001489.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001490.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001491.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001492.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001493.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001494.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001495.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001496.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001497.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001498.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001499.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001500.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001501.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001502.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001503.json... Before 10 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001504.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001505.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001506.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001507.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001508.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001509.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001510.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001511.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001512.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001513.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001514.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001515.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001516.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001517.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001518.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001519.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001520.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001521.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001522.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001523.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001524.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001525.json... Before 11 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001526.json... Before 19 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001527.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001528.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001529.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001530.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001531.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001532.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001533.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001534.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001535.json... Before 7 error(s), After: 0 error(s)\n",
      "Missing publication: KY810815\n",
      "Validated and fixed BGC0001536.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001537.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001538.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001539.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001540.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001541.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001542.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001543.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001544.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001545.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001546.json... Before 7 error(s), After: 0 error(s)\n",
      "Missing publication: LC102187\n",
      "Validated and fixed BGC0001547.json... Before 8 error(s), After: 0 error(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated and fixed BGC0001548.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001549.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001550.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001551.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001552.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001553.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001554.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001555.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001556.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001557.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001558.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001559.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001560.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001561.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001562.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001563.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001564.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001565.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001566.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001567.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001568.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001569.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001570.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001571.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001572.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001573.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001574.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001575.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001576.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001577.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001578.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001579.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001580.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001581.json... Before 8 error(s), After: 0 error(s)\n",
      "Missing publication: KJ144825\n",
      "Validated and fixed BGC0001582.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001583.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001584.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001585.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001586.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001587.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001588.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001589.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001590.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001591.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001592.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001593.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001594.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001595.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001596.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001597.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001598.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001599.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001600.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001601.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001602.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001603.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001604.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001605.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001606.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001607.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001608.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001609.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001610.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001611.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001612.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001613.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001614.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001615.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001616.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001617.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001618.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001619.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001620.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001621.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001622.json... Before 11 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001623.json... Before 7 error(s), After: 0 error(s)\n",
      "Missing publication: KU963195\n",
      "Validated and fixed BGC0001624.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001625.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001626.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001627.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001628.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001629.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001630.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001631.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001632.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001633.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001634.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001635.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001636.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001637.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001638.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001639.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001640.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001641.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001642.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001643.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001644.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001645.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001646.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001647.json... Before 12 error(s), After: 0 error(s)\n",
      "Missing publication: KT591188\n",
      "Validated and fixed BGC0001648.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001649.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001650.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001651.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001652.json... Before 15 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001653.json... Before 10 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001654.json... Before 7 error(s), After: 0 error(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated and fixed BGC0001655.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001656.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001657.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001658.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001659.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001660.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001661.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001662.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001663.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001664.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001665.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001666.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001667.json... Before 20 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001668.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001669.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001670.json... Before 7 error(s), After: 0 error(s)\n",
      "Missing publication: KY452017\n",
      "Validated and fixed BGC0001671.json... Before 8 error(s), After: 0 error(s)\n",
      "Missing publication: KY502195\n",
      "Validated and fixed BGC0001672.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001673.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001674.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001675.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001676.json... Before 15 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001677.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001678.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001679.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001680.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001681.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001682.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001683.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001684.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001685.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001686.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001687.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001688.json... Before 7 error(s), After: 0 error(s)\n",
      "Missing publication: MF589902\n",
      "Validated and fixed BGC0001689.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001690.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001691.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001692.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001693.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001694.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001695.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001696.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001697.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001698.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001699.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001700.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001701.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001702.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001703.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001704.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001705.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001706.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001707.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001708.json... Before 7 error(s), After: 0 error(s)\n",
      "Missing publication: MG742725\n",
      "Validated and fixed BGC0001709.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001710.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001711.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001712.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001713.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001714.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001715.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001716.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001717.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001718.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001719.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001720.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001721.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001722.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001723.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001724.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001725.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001726.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001727.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001728.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001729.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001730.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001731.json... Before 7 error(s), After: 0 error(s)\n",
      "Missing publication: KJ721165\n",
      "Validated and fixed BGC0001732.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001733.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001734.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001735.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001736.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001737.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001738.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001739.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001740.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001741.json... Before 10 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001742.json... Before 8 error(s), After: 0 error(s)\n",
      "Missing publication: NDXC01000075\n",
      "Validated and fixed BGC0001743.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001745.json... Before 11 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001746.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001747.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001748.json... Before 18 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001749.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001750.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001751.json... Before 10 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001752.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001753.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001754.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001755.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001756.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001757.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001758.json... Before 11 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001759.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001760.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001761.json... Before 11 error(s), After: 0 error(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated and fixed BGC0001762.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001763.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001764.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001765.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001766.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001767.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001768.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001769.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001770.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001771.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001772.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001773.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001774.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001775.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001776.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001777.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001778.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001779.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001780.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001781.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001782.json... Before 11 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001783.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001784.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001785.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001786.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001787.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001788.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001789.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001790.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001791.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001792.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001793.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001794.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001795.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001796.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001797.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001798.json... Before 13 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001799.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001800.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001801.json... Before 15 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001802.json... Before 9 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001803.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001804.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001805.json... Before 7 error(s), After: 0 error(s)\n",
      "Missing publication: KX432141\n",
      "Validated and fixed BGC0001806.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001807.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001808.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001809.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001810.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001811.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001812.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001813.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001814.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001815.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001816.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001817.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001818.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001819.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001820.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001821.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001822.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001823.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001824.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001825.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001826.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001827.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001828.json... Before 7 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001829.json... Before 8 error(s), After: 0 error(s)\n",
      "Validated and fixed BGC0001830.json... Before 12 error(s), After: 0 error(s)\n",
      "All data validated!\n"
     ]
    }
   ],
   "source": [
    "# 4: fix conflicts, then save to final output folder\n",
    "def fix_data_new_schema(data, error):\n",
    "    if len(error.path) < 1:\n",
    "        # problem is in root, need a separate approach\n",
    "        if error.validator == \"required\":\n",
    "            missing_keys = set(error.validator_value) - set(data.keys())\n",
    "            if \"created\" in missing_keys:\n",
    "                data[\"created\"] = date2iso(now)\n",
    "            if \"modified\" in missing_keys:\n",
    "                data[\"modified\"] = date2iso(now)\n",
    "            if \"personal\" in missing_keys:\n",
    "                data[\"personal\"] = {\n",
    "                    \"submitter_name\": \"mibig.secondarymetabolites.org\",\n",
    "                    \"submitter_institution\": \"MIBiG\",\n",
    "                    \"submitter_email\": \"info@mibig.secondarymetabolites.org\"\n",
    "                }\n",
    "    else:        \n",
    "        # get problematic parent instance from data (so that we can fix it)\n",
    "        error_container = data\n",
    "        error_container_parent = None # for catching grandparent\n",
    "        error_container_attribute = None\n",
    "        while len(error.path) > 1:\n",
    "            if len(error.path) == 2:\n",
    "                error_container_parent = error_container\n",
    "            error_container_attribute = error.path.popleft()\n",
    "            error_container = error_container[error_container_attribute] # parent node containing the error instance\n",
    "        error_attribute = error.path.popleft() # attribute from parent node containing the error instance\n",
    "\n",
    "        if isinstance(error_container, ToDelete):\n",
    "            return\n",
    "        elif isinstance(error_container[error_attribute], ToDelete):\n",
    "            return\n",
    "        \n",
    "        # function to replace attribute values\n",
    "        def replace_attr(attr_pairs):\n",
    "            for attr_from, attr_to in attr_pairs:\n",
    "                if error.instance == attr_from:\n",
    "                    error_container[error_attribute] = attr_to\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        # fix type errors (should be generalizable)\n",
    "        if error.validator == \"type\":\n",
    "            if error.validator_value == \"integer\":\n",
    "                try:\n",
    "                    error_container[error_attribute] = int(error.instance)\n",
    "                    return\n",
    "                except:\n",
    "                    # may need tailored fix\n",
    "                    pass\n",
    "            elif error.validator_value == \"number\":\n",
    "                try:\n",
    "                    error_container[error_attribute] = float(error.instance)\n",
    "                    return\n",
    "                except:\n",
    "                    # may need tailored fix\n",
    "                    pass\n",
    "            elif error.validator_value == \"string\":\n",
    "                # fix mut_pheno, error list shown that these are None/null values, delete the attribute instead\n",
    "                if error_attribute == \"mut_pheno\":\n",
    "                    del error_container[error_attribute]\n",
    "                # fix gene_function\n",
    "                elif error_attribute == \"gene_function\":\n",
    "                    error_container[error_attribute] = \"Unknown\"\n",
    "                # fix gene_annotation, name, id\n",
    "                elif error_attribute in [\"gene_annotation\", \"gene_name\", \"gene_id\"]:\n",
    "                    del error_container[error_attribute]\n",
    "                # fix comments, gene_comments\n",
    "                elif error_attribute in [\"comments\", \"gene_comments\"]:\n",
    "                    error_container[error_attribute] = \"\"\n",
    "\n",
    "        # fix minimum errors (generalizable, but needs to be careful)\n",
    "        elif error.validator == \"minimum\":\n",
    "            # fix gene.startpos\n",
    "            if error_attribute == \"gene_startpos\":\n",
    "                error_container[error_attribute] = 0\n",
    "            # fix gene.endpos\n",
    "            if error_attribute == \"gene_endpos\":\n",
    "                error_container[error_attribute] = 0\n",
    "\n",
    "        # fix minItems errors (generalizable but limited e.g. need to consider 'required')\n",
    "        elif error.validator == \"minItems\":\n",
    "            # fix operon_genes = [], then delete operon\n",
    "            if error_attribute == \"operon_genes\":\n",
    "                del data[\"general_params\"][\"genes\"][\"operon\"]\n",
    "            # fix ripp..gene_id = []\n",
    "            elif error_attribute == \"gene_id\":\n",
    "                del data[\"general_params\"][\"ripp\"][\"precursor_loci\"]\n",
    "                data[\"general_params\"][\"minimal\"] = True\n",
    "                \n",
    "        # fix enum errors (semi-generalizable, needs to know what to put in place of the wrong value)\n",
    "        elif error.validator == \"enum\":\n",
    "            # fix biosyn_class\n",
    "            if error_container_attribute == \"biosyn_class\":\n",
    "                attr_pairs = []\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    pass\n",
    "            # fix genes..gene_function\n",
    "            if error_attribute == \"gene_function\":\n",
    "                attr_pairs = [\n",
    "                    (\"\", \"Unknown\"),\n",
    "                    (\"None\", \"Unknown\"),\n",
    "                    (\"Scaffold Biosynthesis\", \"Scaffold biosynthesis\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix genes..tailoring\n",
    "            if error_attribute == \"tailoring\":\n",
    "                attr_pairs = [\n",
    "                    (\"None\", \"Unknown\"),\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    pass\n",
    "            # fix genes..evidence_genefunction\n",
    "            if error_container_attribute == \"evidence_genefunction\":\n",
    "                evidences = []\n",
    "                errorinstance = error.instance.replace(\" \", \"\").replace(\"-\", \"\").lower()\n",
    "                if \"activityassay\" in errorinstance:\n",
    "                    evidences.append(\"Activity assay\")\n",
    "                if \"knockout\" in errorinstance:\n",
    "                    evidences.append(\"Knock-out\")\n",
    "                if \"invivo\" in errorinstance:\n",
    "                    evidences.append(\"Other in vivo study\")\n",
    "                if \"sequence\" in errorinstance:\n",
    "                    evidences.append(\"Sequence-based prediction\")\n",
    "                if \"expression\" in errorinstance:\n",
    "                    evidences.append(\"Heterologous expression\")\n",
    "                if len(evidences) < 1:\n",
    "                    evidences.append(\"Other\")\n",
    "                error_container[error_attribute] = evidences[0]\n",
    "                if len(evidences) > 1:\n",
    "                    error_container.extend(evidences)\n",
    "            # fix loci..complete\n",
    "            elif error_attribute == \"complete\":\n",
    "                if error.instance == \"partial\":\n",
    "                    error_container[error_attribute] = \"incomplete\"\n",
    "            # fix loci..conn_comp_cluster\n",
    "            elif error_container_attribute == \"conn_comp_cluster\":\n",
    "                attr_pairs = [\n",
    "                    (\"Proven expression in natural host\", \"Gene expression correlated with compound production\"),\n",
    "                    (\"Knock-outstudies\", \"Knock-out studies\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix compounds..chem_act\n",
    "            elif error_container_attribute == \"chem_act\":\n",
    "                attr_pairs = [\n",
    "                    (\"\", \"Unknown\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    error_container_parent[\"other_chem_act\"] = error.instance\n",
    "                    pass\n",
    "            # fix compounds..chem_moiety\n",
    "            elif error_attribute == \"chem_moiety\":\n",
    "                attr_pairs = []\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    error_container[\"other_chem_moiety\"] = error.instance\n",
    "                    pass                \n",
    "            # fix compounds.mass_ion_type\n",
    "            elif error_attribute == \"mass_ion_type\":\n",
    "                error_container[error_attribute] = \"Other\"\n",
    "            # fix polyketide.pks_subclass\n",
    "            elif error_container_attribute == \"pks_subclass\":\n",
    "                attr_pairs = [\n",
    "                    (\"Type I\", \"Modular type I\"), # (aculeximycin)\n",
    "                    (\"Iterative typeI\", \"Iterative type I\"),\n",
    "                    (\"Modular Type I\", \"Modular type I\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix polyketide.starter_unit\n",
    "            elif error_attribute == \"starter_unit\":\n",
    "                attr_pairs = [\n",
    "                    (\"methylmalonate-CoA\", \"Other\"),\n",
    "                    (\"None\", \"Unknown\"),\n",
    "                    (\"4-hydroxyphenylpyruvate\", \"Other\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix polyketide.pks_te_type\n",
    "            elif error_attribute == \"pks_te_type\":\n",
    "                attr_pairs = [\n",
    "                    (\"other\", \"Other\"),\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix polyketide..pks_domains\n",
    "            elif error_container_attribute == \"pks_domains\":\n",
    "                attr_pairs = [\n",
    "                    (\"AT\", \"Acyltransferase\"),\n",
    "                    (\"DH\", \"Dehydratase\"),\n",
    "                    (\"KR\", \"Ketoreductase\"),\n",
    "                    (\"ACP\", \"Thiolation (ACP/PCP)\"),\n",
    "                    (\"PCP\", \"Thiolation (ACP/PCP)\"),\n",
    "                    (\"T\", \"Thiolation (ACP/PCP)\"),\n",
    "                    (\"CAL\", \"CoA-ligase\"),\n",
    "                    (\"ER\", \"Enoylreductase\"),\n",
    "                    (\"KS\", \"Ketosynthase\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    # what to do? TE;PT;SAT;PPTASE;A;E;TE/CLC;CMET;FAAL;ST;C;TR\n",
    "                    # (lazily) delete?\n",
    "                    error_container[error_attribute] = ToDelete()\n",
    "                    pass\n",
    "            # fix mod_pks_genes..kr_stereochem, A->S->D-OH, B->R->L-OH\n",
    "            elif error_attribute == \"kr_stereochem\":\n",
    "                attr_pairs = [\n",
    "                    (\"A-group\", \"D-OH\"),\n",
    "                    (\"B-group\", \"L-OH\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix mod_pks_genes..at_substr_spec\n",
    "            elif error_attribute == \"at_substr_spec\":\n",
    "                attr_pairs = [\n",
    "                    (\"malonyl-CoA\", \"Malonyl-CoA\"),\n",
    "                    (\"methylmalonyl-CoA\", \"Methylmalonyl-CoA\"),\n",
    "                    (\"Malonyl-CoA/Malonyl-CoA/Malonyl-CoA\", \"Malonyl-CoA\"),\n",
    "                    (\"Methylmalonyl-CoA/Methylmalonyl-CoA\", \"Methylmalonyl-CoA\"),\n",
    "                    (\"N/A\", \"None\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    if error.instance == \"Acetyl-CoA/Methylmalonyl-CoA\":\n",
    "                        error_container[error_attribute] = \"Multiple (promiscuous)\"\n",
    "                        error_container[\"at_multiple_spec\"] = [\"Acetyl-CoA\", \"Methylmalonyl-CoA\"]\n",
    "                    elif error.instance == \"Methylmalonyl-CoA/Malonyl-CoA\":\n",
    "                        error_container[error_attribute] = \"Multiple (promiscuous)\"\n",
    "                        error_container[\"at_multiple_spec\"] = [\"Malonyl-CoA\", \"Methylmalonyl-CoA\"]\n",
    "                    elif error.instance == \"Acetyl-CoA + Malonyl CoA\":\n",
    "                        error_container[error_attribute] = \"Multiple (promiscuous)\"\n",
    "                        error_container[\"at_multiple_spec\"] = [\"Malonyl-CoA\", \"Acetyl-CoA\"]\n",
    "                    else:\n",
    "                        # what to do? 4-hydroxyphenylpyruvate;Various atypical acyl-CoAs;phenylacetate-like;Decanoyl-CoA\n",
    "                        # set to 'Other'\n",
    "                        error_container[error_attribute] = \"Other\"\n",
    "                    pass\n",
    "            # fix mod_pks_genes..evidence_at_spec\n",
    "            elif error_attribute == \"evidence_at_spec\":\n",
    "                attr_pairs = [\n",
    "                    (\"Feeding study\", \"Other\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix mod_pks_genes..pks_mod_doms\n",
    "            elif error_container_attribute == \"pks_mod_doms\":\n",
    "                attr_pairs = []\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    error_container_parent[\"pks_other_mod_dom\"] = error.instance\n",
    "                    pass\n",
    "            # fix nrps_modules..nrps_mod_doms\n",
    "            elif error_attribute == \"nrps_mod_doms\":\n",
    "                attr_pairs = []\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    error_container[\"nrps_other_mod_dom\"] = error.instance\n",
    "                    pass\n",
    "            # fix nrps_modules..prot_adom_spec\n",
    "            elif error_attribute == \"prot_adom_spec\":\n",
    "                if error.instance == \"Asparigine\":\n",
    "                    error_container[error_attribute] = \"Asparagine\"\n",
    "            # fix nrp..cdom_subtype\n",
    "            elif error_attribute == \"cdom_subtype\":\n",
    "                attr_pairs = [\n",
    "                    (\"N/A\", \"Unknown\"),\n",
    "                    (\"None\", \"Unknown\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix nrp..nonprot_adom_spec\n",
    "            elif error_attribute == \"nonprot_adom_spec\":\n",
    "                # a bit too random, consider freeforming this?\n",
    "                error_container[error_attribute] = \"Other\"\n",
    "                error_container[\"other_spec\"] = error.instance\n",
    "            # fix ripp..ripp_subclass\n",
    "            elif error_attribute == \"ripp_subclass\":\n",
    "                attr_pairs = [\n",
    "                    (\"Lantipeptide\", \"Lanthipeptide\"),\n",
    "                    (\"Head-To-Tail Cyclized Peptide\", \"Head-to-tailcyclized peptide\"),\n",
    "                    (\"Lap\", \"LAP\"),\n",
    "                    (\"Lap / Microcin\", \"LAP\"),\n",
    "                    (\"Lasso Peptide\", \"Lassopeptide\"),\n",
    "                    (\"None\", \"Other\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix ripp..lin_cycl_ripp\n",
    "            elif error_attribute == \"lin_cycl_ripp\":\n",
    "                attr_pairs = [\n",
    "                    (\"linear\", \"Linear\")\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "            # fix saccharide..saccharide_subclass\n",
    "            elif error_attribute == \"saccharide_subclass\":\n",
    "                if error.instance == \"hyrbid/tailoring\":\n",
    "                    error_container[error_attribute] = \"hybrid/tailoring\"\n",
    "            # fix saccharide..gt_specificity\n",
    "            elif error_attribute == \"gt_specificity\":\n",
    "                if error.instance == \"None\":\n",
    "                    error_container[error_attribute] = \"Unknown\"\n",
    "                else:\n",
    "                    error_container[error_attribute] = \"Other\"\n",
    "                    error_container[\"other_gt_spec\"] = error.instance\n",
    "            # fix saccharide..evidence_gt_spec\n",
    "            elif error_attribute == \"evidence_gt_spec\":\n",
    "                if error.instance == \"structure-based inference\":\n",
    "                    error_container[error_attribute] = \"Structure-based inference\"                \n",
    "            # fix other.other_subclass\n",
    "            elif error_attribute == \"other_subclass\":\n",
    "                error_container[error_attribute] = \"Other\"\n",
    "            ## try to fix \"None\"/\"N/A\"/\"\" --> \"Unknown\"\n",
    "            else:\n",
    "                attr_to_use = \"Unknown\"\n",
    "                use_others_instead = [\n",
    "                    \"crosslink_type\",\n",
    "                    \"evidence_a_spec\",\n",
    "                    \"nrps_evidence_skip_iter\",\n",
    "                    \"terpene_subclass\",\n",
    "                    \"terpene_c_len\",\n",
    "                    \"terpene_precursor\",\n",
    "                    \"pk_subclass\",\n",
    "                    \"subclass\",\n",
    "                    \"pks_evidence_skip_iter\"\n",
    "                ]\n",
    "                if error_attribute in use_others_instead:\n",
    "                    attr_to_use = \"Other\"\n",
    "                attr_pairs = [\n",
    "                    (\"N/A\", attr_to_use),\n",
    "                    (\"None\", attr_to_use),\n",
    "                    (\"\", attr_to_use)\n",
    "                ]\n",
    "                if not replace_attr(attr_pairs):\n",
    "                    pass\n",
    "                \n",
    "        # fix requirement errors (needs hand-on approach)        \n",
    "        elif error.validator == \"required\":\n",
    "            missing_keys = set(error.validator_value) - set(error_container[error_attribute].keys())\n",
    "            # fix saccharide subclasses\n",
    "            if \"saccharide\" in missing_keys:\n",
    "                # if PKS+Saccharide, assume it is a tailoring GT\n",
    "                if \"Polyketide\" in data[\"general_params\"][\"biosyn_class\"]:\n",
    "                    data[\"general_params\"][\"saccharide\"] = { \"saccharide_subclass\": \"hybrid/tailoring\" }\n",
    "                # if NR+Saccharide, assume it is a tailoring GT\n",
    "                elif \"NRP\" in data[\"general_params\"][\"biosyn_class\"]:\n",
    "                    data[\"general_params\"][\"saccharide\"] = { \"saccharide_subclass\": \"hybrid/tailoring\" }\n",
    "            # fix other subclasses\n",
    "            if \"other\" in missing_keys:\n",
    "                data[\"general_params\"][\"other\"] = {\"other_subclass\": \"Other\"}\n",
    "            # fix nrp subclasses\n",
    "            if \"nrp\" in missing_keys:\n",
    "                data[\"general_params\"][\"nrp\"] = {}\n",
    "            # fix polyketide subclasses\n",
    "            if \"polyketide\" in missing_keys:\n",
    "                data[\"general_params\"][\"polyketide\"] = {}\n",
    "            # fix publication\n",
    "            if \"publications\" in missing_keys:\n",
    "                ncbi_acc = data[\"general_params\"][\"loci\"][\"nucl_acc\"][0][\"accession\"]\n",
    "                def fix_publications(acc_pub_pairs):\n",
    "                    for ncbi_accs, pubs in acc_pub_pairs:\n",
    "                        if ncbi_acc in ncbi_accs:\n",
    "                            error_container[error_attribute][\"publications\"] = pubs\n",
    "                            return True\n",
    "                    return False\n",
    "                acc_pub_pairs = [\n",
    "                    ([\"AY510455\"], [\"16108793\"]), # aflatoxin\n",
    "                    ([\"AY092402\"], [\"19537208\"]), # aflatoxin/sterigmatocystin\n",
    "                    ([\"GP697151\"], [\"patent:US7595187\"]), # Elaiophylin\n",
    "                    ([\"FM173265\"], [\"19025863\"]), # lasalocid\n",
    "                    ([\"BD420675\"], [\"patent:CN1896226B\"]), # Midecamycin\n",
    "                    ([\"AB363939\"], [\"3372359\"]), # nemadectin (LL-F28249)\n",
    "                    ([\"CP000850\"], [\"19474814\"]), # rifamycins\n",
    "                    ([\"FN565166\"], [\"20140934\"]), # chrysomycin\n",
    "                    ([\"EU220288\", \"EU232693\"], [\"18802638\"]), # anglomycin\n",
    "                    ([\"AF141924\", \"AF141925\"], [\"10334994\"]), # lovastatin\n",
    "                    ([\"AY228175\"], [\"10.1021/ja00275a058\"]), # kinamycin\n",
    "                    ([\"AF323753\"], [\"11683270\"]), # nogalamycin\n",
    "                    ([\"BD251846\"], [\"patent:JP2002528068A\"]), # another nogalamycin?                    \n",
    "                    ([\"EF151801\"], [\"25677666\"]), # pradimicin                \n",
    "                    ([\"DQ266254\"], [\"17381736\"]), # prodigiosin                \n",
    "                    ([\"FN565485\"], [\"20140934\"]), # ravidomycin\n",
    "                    ([\"AF293355\"], [\"26433383\"]), # rubrinomycin\n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"\"], [\"\"]), # \n",
    "                    ([\"FJ719113\"], [\"19621341\"]) # erdacin\n",
    "                ]\n",
    "                if not fix_publications(acc_pub_pairs):\n",
    "                    print(\"Missing publication: {}\".format(ncbi_acc))\n",
    "                    # for now, fill with empty string (let's tackle other issues first)\n",
    "                    error_container[error_attribute][\"publications\"] = [\"\"]\n",
    "                if ncbi_acc in [\"AF141924\", \"AF141925\"]: # lovastatin (2 of them?), note: accession is obsolete! replace with AH007774\n",
    "                    data[\"general_params\"][\"loci\"][\"nucl_acc\"][0][\"accession\"] = \"AH007774\"\n",
    "                elif ncbi_acc == \"AY228175\": # kinamycin accession is obsolete! replace with AH012623\n",
    "                    data[\"general_params\"][\"loci\"][\"nucl_acc\"][0][\"accession\"] = \"AH012623\"\n",
    "                    \n",
    "            # fix compounds.evidence_struct, set to 'Other'\n",
    "            if \"evidence_struct\" in missing_keys:\n",
    "                error_container[error_attribute][\"evidence_struct\"] = [\"Other\"]\n",
    "            # fix compounds.chem_act, set to 'Unknown'\n",
    "            if \"chem_act\" in missing_keys:\n",
    "                error_container[error_attribute][\"chem_act\"] = [\"Unknown\"]\n",
    "            # fix compounds.other_chem_act, set to ''\n",
    "            if \"other_chem_act\" in missing_keys:\n",
    "                error_container[error_attribute][\"other_chem_act\"] = \"\"\n",
    "            # fix compounds..other_chem_moiety\n",
    "            if \"other_chem_moiety\" in missing_keys:\n",
    "                error_container[error_attribute][\"other_chem_moiety\"] = \"\"\n",
    "            # fix loci.evidence_struct, set to 'Other'\n",
    "            if \"conn_comp_cluster\" in missing_keys:\n",
    "                error_container[error_attribute][\"conn_comp_cluster\"] = [\"Other\"]\n",
    "            # fix loci.complete, set to 'unknown'\n",
    "            if \"complete\" in missing_keys:\n",
    "                error_container[error_attribute][\"complete\"] = \"unknown\"\n",
    "            # fix genes..tailoring\n",
    "            if \"tailoring\" in missing_keys:\n",
    "                error_container[error_attribute][\"gene_function\"] = \"Unknown\"\n",
    "            # fix genes..gene_function\n",
    "            if \"gene_function\" in missing_keys:\n",
    "                error_container[error_attribute][\"gene_function\"] = \"Unknown\"\n",
    "            # fix genes..evidence_genefunction\n",
    "            if \"evidence_genefunction\" in missing_keys:\n",
    "                if \"mut_pheno\" in error_container[error_attribute]:\n",
    "                    error_container[error_attribute][\"evidence_genefunction\"] = [\"Knock-out\"]\n",
    "                else:\n",
    "                    error_container[error_attribute][\"evidence_genefunction\"] = [\"Other\"]\n",
    "            # fix genes..evidence_operon\n",
    "            if \"evidence_operon\" in missing_keys:\n",
    "                error_container[error_attribute][\"evidence_operon\"] = \"Other\"\n",
    "            # fix saccharide..evidence_gt_spec\n",
    "            if \"evidence_gt_spec\" in missing_keys:\n",
    "                error_container[error_attribute][\"evidence_gt_spec\"] = \"Other\"\n",
    "            # fix saccharide..gt_gene\n",
    "            if \"gt_gene\" in missing_keys:\n",
    "                error_container_parent[error_container_attribute] = ToDelete()\n",
    "            # fix ripp..precursor_loci by setting minimal=True (no good solution otherwise)\n",
    "            if \"precursor_loci\" in missing_keys:\n",
    "                data[\"general_params\"][\"minimal\"] = True\n",
    "                \n",
    "        # fix pattern errors (needs hand-on approach)        \n",
    "        elif error.validator == \"pattern\":                \n",
    "            # fix pks and nrp module_nr\n",
    "            if error_attribute == \"module_nr\":\n",
    "                # X/x, let's delete it for now (?)\n",
    "                del error_container[error_attribute]\n",
    "    return\n",
    "\n",
    "if not path.exists(\"../../outputs/json_2.0/\"):\n",
    "    makedirs(\"../../outputs/json_2.0/\")\n",
    "\n",
    "with open(\"../../outputs/mibig_schema_draft7.json\") as json_file:\n",
    "    schema_obj = json.load(json_file)\n",
    "    validator = Draft7Validator(schema_obj)\n",
    "    errors = {}\n",
    "    for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/json_1.4_phase_5/\")):\n",
    "        bgc_id = path.basename(json_path)\n",
    "        id_int = int(bgc_id[3:-5])\n",
    "        if (id_int < last_error) and True:\n",
    "            continue\n",
    "        with open(json_path, \"r\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "            error_counts_before = 0\n",
    "            error_counts_after = 0\n",
    "            for error in sorted(validator.iter_errors(data), key=str):\n",
    "                fix_data_new_schema(data, error)\n",
    "                error_counts_before += 1\n",
    "            lazily_deletes(data)\n",
    "            error_counts = 0\n",
    "            for error in sorted(validator.iter_errors(data), key=str):\n",
    "                print(error.path)\n",
    "                error_counts_after += 1\n",
    "            print(\"Validated and fixed {}... Before {} error(s), After: {} error(s)\".format(bgc_id, error_counts_before, error_counts_after))\n",
    "            with open(path.join(\"../../outputs/json_2.0/\", bgc_id), \"w\") as jo:\n",
    "                json.dump(data, jo, indent=4, separators=(',', ': '))\n",
    "            if error_counts_after > 0:\n",
    "                last_error = id_int\n",
    "                exit(1)\n",
    "    print(\"All data validated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
