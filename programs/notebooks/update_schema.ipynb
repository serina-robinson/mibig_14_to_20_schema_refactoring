{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase 0: compare old schema vs old data, output the summary in an excel file* to help decision making\n",
    "# phase 1: transform old schema to match (correct version) of old data\n",
    "# phase 2: compare phase 1 schema vs old data, output the summary in an excel file* to help decision making\n",
    "# phase 3: transform old data to match schema from phase 1 (doesn't count dependencies/required keywords)\n",
    "# phase 4: transform schema from phase 1 to match JSON Schema draft v7 (we will call it 'new schema')\n",
    "# phase 5: transform data from phase 2 to match the new schema (include all dependencies/required keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## common imports ##\n",
    "from os import path, makedirs\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## common functions ##\n",
    "def fetch_mibig_json_filepaths(dir_path):\n",
    "    \"\"\"fetch mibig json paths from a specific folder\"\"\"\n",
    "    return glob.glob(path.join(dir_path, \"BGC*.json\"))\n",
    "\n",
    "def count_props(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json, construct a list of property paths\n",
    "    along with its presence count in the json object\"\"\"\n",
    "    key_path = cur_path\n",
    "    \n",
    "    if isinstance(input_dict, dict):\n",
    "        for key in input_dict.keys():\n",
    "            result = count_props(input_dict[key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif isinstance(input_dict, list):\n",
    "        key_path = \"{}[]\".format(key_path)\n",
    "        for node in input_dict:\n",
    "            result = count_props(node, \"{}\".format(key_path), result)\n",
    "\n",
    "    if not isinstance(input_dict, dict):\n",
    "        if key_path not in result:\n",
    "            result[key_path] = 0\n",
    "        result[key_path] += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_props_old_schema(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json schema, construct a list of property paths\n",
    "    along with either required == True for each properties\"\"\"\n",
    "    key_path = cur_path\n",
    "    if (\"type\" not in input_dict) or (input_dict[\"type\"] not in [\"object\", \"array\"]):\n",
    "        key_path = \"{}\".format(cur_path) # string / etc.\n",
    "    elif input_dict[\"type\"] == \"object\":\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            result = fetch_props_old_schema(input_dict[\"properties\"][key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif input_dict[\"type\"] == \"array\":\n",
    "        key_path = \"{}[]\".format(cur_path)\n",
    "        result = fetch_props_old_schema(input_dict[\"items\"], \"{}\".format(key_path), result)\n",
    "    \n",
    "    if key_path not in result and \"properties\" not in input_dict:\n",
    "        result[key_path] = \"required\" in input_dict and input_dict[\"required\"] == True\n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_props_new_schema(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json draft7 schema, construct a list of property paths\n",
    "    along with either required == True for each properties\"\"\"\n",
    "    key_path = cur_path\n",
    "    if (\"type\" not in input_dict) or (input_dict[\"type\"] not in [\"object\", \"array\"]):\n",
    "        key_path = \"{}\".format(cur_path) # string / etc.\n",
    "    elif input_dict[\"type\"] == \"object\":\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            result = fetch_props_old_schema(input_dict[\"properties\"][key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif input_dict[\"type\"] == \"array\":\n",
    "        key_path = \"{}[]\".format(cur_path)\n",
    "        result = fetch_props_old_schema(input_dict[\"items\"], \"{}\".format(key_path), result)\n",
    "    \n",
    "    if key_path not in result and \"properties\" not in input_dict:\n",
    "        result[key_path] = False # can't really use this\n",
    "    return result\n",
    "\n",
    "\n",
    "def search_and_delete(key, input_dict):\n",
    "    \"\"\"delete keys from nested dict\"\"\"\n",
    "    if isinstance(input_dict, list):\n",
    "        for i in input_dict:\n",
    "            search_and_delete(key, i)\n",
    "    elif not isinstance(input_dict, dict):\n",
    "        return\n",
    "    to_del = []\n",
    "    for k in input_dict:\n",
    "        if k == key:\n",
    "            to_del.append(k)\n",
    "        elif isinstance(input_dict[k], dict):\n",
    "            search_and_delete(key, input_dict[k])\n",
    "    for k in to_del:\n",
    "        del input_dict[k]\n",
    "        \n",
    "        \n",
    "def rename_key(from_key, to_key, parent_dict):\n",
    "    \"\"\"rename key in dict\"\"\"\n",
    "    if from_key in parent_dict:\n",
    "        parent_dict[to_key] = parent_dict[from_key]\n",
    "        del parent_dict[from_key]\n",
    "\n",
    "def del_key(key, parent_dict):\n",
    "    if key in parent_dict:\n",
    "        del parent_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 0: compare old schema vs old data, output the summary in an excel file* to help decision making ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_props = {}\n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../inputs/json_1.4/\")):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        this_file_props = count_props(json_obj, \"\", {})\n",
    "        for prop in this_file_props:\n",
    "            if prop not in all_props:\n",
    "                all_props[prop] = [path.basename(json_path)]\n",
    "            else:\n",
    "                all_props[prop].append(path.basename(json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ../../preprocessed/old_schema_properties.csv\n",
      "File written: ../../preprocessed/old_data_vs_old_schema.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../inputs/mibig_schema.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_old_schema(json_obj, \"\", {})\n",
    "    with open(\"../../preprocessed/old_schema_properties.csv\", \"w\") as o:\n",
    "        for key in sorted(schema_props.keys()):\n",
    "            o.write(\"{},{}\\n\".format(key, schema_props[key]))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    with open(\"../../preprocessed/old_data_vs_old_schema.csv\", \"w\") as o:\n",
    "        not_in_schema = []\n",
    "        for key in sorted(all_props.keys()):\n",
    "            if key not in schema_props.keys():\n",
    "                not_in_schema.append((key, all_props[key]))\n",
    "        for rep in sorted(not_in_schema, key=lambda x: len(x[1]), reverse = True):\n",
    "            o.write(\"{},{}\\n\".format(rep[0], len(rep[1])))\n",
    "        print(\"File written: {}\".format(o.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 1: transform old schema to match (correct version) of old data ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ../../preprocessed/mibig_schema_phase_1.json\n"
     ]
    }
   ],
   "source": [
    "# (everything is manually done) -- TODO: should write hardcoded scripts to make it reproducible\n",
    "# update all comma-separated based properties into arrays\n",
    "# gene_pubs: integer --> gene_pubs: array\n",
    "print(\"File written: ../../preprocessed/mibig_schema_phase_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### phase 2: compare phase 1 schema vs old data, output the summary in an excel file* to help decision making ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ../../preprocessed/schema_phase_1_properties.csv\n",
      "File written: ../../preprocessed/old_data_vs_schema_phase_1.csv\n",
      "File written: ../../preprocessed/bgc_to_fix_phase_2.csv\n"
     ]
    }
   ],
   "source": [
    "# use all_props from phase 0\n",
    "with open(\"../../preprocessed/mibig_schema_phase_1.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_old_schema(json_obj, \"\", {})\n",
    "    with open(\"../../preprocessed/schema_phase_1_properties.csv\", \"w\") as o:\n",
    "        for key in sorted(schema_props.keys()):\n",
    "            o.write(\"{},{}\\n\".format(key, schema_props[key]))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append((key, all_props[key]))\n",
    "    with open(\"../../preprocessed/old_data_vs_schema_phase_1.csv\", \"w\") as o:\n",
    "        for rep in sorted(not_in_schema, key=lambda x: len(x[1]), reverse = True):\n",
    "            o.write(\"{},{}\\n\".format(rep[0], len(rep[1])))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    with open(\"../../preprocessed/bgc_to_fix_phase_2.csv\", \"w\") as o:\n",
    "        bgc_to_fix = {}\n",
    "        for rep in not_in_schema:\n",
    "            for bgc in rep[1]:\n",
    "                if bgc not in bgc_to_fix:\n",
    "                    bgc_to_fix[bgc] = []\n",
    "                bgc_to_fix[bgc].append(rep[0])\n",
    "        for bgc in bgc_to_fix:\n",
    "            o.write(\"{},{}\\n\".format(bgc, \";\".join(bgc_to_fix[bgc])))\n",
    "        print(\"File written: {}\".format(o.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### phase 3: transform old data to match schema from phase 1 (doesn't count dependencies/required keywords) ####\n",
    "def match_attributes_to_schema_phase_1(data):\n",
    "    # fix /Comments\n",
    "    rename_key(\"Comments\", \"comments\", data)\n",
    "    # fix /general_params/loci/nucl_acc[]/conn_comp_cluster\n",
    "    for nuac in data[\"general_params\"][\"loci\"][\"nucl_acc\"]:\n",
    "        if \"conn_comp_cluster\" in nuac and isinstance(nuac[\"conn_comp_cluster\"], str):\n",
    "            nuac[\"conn_comp_cluster\"] = nuac[\"conn_comp_cluster\"].replace(\" \", \"\").split(\",\")\n",
    "    # fix /general_params/Polyketide/Saccharide\n",
    "    if \"Polyketide\" in data[\"general_params\"] and \"Saccharide\" in data[\"general_params\"][\"Polyketide\"]:\n",
    "        data[\"general_params\"][\"Saccharide\"] = data[\"general_params\"][\"Polyketide\"][\"Saccharide\"]\n",
    "        del data[\"general_params\"][\"Polyketide\"][\"Saccharide\"]\n",
    "    # fix /general_params/Saccharide/Sugar_subclass\n",
    "    if \"Saccharide\" in data[\"general_params\"] and \"Sugar_subclass\" in data[\"general_params\"][\"Saccharide\"]:\n",
    "        rename_key(\"Sugar_subclass\", \"saccharide_subclass\", data[\"general_params\"][\"Saccharide\"])\n",
    "    # fix /general_params/Saccharide/gt_genes[]/sugar_subcluster\n",
    "    if \"Saccharide\" in data[\"general_params\"] and \"gt_genes\" in data[\"general_params\"][\"Saccharide\"]:\n",
    "        for gtg in data[\"general_params\"][\"Saccharide\"][\"gt_genes\"]:\n",
    "            if \"sugar_subcluster\" in gtg and isinstance(gtg[\"sugar_subcluster\"], str):\n",
    "                gtg[\"sugar_subcluster\"] = gtg[\"sugar_subcluster\"].replace(\" \", \"\").split(\",\")    \n",
    "    for comp in data[\"general_params\"][\"compounds\"]:\n",
    "        # fix /general_params/compounds[]/chem_target\n",
    "        if \"chem_target\" in comp and isinstance(comp[\"chem_target\"], str):\n",
    "            comp[\"chem_target\"] = comp[\"chem_target\"].replace(\" \", \"\").split(\",\")  \n",
    "        # fix /general_params/compounds[]/chem_moieties[]/subcluster\n",
    "        if \"chem_moieties\" in comp:\n",
    "            for moi in comp[\"chem_moieties\"]:\n",
    "                if \"subcluster\" in moi:\n",
    "                    if moi[\"subcluster\"] == \"unknown\":\n",
    "                        del moi[\"subcluster\"]\n",
    "    # fix /general_params/genes/gene[]/evidence_genefunction[][]**\n",
    "    if \"genes\" in data[\"general_params\"]:\n",
    "        if \"gene\" in data[\"general_params\"][\"genes\"]:\n",
    "            for gen in data[\"general_params\"][\"genes\"][\"gene\"]:\n",
    "                if \"evidence_genefunction\" in gen:\n",
    "                    for i, evgen in enumerate(gen[\"evidence_genefunction\"]):\n",
    "                        def getvalevgen(ar):\n",
    "                            if isinstance(ar, list):\n",
    "                                return getvalevgen(ar[0])\n",
    "                            else:\n",
    "                                return ar\n",
    "                        gen[\"evidence_genefunction\"][i] = getvalevgen(evgen)\n",
    "    # fix /general_params/Other/biosyn_class[]\n",
    "    if \"Other\" in data[\"general_params\"] and \"biosyn_class\" in data[\"general_params\"][\"Other\"]:\n",
    "        clas = data[\"general_params\"][\"Other\"][\"biosyn_class\"][0]\n",
    "        del data[\"general_params\"][\"Other\"][\"biosyn_class\"]\n",
    "        data[\"general_params\"][\"Other\"][\"other_subclass\"] = clas\n",
    "    # fix /general_params/publications\n",
    "    if \"publications\" in data[\"general_params\"] and isinstance(data[\"general_params\"][\"publications\"], str):\n",
    "        data[\"general_params\"][\"publications\"] = data[\"general_params\"][\"publications\"].replace(\" \", \"\").split(\",\")\n",
    "    # Polyketide\n",
    "    if \"Polyketide\" in data[\"general_params\"]:\n",
    "        pol = data[\"general_params\"][\"Polyketide\"]\n",
    "        # fix /general_params/Polyketide/cyclases\n",
    "        if \"cyclases\" in pol and isinstance(pol[\"cyclases\"], str):\n",
    "            pol[\"cyclases\"] = pol[\"cyclases\"].replace(\" \", \"\").split(\",\")\n",
    "        # fix /general_params/Polyketide/pks_genes\n",
    "        if \"pks_genes\" in pol and isinstance(pol[\"pks_genes\"], str):\n",
    "            pol[\"pks_genes\"] = pol[\"pks_genes\"].replace(\" \", \"\").split(\",\")\n",
    "        # fix /general_params/Polyketide/pufa_mod_doms\n",
    "        if \"pufa_mod_doms\" in pol and isinstance(pol[\"pufa_mod_doms\"], str):\n",
    "            pol[\"pufa_mod_doms\"] = pol[\"pufa_mod_doms\"].replace(\" \", \"\").split(\",\")\n",
    "        # fix /general_params/Polyketide/mod_pks_genes[]/pks_module[]/pks_mod_doms\n",
    "        if \"mod_pks_genes\" in pol:\n",
    "            for modpk in pol[\"mod_pks_genes\"]:\n",
    "                if \"pks_module\" in modpk:\n",
    "                    for pkmod in modpk[\"pks_module\"]:\n",
    "                        if \"pks_mod_doms\" in pkmod and isinstance(pkmod[\"pks_mod_doms\"], str):\n",
    "                            pkmod[\"pks_mod_doms\"] = pkmod[\"pks_mod_doms\"].replace(\" \", \"\").split(\",\")\n",
    "    # RiPP\n",
    "    if \"RiPP\" in data[\"general_params\"]:\n",
    "        rip = data[\"general_params\"][\"RiPP\"]\n",
    "        if \"precursor_loci\" in rip:\n",
    "            for ploc in rip[\"precursor_loci\"]:\n",
    "                # fix /general_params/RiPP/precursor_loci[]/cleavage_recogn_site\n",
    "                if \"cleavage_recogn_site\" in ploc and isinstance(ploc[\"cleavage_recogn_site\"], str):\n",
    "                    ploc[\"cleavage_recogn_site\"] = ploc[\"cleavage_recogn_site\"].replace(\" \", \"\").split(\",\")\n",
    "                # fix /general_params/RiPP/precursor_loci[]/core_pept_aa\n",
    "                if \"core_pept_aa\" in ploc and isinstance(ploc[\"core_pept_aa\"], str):\n",
    "                    ploc[\"core_pept_aa\"] = ploc[\"core_pept_aa\"].replace(\" \", \"\").split(\",\")\n",
    "                            \n",
    "    return\n",
    "\n",
    "if not path.exists(\"../../preprocessed/json_1.4_phase_3/\"):\n",
    "    makedirs(\"../../preprocessed/json_1.4_phase_3/\")\n",
    "    \n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../inputs/json_1.4/\")):\n",
    "    json_obj = None\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        match_attributes_to_schema_phase_1(json_obj)        \n",
    "        with open(path.join(\"../../preprocessed/json_1.4_phase_3/\", path.basename(json_path)), \"w\") as json_file:\n",
    "            json.dump(json_obj, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that all data matched schema\n",
    "all_props_phase_3 = {}\n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/json_1.4_phase_3/\")):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        this_file_props = count_props(json_obj, \"\", {})\n",
    "        for prop in this_file_props:\n",
    "            if prop not in all_props_phase_3:\n",
    "                all_props_phase_3[prop] = [path.basename(json_path)]\n",
    "            else:\n",
    "                all_props_phase_3[prop].append(path.basename(json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conflicts: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../preprocessed/mibig_schema_phase_1.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_old_schema(json_obj, \"\", {})\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props_phase_3.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append((key, all_props_phase_3[key]))\n",
    "            print(key)\n",
    "    print(\"Number of conflicts: {}\".format(len(not_in_schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 4: transform schema from phase 1 to match JSON Schema draft v7 (we will call it 'new schema') ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema = None\n",
    "with open(\"../../preprocessed/mibig_schema_phase_1.json\") as json_file:\n",
    "    new_schema = json.load(json_file) # pre-load with old schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: convert 'required' to Json Schema draft 7 style\n",
    "def fix_required(input_dict):\n",
    "    if \"type\" in input_dict and input_dict[\"type\"] == \"object\":\n",
    "        input_dict[\"required\"] = []\n",
    "        for prop in input_dict[\"properties\"]:\n",
    "            child = input_dict[\"properties\"][prop]\n",
    "            if \"required\" in child and child[\"required\"] == True:\n",
    "                input_dict[\"required\"].append(prop)\n",
    "            fix_required(child)\n",
    "        if len(input_dict[\"required\"]) < 1:\n",
    "            del input_dict[\"required\"]\n",
    "    else:\n",
    "        if \"type\" in input_dict and input_dict[\"type\"] == \"array\":\n",
    "            fix_required(input_dict[\"items\"])\n",
    "        if \"required\" in input_dict:\n",
    "            del input_dict[\"required\"]\n",
    "fix_required(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: convert 'dependencies' to Json Schema draft 7 style\n",
    "def fix_dependencies(input_dict):\n",
    "    if \"type\" in input_dict and input_dict[\"type\"] == \"object\":\n",
    "        input_dict[\"dependencies\"] = {}\n",
    "        for prop in input_dict[\"properties\"]:\n",
    "            child = input_dict[\"properties\"][prop]\n",
    "            if \"dependencies\" in child and isinstance(child[\"dependencies\"], str):\n",
    "                if child[\"dependencies\"] in input_dict[\"properties\"]:\n",
    "                    if child[\"dependencies\"] not in input_dict[\"dependencies\"]:\n",
    "                        input_dict[\"dependencies\"][child[\"dependencies\"]] = []\n",
    "                    input_dict[\"dependencies\"][child[\"dependencies\"]].append(prop)\n",
    "                else:\n",
    "                    print(\"Error: {} not found\".format(child[\"dependencies\"]))\n",
    "            fix_dependencies(child)\n",
    "        if len(input_dict[\"dependencies\"].keys()) < 1:\n",
    "            del input_dict[\"dependencies\"]\n",
    "    else:\n",
    "        if \"type\" in input_dict and input_dict[\"type\"] == \"array\":\n",
    "            fix_dependencies(input_dict[\"items\"])\n",
    "        if \"dependencies\" in input_dict:\n",
    "            del input_dict[\"dependencies\"]\n",
    "fix_dependencies(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: make sure 'enum' contain unique items\n",
    "def fix_enum(input_dict):\n",
    "    if \"type\" in input_dict and input_dict[\"type\"] == \"object\":        \n",
    "        for prop in input_dict[\"properties\"]:\n",
    "            fix_enum(input_dict[\"properties\"][prop])\n",
    "    elif \"type\" in input_dict and input_dict[\"type\"] == \"array\":\n",
    "        fix_enum(input_dict[\"items\"])\n",
    "            \n",
    "    if \"enum\" in input_dict:\n",
    "        input_dict[\"enum\"] = list(set(input_dict[\"enum\"]))\n",
    "fix_enum(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: manual (but reproducible) curations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "new_schema[\"$schema\"] = \"http://json-schema.org/draft-07/schema#\"\n",
    "new_schema[\"$schema_version\"] = \"2.0\"\n",
    "new_schema[\"$schema_created\"] = now.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "# remove version, replace with created and modified (datetime)\n",
    "del new_schema[\"properties\"][\"version\"]\n",
    "new_schema[\"properties\"][\"created\"] = { \"type\": \"string\", \"format\": \"date-time\" }\n",
    "new_schema[\"properties\"][\"modified\"] = { \"type\": \"string\", \"format\": \"date-time\" }\n",
    "\n",
    "# require \"created\", \"modified\", \"general_params\", \"personal\"\n",
    "new_schema[\"required\"] = [\"created\", \"modified\", \"general_params\", \"personal\"]\n",
    "\n",
    "# require \"mibig_accession\", \"biosyn_class\", \"compounds\", \"publications\"\n",
    "new_schema[\"properties\"][\"general_params\"][\"required\"] = [\"mibig_accession\", \"biosyn_class\", \"compounds\", \"publications\"]\n",
    "\n",
    "# delete properties we don't need anymore (i.e. ones meant for AlpacaJS forms)\n",
    "del new_schema[\"properties\"][\"personal\"][\"properties\"][\"submitter_institution\"][\"format\"]\n",
    "search_and_delete(\"default\", new_schema)\n",
    "\n",
    "# rename Polyketide, NRP, RiPP, Terpene, Saccharide, Alkaloid, Other, to lowercases\n",
    "rename_key(\"Polyketide\", \"polyketide\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"NRP\", \"nrp\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"RiPP\", \"ripp\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Terpene\", \"terpene\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Saccharide\", \"saccharide\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Alkaloid\", \"alkaloid\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "rename_key(\"Other\", \"other\", new_schema[\"properties\"][\"general_params\"][\"properties\"])\n",
    "\n",
    "# if class = \"Polyketide\", requires \"polyketide\" attribute, except if minimal = true\n",
    "del new_schema[\"properties\"][\"general_params\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"allOf\"] = []\n",
    "prop_attr_pairs = [\n",
    "    (\"NRP\", \"nrp\"),\n",
    "    (\"Polyketide\", \"polyketide\"),\n",
    "    (\"RiPP\", \"ripp\"),\n",
    "    (\"Terpene\", \"terpene\"),\n",
    "    (\"Saccharide\", \"saccharide\"),\n",
    "    (\"Alkaloid\", \"alkaloid\"),\n",
    "    (\"Other\", \"other\")\n",
    "]\n",
    "for prop_attr_pair in prop_attr_pairs:\n",
    "    new_schema[\"properties\"][\"general_params\"][\"allOf\"].append({\n",
    "        \"if\": {\n",
    "            \"not\": {\"properties\": {\"minimal\": {\"const\": True}}, \"required\": [\"minimal\"]},\n",
    "            \"properties\": {\"biosyn_class\": {\"contains\":{\"enum\": [prop_attr_pair[0]]}}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [prop_attr_pair[1]]\n",
    "          }\n",
    "    })\n",
    "\n",
    "# fix publications.pattern (it doesn't cover doi entries)\n",
    "# for now, just delete the pattern matching constraint\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"publications\"][\"items\"][\"pattern\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.compounds ###\n",
    "\n",
    "# remove database_deposited and databases_deposited. we can always infer it from their respective accession ids\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"database_deposited\"]\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"properties\"][\"databases_deposited\"]\n",
    "# delete old dependencies format\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"dependencies\"]\n",
    "# required = [\"compound\", \"evidence_struct\", \"chem_act\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"required\"] = [\"compound\", \"evidence_struct\", \"chem_act\"]\n",
    "# if chem_act == \"other\", require other_chem_act\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"compounds\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"chem_act\": {\"contains\":{\"enum\": [\"Other\"]}}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"other_chem_act\"]\n",
    "          }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.loci ###\n",
    "\n",
    "# move conn_comp_cluster from nucl_acc to loci (a cluster should only have one conn_comp_cluster)\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"conn_comp_cluster\"] = new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"][\"items\"][\"properties\"][\"conn_comp_cluster\"]\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"][\"items\"][\"properties\"][\"conn_comp_cluster\"]\n",
    "# loci.required = [\"complete\", \"nucl_acc\", \"conn_comp_cluster\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"required\"] = [\"complete\", \"nucl_acc\", \"conn_comp_cluster\"]\n",
    "# change 'Accession' to 'accession'\n",
    "rename_key(\"Accession\", \"accession\", new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"][\"items\"][\"properties\"])\n",
    "# nucl_acc.required = [\"accession\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"loci\"][\"properties\"][\"nucl_acc\"][\"items\"][\"required\"] = [\"accession\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.genes ###\n",
    "\n",
    "# operon.required = [\"operon_genes\", \"evidence_operon\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"operon\"][\"items\"][\"required\"] = [\"operon_genes\", \"evidence_operon\"]\n",
    "# gene.required = [\"gene_function\", \"evidence_genefunction\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"required\"] = [\"gene_function\", \"evidence_genefunction\"]\n",
    "# add nucl_acc, to specify which of the supplied accessions does this gene belongs to\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"properties\"][\"nucl_acc\"] = { \"type\": \"string\" }\n",
    "# delete gene.not_in_gbk, we can infer it from gene_id\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"properties\"][\"not_in_gbk\"]\n",
    "# delete old dependencies format, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"] = []\n",
    "# if gene_id is not supplied, require gene_startpos,gene_endpos,gene_name,accession\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": {\"required\": [\"gene_id\"]}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"gene_startpos\", \"gene_endpos\", \"gene_name\", \"nucl_acc\"]\n",
    "          },\n",
    "})\n",
    "# if gene_function == Tailoring, require tailoring\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"genes\"][\"properties\"][\"gene\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"gene_function\": {\"enum\": [\"Tailoring\"]}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"tailoring\"]\n",
    "          }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.polyketide ###\n",
    "\n",
    "# required = [\"pk_subclass\", \"pks_subclass\", \"pks_te_type\", \"lin_cycl_pk\", \"starter_unit\", \"ketide_length\", \"pks_te_type\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"required\"] = [\"pk_subclass\", \"pks_subclass\", \"pks_te_type\", \"lin_cycl_pk\", \"starter_unit\", \"ketide_length\"]\n",
    "\n",
    "# delete old dependencies format, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"pks_subclass\": {\"enum\": [\"Modular type I\"]}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"mod_pks_genes\"],\n",
    "            \"properties\": {\"mod_pks_genes\": {\"minItems\": 1}}\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"pks_subclass\": {\"enum\": [\"Trans-AT type I\"]}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"trans_at\"],\n",
    "            \"properties\": {\"trans_at\": {\"minItems\": 1}}\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"pks_subclass\": {\"enum\": [\"Iterative type I\"]}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"iterative_subtype\", \"nr_iterations\", \"iter_cycl_type\"]\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"pks_subclass\": {\"enum\": [\"PUFA synthase or related\"]}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"pufa_mod_doms\"],\n",
    "            \"properties\": {\"pufa_mod_doms\": {\"minItems\": 1}}\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"polyketide\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": { \"properties\": {\"pks_te_type\": {\"enum\": [\"None\"]}} }\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"pks_thioesterase\"],\n",
    "            \"properties\": {\"pks_thioesterase\": {\"minItems\": 1}}\n",
    "          }\n",
    "})\n",
    "\n",
    "# --- todo: assert dependencies of pks_subtype --> requirements\n",
    "# --- todo: apply dependencies for mod_pks_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.nrp ###\n",
    "\n",
    "# delete old dependencies format, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": { \"properties\": {\"nrps_te_type\": {\"enum\": [\"None\"]}} }\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"nrps_thioesterase\"],\n",
    "            \"properties\": {\"nrps_thioesterase\": {\"minItems\": 1}}\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"subclass\": {\"enum\": [\"Other lipopeptide\"]}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"lipid_moiety\"]\n",
    "          }\n",
    "})\n",
    "\n",
    "# nrps_module -- required = [\"module_nr\", \"cdom_subtype\", \"nrps_mod_doms\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"required\"] = [\"module_nr\", \"cdom_subtype\", \"nrps_mod_doms\"]\n",
    "# nrps_module -- delete old dependencies, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"nrps_mod_doms\": {\"enum\": [\"Other\"]}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"nrps_other_mod_dom\"]\n",
    "          }    \n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"nrp\"][\"properties\"][\"nrps_genes\"][\"items\"][\"properties\"][\"nrps_module\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": {\"properties\": {\"nrps_mod_skip_iter\": {\"enum\": [\"Neither\"]}}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"nrps_evidence_skip_iter\"]\n",
    "          }    \n",
    "})\n",
    "\n",
    "## todo -- assert dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general_params.saccharide ###\n",
    "\n",
    "# delete old dependencies format, replace with allOf\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"dependencies\"]\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"allOf\"] = []\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"properties\": {\"gt_specificity\": {\"enum\": [\"Other\"]}}\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"other_gt_spec\"]\n",
    "          }\n",
    "})\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"allOf\"].append({\n",
    "    \"if\": {\n",
    "            \"not\": { \"properties\": {\"gt_specificity\": {\"enum\": [\"Unknown\"]}} }\n",
    "          },\n",
    "          \"then\": {\n",
    "            \"required\": [\"evidence_gt_spec\"]\n",
    "          }\n",
    "})\n",
    "\n",
    "# move 'gt_genes.sugar_subcluster' to sugar_subclusters, we can infer the specificities from the list of gene ids\n",
    "new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"sugar_subclusters\"] = {\n",
    "    \"title\": \"Sub-clusters for sugar biosynthesis\",\n",
    "    \"type\": \"array\",\n",
    "    \"items\": new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"properties\"][\"sugar_subcluster\"]\n",
    "}\n",
    "del new_schema[\"properties\"][\"general_params\"][\"properties\"][\"saccharide\"][\"properties\"][\"gt_genes\"][\"items\"][\"properties\"][\"sugar_subcluster\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for all 'required' fields, if it is an array, specify 'minItems = 1' so that it can't be empty\n",
    "def apply_array_required(input_dict):\n",
    "    if \"properties\" in input_dict:\n",
    "        if \"required\" in input_dict:\n",
    "            for req in input_dict[\"required\"]:\n",
    "                if \"type\" in input_dict[\"properties\"][req] and input_dict[\"properties\"][req][\"type\"] == \"array\":\n",
    "                    if \"minItems\" not in input_dict[\"properties\"][req]:\n",
    "                        input_dict[\"properties\"][req][\"minItems\"] = 1\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            apply_array_required(input_dict[\"properties\"][key])\n",
    "    elif \"items\" in input_dict:\n",
    "        apply_array_required(input_dict[\"items\"])\n",
    "                                \n",
    "apply_array_required(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: save new schema\n",
    "with open(\"../../outputs/mibig_schema_draft7.json\", \"w\") as o:\n",
    "    o.write(json.dumps(new_schema, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### phase 5: transform data from phase 2 to match the new schema (include all dependencies/required keywords) ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ../../preprocessed/schema_draft7_properties.csv\n",
      "File written: ../../preprocessed/data_phase_3_vs_schema_draft7.csv\n",
      "File written: ../../preprocessed/bgc_to_fix_phase_5.csv\n"
     ]
    }
   ],
   "source": [
    "# 1: check data vs new schema to get quick overview of changed structures\n",
    "# use all_props from phase 3\n",
    "with open(\"../../outputs/mibig_schema_draft7.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_new_schema(json_obj, \"\", {})\n",
    "    with open(\"../../preprocessed/schema_draft7_properties.csv\", \"w\") as o:\n",
    "        for key in sorted(schema_props.keys()):\n",
    "            o.write(\"{}\\n\".format(key, schema_props[key]))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props_phase_3.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append((key, all_props_phase_3[key]))\n",
    "    with open(\"../../preprocessed/data_phase_3_vs_schema_draft7.csv\", \"w\") as o:\n",
    "        for rep in sorted(not_in_schema, key=lambda x: len(x[1]), reverse = True):\n",
    "            o.write(\"{},{}\\n\".format(rep[0], len(rep[1])))\n",
    "        print(\"File written: {}\".format(o.name))\n",
    "    with open(\"../../preprocessed/bgc_to_fix_phase_5.csv\", \"w\") as o:\n",
    "        bgc_to_fix = {}\n",
    "        for rep in not_in_schema:\n",
    "            for bgc in rep[1]:\n",
    "                if bgc not in bgc_to_fix:\n",
    "                    bgc_to_fix[bgc] = []\n",
    "                bgc_to_fix[bgc].append(rep[0])\n",
    "        for bgc in bgc_to_fix:\n",
    "            o.write(\"{},{}\\n\".format(bgc, \";\".join(bgc_to_fix[bgc])))\n",
    "        print(\"File written: {}\".format(o.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conflicts: 0\n"
     ]
    }
   ],
   "source": [
    "# 2: fix data and assert that there is no more unrecognized attributes present\n",
    "def match_attributes_to_schema_7(data):\n",
    "    # remove 'version'\n",
    "    del_key(\"version\", data)\n",
    "    con_comp_temp = []\n",
    "    for nuc in data[\"general_params\"][\"loci\"][\"nucl_acc\"]:\n",
    "        # rename Accession to accession\n",
    "        rename_key(\"Accession\", \"accession\", nuc)\n",
    "        if \"conn_comp_cluster\" in nuc:\n",
    "            for con_comp in nuc[\"conn_comp_cluster\"]:\n",
    "                if con_comp not in con_comp_temp:\n",
    "                    con_comp_temp.append(con_comp)\n",
    "            del nuc[\"conn_comp_cluster\"]\n",
    "    # fix /general_params/loci/nucl_acc[]/conn_comp_cluster[]\n",
    "    if len(con_comp_temp) > 0:\n",
    "        data[\"general_params\"][\"loci\"][\"conn_comp_cluster\"] = con_comp_temp\n",
    "    # rename Polyketide, NRP, etc. to its lowercase version\n",
    "    rename_key(\"Polyketide\", \"polyketide\", data[\"general_params\"])\n",
    "    rename_key(\"NRP\", \"nrp\", data[\"general_params\"])\n",
    "    rename_key(\"RiPP\", \"ripp\", data[\"general_params\"])\n",
    "    rename_key(\"Terpene\", \"terpene\", data[\"general_params\"])\n",
    "    rename_key(\"Saccharide\", \"saccharide\", data[\"general_params\"])\n",
    "    rename_key(\"Alkaloid\", \"alkaloid\", data[\"general_params\"])\n",
    "    rename_key(\"Other\", \"other\", data[\"general_params\"])\n",
    "    for comp in data[\"general_params\"][\"compounds\"]:\n",
    "        # fix /general_params/compounds[]/database_deposited\n",
    "        if \"database_deposited\" in comp:\n",
    "            del comp[\"database_deposited\"]\n",
    "        # fix /general_params/compounds[]/databases_deposited[]\n",
    "        if \"databases_deposited\" in comp:\n",
    "            del comp[\"databases_deposited\"]\n",
    "    # fix /general_params/genes/gene[]/not_in_gbk\n",
    "    if \"genes\" in data[\"general_params\"]:\n",
    "        if \"gene\" in data[\"general_params\"][\"genes\"]:\n",
    "            for gen in data[\"general_params\"][\"genes\"][\"gene\"]:\n",
    "                del_key(\"not_in_gbk\", gen)\n",
    "    # fix /general_params/saccharide/gt_genes[]/sugar_subcluster[]\n",
    "    sugsub = []\n",
    "    if \"saccharide\" in data[\"general_params\"]:\n",
    "        if \"gt_genes\" in data[\"general_params\"][\"saccharide\"]:\n",
    "            for gtg in data[\"general_params\"][\"saccharide\"][\"gt_genes\"]:\n",
    "                if \"sugar_subcluster\" in gtg:\n",
    "                    sugsub.append(gtg[\"sugar_subcluster\"])\n",
    "                    del gtg[\"sugar_subcluster\"]\n",
    "        if len(sugsub) > 0:\n",
    "            data[\"general_params\"][\"saccharide\"][\"sugar_subclusters\"] = sugsub\n",
    "    return\n",
    "\n",
    "if not path.exists(\"../../preprocessed/json_1.4_phase_5/\"):\n",
    "    makedirs(\"../../preprocessed/json_1.4_phase_5/\")\n",
    "    \n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/json_1.4_phase_3/\")):\n",
    "    json_obj = None\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        match_attributes_to_schema_7(json_obj)\n",
    "        with open(path.join(\"../../preprocessed/json_1.4_phase_5/\", path.basename(json_path)), \"w\") as json_file:\n",
    "            json.dump(json_obj, json_file)\n",
    "\n",
    "all_props_phase_5 = {}\n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/json_1.4_phase_5/\")):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        this_file_props = count_props(json_obj, \"\", {})\n",
    "        for prop in this_file_props:\n",
    "            if prop not in all_props_phase_5:\n",
    "                all_props_phase_5[prop] = [path.basename(json_path)]\n",
    "            else:\n",
    "                all_props_phase_5[prop].append(path.basename(json_path))\n",
    "\n",
    "with open(\"../../outputs/mibig_schema_draft7.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_new_schema(json_obj, \"\", {})\n",
    "    not_in_schema = []\n",
    "    for key in sorted(all_props_phase_5.keys()):\n",
    "        if key not in schema_props.keys():\n",
    "            not_in_schema.append(key)\n",
    "            print(\"{},{}\".format(key, len(all_props_phase_5[key])))\n",
    "    print(\"Number of conflicts: {}\".format(len(not_in_schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: validate data using JSON Schema V7 validator, fix conflicts, then save to final output folder\n",
    "if not path.exists(\"../../outputs/json_2.0/\"):\n",
    "    makedirs(\"../../outputs/json_2.0/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
